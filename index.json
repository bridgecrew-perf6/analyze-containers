{
  "content/add_data_config_dev.html": {
    "href": "content/add_data_config_dev.html",
    "title": "Add data to your config development environment",
    "keywords": "Add data to your config development environment Developing the configuration is separated into two parts: enabling i2 Analyze to work with your data, and defining how analysts interact with your data. First, update the DEPLOYMENT_PATTERN variable in your <config_name>/utils/variables.sh file to your intended deployment pattern. For example, to deploy with the i2 Connect gateway and the Information Store set the following value: DEPLOYMENT_PATTERN=\"i2c_istore\" Depending on the composition of your deployment, there are three methods that you can use to get your data into i2 Analyze for analysis. To develop the process for ingesting data into the Information Store, refer to Ingesting data into the Information Store . To develop connections to external data sources, refer to Connecting to external data sources . Ensure that analysts can import and create representative records in Analyst's Notebook Premium. Then, if required, upload records to the Information Store. For more information, see Import data and Create i2 Analyze chart items . When you develop the process to get your data into i2 Analyze, you might realize that your schema or security schema are not correct for your data. You can update the deployed schemas to better represent your data and security model. If you need to make destructive changes, change the DEPLOYMENT_PATTERN variable in your <config_name>/utils/variables.sh file to schema_dev and redeploy. Then complete the process of developing schemas . After you add data to your environment, you can configure the rest of the configuration ."
  },
  "content/backup and restore/backup.html": {
    "href": "content/backup and restore/backup.html",
    "title": "Backup",
    "keywords": "Backup This section describes the process to back up the Solr collections and the Information Store database in SQL Server in an i2 Analyze deployment in a containerized environment. Understanding the back up process In a deployment of i2 Analyze, data is stored in the Information Store database and indexed in Solr collections. You must back up these components as a pair to enable you to restore the data in your system after a failure. When you back up the Solr collections, the configuration in ZooKeeper is also backed up. To ensure that data can be restored correctly, you must back up the components in the following order: Solr collections Information Store database If data is changed in the database after taking the Solr backup, Solr can update the index to reflect these changes when the collections and database are restored. When you create your backups, ensure that you store both backups so that you can identify the pair of backups that you must restore if required. Note: In the walkthrough, both the database and Solr backups are versioned using the variable backup_version . In the walkthrough the backup version is 1 . This is used to create a directory where the all of the backup files are stored for this pair. To create another backup pair, increment the backup_version so that the backup files are stored in a different directory. Backing up the Solr collections To back up Solr collections, you must have a shared filesystem available that is shared and accessible by all Solr nodes. In a containerized environment, a backup volume is shared between all Solr containers. Note: In the example, the backup volume is mounted to /backup in the Solr container. To ensure Solr can write the backup file, you must make sure the Solr process has permission to the backup folder. In order for Solr to write to the folder the user solr and group 0 must have read and write permission to the backup folder. The following chown command is an example that gives the Solr process permission to write to the backup location: chown -R solr:0 /backup The runSolrClientCommand client function is used to run the BACKUP API request. The backup operation must be performed for each non-transient collection. The non-transient collections are the main_index , match_index , and chart_index . The following curl command is an example that creates a backup of the main_index collection: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert /tmp/i2acerts/CA.cer https://solr1:8983//solr/admin/collections?action=BACKUP&async=main_index_backup&name=main_index_backup&collection=main_index&location=/backup/1/ \" To perform a backup operation, use the Solr Collections API. For more information about the backup API command, see BACKUP: Backup Collection Note: The BACKUP API request must be an asynchronous call otherwise the backup procedure will timeout. This is done by adding async flag with a corresponding id to the curl command. In the above example, this is &async=main_index_backup . See the Backing up Solr section of the walkthrough script. Determining status of Solr backup The Monitoring Solr backup process section of the walkthrough script runs a loop around the getAsyncRequestStatus client function that reports the status of the Asynchronous backup request. For more information about the client function, see getAsyncRequestStatus . See the Monitoring Solr backup progress section of the walkthrough script. Backing up the system match rules file The system match rules are stored in the ZooKeeper configuration, and can be backed up by using the Solr ZooKeeper Command Line Interface (zkcli) from the Solr client container. The runSolrClientCommand client function is used to run the zkcli command. The following command is an example of how to use the getfile command to retrieve the system match rules: runSolrClientCommand \"/opt/solr/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd getfile /configs/match_index1/match_index1/app/match-rules.xml /backup/1/system-match-rules.xml Note: The system match rules file is backed up to the /backup/1/ folder where the Solr backup is located. For more information about the ZooKeeper command line utilities, see Using Solrâ€™s ZooKeeper CLI . Backing up the Information Store database In the containerized environment, the mounted backup volume is used to store the backup file. Only the Information Store database is backed up. The SQL Server instance and system databases are not included in the backup. For more information about the backup volume, see Running a SQL Server . The backup is performed by a user that has the built-in db_backupoperator SQL Server role. In this case, that is the dbb user. Use the runSQLServerCommandAsDBB client function to run the following SQL command to create the backup file. For example: runSQLServerCommandAsDBB bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"USE ISTORE; BACKUP DATABASE ISTORE TO DISK = '/backup/1/IStore.bak' WITH FORMAT;\\\"\" For more information about backing up a SQL Server database, see: Backup SQL Documentation SQL Server Backup Permissions For more information about the dbb user and it's role, see: db users See the Backing up SQL Server section of the walkthrough script."
  },
  "content/backup and restore/br.html": {
    "href": "content/backup and restore/br.html",
    "title": "Backup and restore",
    "keywords": "Backup and restore The documentation in this section describes how to back up and restore a deployment of i2 Analyze in a containerized environment."
  },
  "content/backup and restore/restore.html": {
    "href": "content/backup and restore/restore.html",
    "title": "Restore",
    "keywords": "Restore This section describes the process to restore the Solr collections and Information Store database in SQL Server of an i2 Analyze deployment in a containerized environment. Understanding the restore process To restore the data and indexes for a deployment of i2 Analyze, restore the pair of Solr collection and Information Store database backups. To ensure that the data is restored correctly, restore the pair of backups in the following order: Information Store database Solr collections If any data has changed in the Information Store after the Solr backup, the Solr index is updated to reflect the contents of the Information Store database when Liberty is started. Preparing the environment Before you can restore the backups, clean down the Solr, ZooKeeper & SQL Server environment if you are not using a clean environment. See the Simulating Clean down section of the walkthrough script. Stop the liberty servers Before the restore process can begin you must stop the liberty servers. In a this can be done by stopping the containers The following command is an example of how to stop the liberty containers: docker container stop liberty1 liberty2 Restoring the Information Store database The process of restoring the Information Store database contains the following steps: Running a SQL Server container with a new instance of SQL Server Creating the Information Store database from the backup file in the new instance Recreating the required logins in the new SQL Server instance, and the users in the restored database Running a SQL Server container Run a container for the new instance of SQL Server, this will have the backup directory available. For more information about running a SQL Server container, see SQL Server . See the Running the SQL Server section of the walkthrough script. Creating the database from the backup file In a new instance of SQL Server, the only user is the sa user. Use the runSQLServerCommandAsSA client function to run the following SQL command as the sa user to create the ISTORE database from the backup file: runSQLServerCommandAsSA bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"RESTORE DATABASE ISTORE FROM DISK = '/backup/IStore.bak;'\" See the Restoring the ISTORE database section of the walkthrough script. Recreating the logins and users In SQL Server, a login is scoped to the Database Engine. To connect to a specific database, in this case the ISTORE database, a login must be mapped to a database user. Because the backup is completed for the ISTORE database only, the logins from the previous SQL Server instance cannot be restored. Additionally, the database users are restored but there are no logins mapped to them. For more information about SQL Server logins, see SQL Server Login documentation In this environment, create the required logins and users by dropping the users from the database and recreating the logins and the users by using the createDbLoginAndUser client function. The logins and users are created in the same way as when original SQL Server instance was configured. For more information about creating the required logins, users, and permissions, see Configuring SQL Server in the deployment documentation. See the Dropping existing database users and Recreating database logins, users, and permissions sections of the walkthrough script. Restoring the Solr collections Restoring the solr indexes includes the following high-level steps: Deploy a new Solr cluster and ZooKeeper ensemble Restore the non-transient Solr collections Monitor the restore process until completed Recreate transient Solr collections Restore system match rules Deploy a new Solr cluster and ZooKeeper ensemble To restore the Solr indexes, deploy a new Solr cluster & ZooKeeper ensemble. For more information about running a clean ZooKeeper & Solr environment, see Running Solr and ZooKeeper . Create Solr cluster . See the Deploying Clean Solr & Zookeeper section of the walkthrough script. Restoring the non-transient Solr collections The runSolrClientCommand client function is used to run the RESTORE API request. The restore operation must be performed for each non-transient collection that was backed up. The following curl command is an example that restores the main_index collection: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert /tmp/i2acerts/CA.cer https://solr1:8983/solr/admin/collections?action=RESTORE&async=main_index_backup&name=main_index_backup&collection=main_index&location=/backup/1\" \" To perform a restore operation, use the Solr Collections API. For more information about the Restore API command, see RESTORE: Restore Collection . Note: The restore API request must be an asynchronous call otherwise the restore procedure will timeout. This is done by adding async flag with a corresponding id to the curl command. In the above example, this is &async=main_index_backup . See the Restoring non-transient Solr collection section of the walkthrough script. Determining completion of Solr restore procedure The Monitoring Solr restore process section of the walkthrough script runs a loop around the getAsyncRequestStatus client function that reports the status of the Asynchronous request. For more information about the client function, see getAsyncRequestStatus . See the Monitoring Solr restore progress section of the walkthrough script. Recreate transient Solr collections Recreate the transient daod_index , vq_index , and highlightquery_index Solr collections. For more information about the creating Solr collections, see Configuring Solr and ZooKeeper . See the Recreating transient Solr collections section of the walkthrough script. Restore system match rules After the indexes are restored, upload the system match rules file. Use the Solr ZooKeeper Command Line Interface (zkcli) to create the directory in ZooKeeper for the system match rules file and to upload it. The runSolrClientCommand client function is used to run the zkcli request. The following command creates the directory in ZooKeeper where the system match rules file must be stored: runSolrClientCommand \"/opt/solr/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd makepath /configs/match_index1/match_index1/app Note: The path to where the system match rules file must be located is in the following format configs/<index name>/<index name>/app The following command uploads the system match rules file to the directory in ZooKeeper created earlier: runSolrClientCommand \"/opt/solr/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd putfile /configs/match_index1/match_index1/app/match-rules.xml /backup/1/system-match-rules.xml For more information about the ZooKeeper command line utilities, see Using Solrâ€™s ZooKeeper CLI . See the Restoring system match rules section of the walkthrough script. Start the Liberty containers After the Solr collections and Information Store database are restored, start the Liberty containers. The following command is an example of how to start the Liberty containers: docker container start liberty1 liberty2 After the Liberty containers have started, the waitFori2AnalyzeServiceToBeLive common function ensures that the i2 Analyze service is running. See the Restart Liberty containers section of the walkthrough script."
  },
  "content/connector_config_dev.html": {
    "href": "content/connector_config_dev.html",
    "title": "Adding connectors to your development environment",
    "keywords": "Adding connectors to your development environment To add a connector to your deployment of i2 Analyze, you must develop an i2 Connect connector. For more information about developing connectors, see i2 Connect gateway connector development . Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to a pattern that includes the i2 Connect gateway. For example: DEPLOYMENT_PATTERN=\"i2c_istore\" Process overview: Create a custom connector image Update the configuration to reference connectors Build connector images and redeploy Creating a connector image Create a connector image from one of the image templates. You can deploy connectors using the following templates: Spring Boot for Java based connectors i2 Connect server for Node v16 based connectors developed using the i2 Connect server SDK NodeJS for Node v16 based connectors External for connectors hosted outside of the config development environment The connector should be secured and run on port 3443 . For more information about securing your system, see Securing i2 Analyze . Spring Boot based connector image Copy the /templates/springboot-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the springboot-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - target - connector-definition.json - connector-version.json - ... Copy your connector code into the target directory of the connector directory. For example, copy your .jar file into the <connector_name>/target directory. Next, configure your connector . i2 Connect server based connector image Copy the /templates/i2connect-server-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the i2connect-server-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - app - connector-definition.json - connector-version.json - ... Copy the i2 Connect server connector distributable ( the .tar.gz or .tgz file) into the connector directory. Next, configure your connector . NodeJS based connector image Copy the /templates/node-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the node-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - app - connector-definition.json - connector-version.json - ... Copy your connector code into the app directory of the connector directory. For example, copy the files into the <connector_name>/app directory. By default, the image will start node with the file app.js . Next, configure your connector . External connector Copy the /templates/external-connector directory to the /connector-images directory. Rename the external-connector folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - connector-definition.json Next, configure your connector . Configuring a connector Populate the values in the <connector_name>/connector-definition.json file with information about your connector. Key Description id An identifier that is unique for all connectors that will be deployed. name A name for the connector that is displayed to users in the client. configurationPath The full path to the configuration endpoint of the connector. By default, it is /config . gatewaySchema The short name of an optional gateway schema. When no gateway schema is used, do not provide a value. Add the referenced schema and charting scheme to the gateway-schemas directory. type Used to identify the type of connector. For i2 Connect server connectors, set to i2connect-server . For external connectors, set to external . For any other connectors, the type key is not required. sendSensitiveHeaders This setting effectively disables connectors that employ user-specific configuration. baseUrl Used only for connectors of type external . The baseUrl value is the URL address of the connector. For any other connectors, the baseUrl key is not required. For example: { \"id\": \"connector1\", \"name\": \"Connector 1\", \"description\": \"First connector\", \"configurationPath\": \"/config\", \"gatewaySchema\": \"template\", \"sendSensitiveHeaders\": \"false\", \"type\": \"i2connect-server\" } or { \"id\": \"connector2\", \"name\": \"Connector 2\", \"description\": \"Second connector\", \"configurationPath\": \"/config\", \"gatewaySchema\": \"template\", \"sendSensitiveHeaders\": \"false\", \"type\": \"external\", \"baseUrl\": \"https://example-connector:3443\" } Note: If your external connector is hosted in a Docker container on a different network to your config development environment, connect the Liberty container to the same network as your external connector container. For example, run docker network connect <connector-network-name> liberty1.<config-name> . If your connector is running on your host machine, use the IP address of your host machine in the baseUrl value. Provide the version of the connector in the <connector_name>/connector-version.json file. For external connectors, this file is not required. The version value specifies the version of the connector. For example: { \"version\": \"0.0.1\", \"tag\": \"0-0-1\" } For more information about configuring connectors, see Managing connectors . Adding connectors to a config In the configuration you want to deploy your connector with, update the /configs/<config_name>/configuration/connector-references.json file and add the directory name for your connector in the connectors array. For example, to add the connector in connector-images\\example-connector the connector-references.json file contains the following connectors array: { \"connectors\": [ { \"name\": \"example-connector\" } ], ... } Defining a gateway schema (Optional) You can develop the gateway schema and associated charting schemes by following the instructions in Developing schemas in your configuration development environment . After you develop the schemes, complete the following steps to add deploy them with your configuration: Copy the schema and charting scheme files to the /gateway-schemas directory. To define the short name of the gateway schema, the prefix of the file name is used. The convention is defined as: <short>-<name>-schema.xml and <short>-<name>-schema-charting-scheme.xml . For example, files with the names law-enforcement-schema.xml and law-enforcement-charting-schemes.xml have the short name law-enforcement . Add the short name of your schema to the /configs/<config_name>/configuration/connector-references.json file in the gatewaySchemas array. For example, to add a gateway schema with the short name law-enforcement , the file contains the following gatewaySchemas array: { ... ], \"gatewaySchemas\": [ { \"shortName\": \"law-enforcement\" } ] } Note: If you are not using any gateway schemas, do not remove the gatewaySchemas key from the file. Set the short name of your schema in the <connector_name>/connector-definition.json as the value for the gatewaySchema key of the connector that will use the gateway schema. Building the connector images and redeploying Run the ./deploy.sh script with your config name from the /scripts directory to generate the secrets for your connectors, build the connector images, and deploy your environment. ./deploy.sh -c <config_name> After you add connectors to your environment, you can configure the rest of the configuration ."
  },
  "content/deploy_config_dev.html": {
    "href": "content/deploy_config_dev.html",
    "title": "Configuration development environment",
    "keywords": "Configuration development environment To create a production deployment of i2 Analyze, you must develop the i2 Analyze configuration. The production deployment process describes a process of using different development environments to produce your i2 Analyze configuration. For more information about the environments, see Deployment phases and environments . The analyze-containers repository enables you to create a development environment that comprises a containerized deployment of i2 Analyze. The containerized deployment enables you to switch between deployment patterns easily and redeploy changes in a consistent manner. Process overview: Install the prerequisites for the analyze-containers repository Create a config template Create a custom config that you can deploy into a development environment Use the environment to develop the i2 Analyze configuration If you have already run the createDevEnvironment script and you want to create a new custom config from the template, go to Creating a development config . Prerequisites Before you create the configuration development environment, you must configure the analyze-containers repository. For more information, see Getting started with the analyze-containers repository . Creating the configuration development environment To create the configuration development environment and template config, in the /scripts directory run: ./createDevEnvironment.sh The script performs the following actions: Extracts the required files from the i2 Analyze deployment toolkit Builds the required Docker images for the development environment Generates the secrets that are used in the environment Creates the configuration template The configuration template is located in /templates/config-development . Accepting the licenses Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT variable to ACCEPT . The variable is in the licenses.conf file. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The variables are in the licenses.conf file. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y Installing the certificate To access the system, the server that you are connecting from must trust the certificate that it receives from the deployment. To enable trust, install the /dev-environment-secrets/generated-secrets/certificates/externalCA/CA.cer certificate as a trusted root certificate authority in your browser and operating system's certificate store. For information about installing the certificate, see: Windows: Install Certificates with the Microsoft Management Console MacOS: Add certificates to a keychain using Keychain Access on Mac FireFox: In the settings menu, type View Certificates and open it. Then, click Import and locate the CA.cer file. Creating a config Before you can deploy the environment, create a config from the template: Copy the config-development directory (including the sub-directories) from /templates to the /configs directory. You can rename the config-development folder to a custom config name or use the default value. The name of the directory is used to determine which config to deploy, it is recommended to use a short name with no spaces, special characters, or capital letters. The directory structure is: - configs - <config_name> - configuration - database-scripts - utils The /configs/<config_name>/utils/variables.sh file contains variables that are used when you deploy the environment with this config. You can specify values for the following variables: The DEPLOYMENT_PATTERN variable specifies the deployment pattern for the deployment of i2 Analyze in the environment. You can specify: schema_dev - An i2 Connect gateway only deployment that you can use to develop your data and security models istore - Information Store i2c - i2 Connect gateway only cstore - Chart Store i2c_istore - i2 Connect gateway and Information Store i2c_cstore - i2 Connect gateway and Chart Store For more information about the i2 Analyze deployment patterns, see: Components . The following variables specify the ports that are exposed to the host machine for each component of i2 Analyze. Each variable has a default value: HOST_PORT_SOLR specifies the port that you use to connect to the Solr Web UI from the host machine. By default, 8983 . HOST_PORT_DB specifies the port that you use to connect to the database from the host machine. By default, 1433 . HOST_PORT_I2ANALYZE_SERVICE specifies the port that you use to connect to the deployment from the host machine. By default, 9443 . HOST_PORT_PROMETHEUS specifies the port that you use to connect to the Prometheus UI from the host machine. By default, 9090 . HOST_PORT_GRAFANA specifies the port that you use to connect to the Grafana UI from the host machine. By default, 3500 . For example, to deploy with the i2 Connect gateway only for schema development and to use the default ports: DEPLOYMENT_PATTERN=\"schema_dev\" HOST_PORT_SOLR=\"8983\" HOST_PORT_DB=\"1433\" HOST_PORT_I2ANALYZE_SERVICE=\"9443\" HOST_PORT_PROMETHEUS=\"9090\" HOST_PORT_GRAFANA=\"3500\" Specifying the schema and charting schemes files To deploy i2 Analyze, you must provide a schema and charting scheme. The i2 Analyze toolkit includes example files that you can use as a starting point, or you can use existing files. For more information about the example schemas, see Example schemas . Choose an example schema and associated charting scheme as your starting point from the pre-reqs/i2analyze/toolkit/examples/schemas directory. Copy the chosen schema and charting scheme to your configs/<config_name>/configuration directory. Remove the existing schema.xml and schema-charting-schemes.xml files. Then, rename your chosen schema file to schema.xml and your charting scheme to schema-charting-schemes.xml . Specifying the security schema, user registry, and command access control files To connect to an i2 Analyze deployment, you must provide a security schema and user registry. To control access to features and enable to use of the administrator endpoints, provide a command access control file. The i2 Analyze toolkit includes example files that you can use as a starting point. For more information about the example files, see Example schemas . Copy and overwrite the example security-schema.xml , user-registry.xml , and command-access-control.xml files from the pre-reqs/i2analyze/toolkit/examples/security directory to your configs/<config_name>/configuration directory. Deploying your config in the environment Run the ./deploy.sh script from the /scripts directory to deploy and start your environment. Provide the name of the config to deploy with by using the -c parameter and specify the name of the config directory. For example: ./deploy.sh -c config-development You can run only one environment at a time. When you run the deploy.sh script, it stops any containers on the Docker network. If you are deploying with the same config, it update the images and recreates the containers to update the environment with any configuration changes. For more information, see Managing configs . Accessing the system To connect to the deployment, the URL to use is: https://i2analyze.eia:9443/opal Note: If you are using the schema_dev or i2c deployment patterns, you must connect using Analyst's Notebook Premium. Log in as a user that is specified in the user registry file. If you are using the example user registry, the example user has the following credentials: The user name is Jenny The password is Jenny What to do next? After you deploy your environment, you can start to develop the i2 Analyze configuration. Using your environment for configuration development: Developing schemas in your configuration development environment After you develop your schema files, you can develop the rest of the i2 Analyze configuration. Developing the rest of the configuration"
  },
  "content/develop_config_dev.html": {
    "href": "content/develop_config_dev.html",
    "title": "Developing the rest of the configuration",
    "keywords": "Developing the rest of the configuration After you enable i2 Analyze to work with your data, you can define how analysts interact with your data. To do this, use the following information to modify the i2 Analyze configuration to meet your requirements. Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to your intended deployment pattern. For example, to deploy with the i2 Connect gateway and the Information Store set the following value: DEPLOYMENT_PATTERN=\"i2c_istore\" Note: In the analyze-containers repository, the structure of the configuration directory is modified. The configuration files are in the top level configs/<config_name>/configuration directory. You cannot run any toolkit tasks described in the linked documentation in this environment. Instead, after you modify any configuration files, run the deploy.sh script with your config name as you did when you created your deployment. The list of things that you can configure includes: To configure how analysts search for information, including Quick Search, Visual Query, and Highlight Query, refer to Configuring search To configure how analysts can identify matching records, refer to Configuring matching To configure the geospatial mapping options available to analysts, refer to Configuring geospatial mapping To configure the values that analysts can provide in source references, refer to Configuring source references To configure item type access for analysts, refer to Configuring item type security To update the Solr index configuration, refer to Configuring the Solr index For more information about other configuration changes that you can make, see Configuring i2 Analyze ."
  },
  "content/develop_extensions.html": {
    "href": "content/develop_extensions.html",
    "title": "Developing and deploying extensions for i2 Analyze",
    "keywords": "Developing and deploying extensions for i2 Analyze i2 Analyze Developer Essentials contains tools, libraries, and examples that enable development and deployment of custom extensions to i2 Analyze. The following information describes how you can use the config development environment to quickly deploy an extension for a config to test and develop. For more information about the types of extension that are available, see i2 Analyze Developer Essentials . Creating a project for your extension You must create a Maven artifact to develop your extension in. Copy the template templates/extension-development directory to the i2a-extensions directory and rename it to be the name of your extension. This name is also the artifact id. For example, name the directory opal-default-security-example . The directory structure is: - i2a-extensions - <artifact_id> - src - main - java - test - java - pom.xml Update the pom.xml file inside your new directory with information about your extension. The elements to modify are at the root of the file. Update the values of the following elements: <groupId> is the name of the Java package <artifactId> matches the name of the directory for your extension <version> is the version number for your extension For example: <groupId>com.i2group</groupId> <artifactId>opal-default-security-example</artifactId> <version>1.0.0</version> You can now start developing your extension. For examples on i2 Analyze extensions, check the examples in i2 Analyze Developer Essentials . Copy the contents of src/main folder of the extension into the i2a-extensions/<artifact_id>/src/main directory. Add any settings that are required by the extension to the analyze-settings.properties file. Add any resources that are required by the extension to the /configs/<config_name>/configuration directory. For example, the group-based-default-security-dimension-values.xml file. If your extension depends on another extension. You must add it to your pom.xml file under the comment: <!-- Extension specific dependencies --> . For example, the auditing extensions in Developer Essentials depend on the opal-audit-example-common extension. To add it as a dependency, populate the values of the pom.xml file as follows: <!-- Extension specific dependencies --> <dependency> <groupId>com.i2group</groupId> <artifactId>opal-audit-example-common</artifactId> <version>1.0.0</version> </dependency> You will also need to populate the i2a-extensions/extension-dependencies.json file, for example: [ { \"name\": \"opal-audit-file-example\", \"dependencies\": [ \"opal-audit-example-common\" ] } ] Adding extensions to a config In the configuration you want to deploy your extension with, update the /configs/<config_name>/configuration/extension-references.json file and add the directory name for your extension in the extensions array. For example, to add the extension in i2a-extensions/opal-default-security-example the extension-references.json file contains the following extensions array: { \"extensions\": [ { \"name\": \"opal-default-security-example\", \"version\": \"1.0.0\" } ] } Note: The extensions in the extension-references.json file are loaded in order. If an extension depends on another extension, define the dependent extension first. Deploying your extension Use the deploy.sh script to build the extensions for your configuration. For example, to build and deploy the extensions declared in extension-references.json for your configuration: ./deploy.sh -c <config_name> Debugging your extension Deploying in debug mode To deploy with one or more Liberty servers running in debug mode, change the local variable DEBUG_LIBERTY_SERVERS in utils/internalHelperVariables.sh to be a list of the container names of the liberty servers you wish to run in debug mode. For example: DEBUG_LIBERTY_SERVERS=(\"${LIBERTY1_CONTAINER_NAME}\") Then deploy from a clean environment. When the Liberty servers start they all wait for a debugger to be attached before continuing. Note: Each Liberty server will open and expose a unique debug port (starting at port 7777) and wait for a debugger. This behavior is defined by the WLP_DEBUG_SUSPEND environment variable in the liberty container and could be changed if needed. Attaching a debugger The project is shipped with a default launch configuration which can be changed in .vscode/launch.json . Run the Java debugger by pressing F5 and choose Debug Extensions if asked."
  },
  "content/develop_monitoring.html": {
    "href": "content/develop_monitoring.html",
    "title": "Configuring Prometheus and Grafana",
    "keywords": "Configuring Prometheus and Grafana The config dev environment includes Prometheus and Grafana containers that are configured to monitor the deployment of i2 Analyze. You can use the config dev environment to configure the general Prometheus and Grafana configuration. For example you can modify the alerts that can be sent from Prometheus or modify the dashboards in Grafana. Configuring Prometheus The scrape configurations for i2 Analyze and Liberty are configured for you in the config dev environment. This allows you to focus on configuring the behavior of Prometheus. For more information about what you can configure, see Prometheus Docs . To modify the Prometheus configuration, edit the configs/<config-name>/configuration/prometheus/prometheus.yml file. To update the deployment with the configuration, run the following command from the scripts directory: ./deploy.sh -c <config_name> To view the Prometheus UI, connect to https://prometheus.eia:9090 . Log in using the user name prometheus and the password from the dev-environment-secrets/generated-secrets/prometheus/admin_PASSWORD file. Configuring Grafana dashboards In the config dev environment, Prometheus is already configured as a data source in Grafana. This allows you to focus on configuring the Grafana dashboards. To view the Grafana UI, connect to https://grafana.eia:3500/dashboards . Log in using the user name grafana and the password from the dev-environment-secrets/generated-secrets/grafana/admin_PASSWORD file. In the UI you can create or modify dashboards. For more information about Grafana dashboards, see About Grafana dashboards . After you create or modify a dashboard, export it and copy the JSON file into the configs/<config-name>/configuration/grafana/dashboards folder. For more information, see Exporting a dashboard To update the deployment with the modified dashboards, run the following command from the scripts directory: ./deploy.sh -c <config_name>"
  },
  "content/develop_schemas.html": {
    "href": "content/develop_schemas.html",
    "title": "Developing schemas in your configuration development environment",
    "keywords": "Developing schemas in your configuration development environment To reduce the time that it takes to update the environment with your schema changes, use the schema_dev deployment pattern. Note: In the analyze-containers repository, the structure of the configuration directory is modified. The configuration files are in the top level configs/<config_name>/configuration directory. You cannot run any toolkit tasks described in the linked documentation in this environment. Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to schema_dev . For example: DEPLOYMENT_PATTERN=\"schema_dev\" Developing the data model In i2 Analyze Schema Designer, modify the schema.xml and schema-charting-scheme.xml files in the <config_name>\\configuration directory with your changes. For more information about developing your schema, see Updating the schema and the charting scheme . On Windows, open the file from your mapped network drive. After you make your changes, run the deploy.sh script again with your config name. Test your changes in Analyst's Notebook Premium. Note: You might need to log out of Analyst's Notebook Premium and log back in to see your changes. Developing security model Update the security-schema.xml file in the <config_name>\\configuration directory with your changes. For more information about developing your security schema, see Configuring the security schema . After you make your changes, run the deploy.sh script again with your config name. Update the user-registry.xml file to align to your security schema changes. For more information about updating the user registry, see Configuring the Liberty user registry . Note: In this release, you must ensure that your user registry file contains the user Jenny in the Administrator group. After you make your changes, run the deploy.sh script again with your config name. Update the command-access-control.xml file to align to your users and groups. For more information about controlling access to features, see The command access control file . After you make your changes, run the deploy.sh script again with your config name. Test your changes in Analyst's Notebook Premium. Note: You might need to log out of Analyst's Notebook Premium and log back in to see your changes. After you develop your schemas, Add data to your config development environment ."
  },
  "content/develop_solr_config_dev.html": {
    "href": "content/develop_solr_config_dev.html",
    "title": "Configuring the Solr index",
    "keywords": "Configuring the Solr index The way that Solr indexes data can be configured using the files in the /configs/<config_name>/configuration/solr directory. The deployed Solr configuration is built from the template schema-template-definition.xml file and other files referenced within it. Any reference files must also be in the configuration/solr directory. For more information about the changes you can make to the schema-template-definition.xml file, see Configuring search . Note: The process you complete to update a deployment with the Solr configuration is different when the deployment contains data. If the deployment does not contain data, you clean the deployment and then update the deployment in the usual way If the deployment contains data, the process requires you to back up and restore the database Updating a deployment without data After you change the Solr configuration, clean the deployment by running the deploy.sh script with the clean task. For example: ./deploy.sh -c <config_name> -t clean For more information about the script, see The deploy.sh script . Then, re-deploy in the usual way. For example: ./deploy.sh -c <config_name> Your deployment is running with the updated Solr configuration. Updating a deployment with data When data is present in the system you must use the backup and restore tasks of the deploy script to update the deployment. For more information about these tasks, refer to refer to Back up and restore a development database . After you change the Solr configuration, backup the deployment. For example: ./deploy.sh -c <config_name> -t backup -b <backup_name> Then, restore from the backup that you created. For example: ./deploy.sh -c <config_name> -t restore -b <backup_name> Solr is updated with your configuration, and the data in the database is reindexed when the system starts."
  },
  "content/getting_started.html": {
    "href": "content/getting_started.html",
    "title": "Getting started with the analyze-containers repository",
    "keywords": "Getting started with the analyze-containers repository Prerequisites You can run the code in the analyze-containers repository on Windows with Windows Subsystem for Linux and MacOS. Windows Subsystem for Linux (WSL) If you are on Windows , you must use WSL 2 as the backend for Docker and to run the shell scripts in this repository. To install WSL, in an administrator Command Prompt, run: wsl --install Then, restart your machine. For information about installing WSL, see Install Linux on Windows with WSL . Configure your WSL user. For more information, see Set up your Linux username and password . Create a mapped network drive in Windows for WSL. In a command prompt, run the following command to list your WSL distributions. wsl --list Run the following command to map your WSL distribution to the Z: drive. Use the name of your default WSL distribution as displayed after running the previous command. You can use any drive letter. net use Z: \\\\wsl$\\Ubuntu You can now access the WSL filesystem from Windows Docker Install Docker CE for your operating system. For more information about installing Docker CE, see https://docs.docker.com/engine/installation/ . Mac OS : Install Docker CE Windows : Install Docker CE Set up Docker on WSL 2 After you install Docker, allocate at least 5GB of memory to Docker to run the containers in the example deployment. On Windows, Docker is automatically allocated 8GB or 50% of available memory whichever is less. For more information about modifying the resources allocated to Docker on MacOS, see: Docker Desktop for Mac Code Download the tar.gz from https://github.com/i2group/analyze-containers/releases/tag/v2.3.0 . Extract the tar.gz file by using the following steps: On Windows, copy the analyze-containers-2.3.0.tar.gz file to your WSL filesystem. For example: Z:\\home\\<user> Extract the tar.gz file (On Windows, run the commands from a WSL terminal. Start > wsl ): cd /home/<user> tar -xvzf analyze-containers-2.3.0.tar.gz mv analyze-containers-2.3.0/ analyze-containers i2 Analyze minimal toolkit Download the i2 Analyze V4.4.0 Minimal for Linux. To download i2 Analyze, follow the procedure described in Where can I download the latest i2 Products? Populate the subject of the form with Request for i2 Analyze 4.4.0 minimal toolkit for Linux . Rename the i2analyzeMinimal_440.tar.gz file to i2analyzeMinimal.tar.gz , then copy it to the analyze-containers/pre-reqs directory. Note: If you used the analyze-containers repository with a previous version of i2 Analyze, overwrite the existing minimal toolkit. i2 Analyze Fix Packs (optional) If there is a Fix Pack available for the version of i2Analyze, download it and rename the file to i2analyzeFixPack.tar.gz , then copy it to the analyze-containers/pre-reqs directory. Note: If you used the analyze-containers repository with a previous fix pack version of i2 Analyze, overwrite the existing fix pack file. Analyst's Notebook Premium Download i2 Analyst's Notebook Premium version 9.4.0. To download i2 Analyst's Notebook Premium, follow the procedure described in Where can I download the latest i2 Products? Populate the subject of the form with Request for i2 Analyst's Notebook Premium 9.4.0 . Install i2 Analyst's Notebook Premium on a Windows machine. Note: If you are running Docker on MacOS, you can install Analyst's Notebook Premium on a Windows virtual machine. For more information, see Installing i2 Analyst's Notebook Premium . JDBC drivers You must provide the JDBC driver to enable the application to communicate with the database. Download the Microsoft JDBC Driver 9.4.1 for SQL Server from https://github.com/microsoft/mssql-jdbc/releases/tag/v9.4.1 by clicking the mssql-jdbc-9.4.1.jre11.jar asset. Copy the mssql-jdbc-9.4.1.jre11.jar file to the analyze-containers/pre-reqs/jdbc-drivers directory. Modifying the hosts file To enable you to connect to a deployment, the Solr Web UI, the database, the Prometheus Web UI and grafana, update your hosts file to include entries for the containers: 127.0.0.1 solr1.eia 127.0.0.1 sqlserver.eia 127.0.0.1 i2analyze.eia 127.0.0.1 prometheus.eia 127.0.0.1 grafana.eia On Windows, edit the hosts file in C:\\Windows\\System32\\drivers\\etc . On MacOS, edit the hosts file in /etc/hosts . If you deploy on MacOS, for your virtual machine to connect to i2 Analyze, complete the following: On your MacOS terminal, run ifconfig and identify the IP address for your virtual machine in a section such as vmnet1 . For example, 172.16.100.1 . Then, on your Windows virtual machine add the following line to the C:\\Windows\\System32\\drivers\\etc\\hosts file: 172.16.100.1 i2analyze.eia Visual Studio Code The repository is designed to be used with VS Code to create the development environment. Download and install VS Code When prompted to Select Additional Tasks during installation, be sure to check the Add to PATH option so you can easily open a folder in WSL using the code command. Install the Remote Containers extension On Windows, use the following instructions to open the folder in WSL. Install the Remote WSL extension Press F1 and type Remote-WSL: New WSL Window and select it. Press F1 (or Cmd+Shift+P in MacOS) and type Remote-Containers: Open Folder in Container and select it. In the file explorer, navigate to your analyze-containers directory. For example: /home/<user-name>/analyze-containers . If you are prompted, click Trust Folder & Continue . After the dev container starts, if you are prompted, click Install in the pop-up that is displayed that prompts you to install the recommended VS Code extensions. Your config dev environment is correctly opened when the following is displayed in the bottom left-hand corner of the VS Code window: For more information about VS Code dev containers, see Developing in a container . To run the scripts in the analyze-containers repository, use the VS Code integrated terminal. To open the integrated terminal, click Terminal > New Terminal . What to do next Create and use a development environment to develop an i2 Analyze configuration. For more information, see Configuration development environment . Create an example pre-production deployment that is used to demonstrate how i2 Analyze can be deployed in a distributed cloud environment. For more information, see Pre-production example environment . To understand how the containerized environment is created, you can review the documentation that explains the images, containers, tools, and functions: Images and containers Tools and functions"
  },
  "content/HA walkthroughs/ha.html": {
    "href": "content/HA walkthroughs/ha.html",
    "title": "High availability walkthroughs",
    "keywords": "High availability walkthroughs The high availability walkthroughs are designed to demonstrate how a deployment of i2 Analyze responds to container failure in a containerized environment."
  },
  "content/HA walkthroughs/liberty_failure.html": {
    "href": "content/HA walkthroughs/liberty_failure.html",
    "title": "Failure of the leader Liberty container",
    "keywords": "Failure of the leader Liberty container This section demonstrates how a deployment of i2 Analyze responds to the failure and recovery of the Liberty server that hosts the leader Liberty instance. This section also describes the messages that you should monitor to detect the failure and ensure that the recovery was successful. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How Liberty is deployed for high availability and the role of the Liberty leader. For more information about Liberty configuration, see Liberty . How a load balancer is used and configured in a deployment of i2 Analyze. For more information about the load balancer configuration, see Deploying a load balancer . That the load balancer is used to monitor the status of the i2 Analyze service. In a containerized deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The libertyHadrFailureWalkthrough.sh script simulates the Liberty leader failure. Identifying the leader Liberty To simulate a failure of the leader Liberty, first identify which server hosts the leader instance. To identify which container is running the leader Liberty, check the logs of the Liberty servers for the message: We are the Liberty leader . In the walkthrough script, this is done with the following grep command: grep -q \"We are the Liberty leader\" See the Identifying the leader Liberty section of the walkthrough script. Simulating leader Liberty failure To simulate leader Liberty failure, stop the leader Liberty. For example, if liberty1 is the leader, run: docker stop liberty1 See the Simulating leader Liberty failure section of the walkthrough script. Detecting failure The load balancer is used to monitor and determine the status of the i2 Analyze service. The load balancer is configured to report the status of the deployment. The status can be either ACTIVE , DEGRADED , or DOWN . When the leader is taken offline, the other Liberty server must restart to become the new leader. During this time, both servers are down and the i2 Analyze service is DOWN . When the new leader Liberty starts and only 1 of the servers is down, the status of the i2 Analyze services is DEGRADED . In the walkthrough, the waitFori2AnalyzeServiceStatus function is used to run a while loop around the geti2AnalyzeServiceStatus function to wait until the i2 Analyze service is in the DEGRADED state. The geti2AnalyzeServiceStatus function is an example of how to return the i2 Analyze service status from a load balancer. See the Detecting failure section of the walkthrough script. Fail over When the Liberty leader fails, one of the remaining Liberty servers is elected as the leader. To identify the new Liberty leader, check the logs of the remaining Liberty servers for the message: We are the Liberty leader . See the Fail over section of the walkthrough script. Reinstating high availability To reinstate high availability to the deployment, restart the failed Liberty server. In this example, that restart the Liberty container by running the following command: docker start liberty1 When both Liberty servers are up, the status of the i2 Analyze services is ACTIVE . In the walkthrough, the waitFori2AnalyzeServiceStatus function is used to run a while loop around the geti2AnalyzeServiceStatus function to wait until the i2 Analyze service is in the ACTIVE state. The geti2AnalyzeServiceStatus function is an example of how to return the i2 Analyze service status from a load balancer. The recovered Liberty server is in the non-leader mode when it starts because the new leader has already been elected while the server was unavailable. To determine it is in the non-leader mode, the following message is displayed in the logs: We are not the Liberty leader . See the Reinstating high availability section of the walkthrough script."
  },
  "content/HA walkthroughs/solr_cluster_failure.html": {
    "href": "content/HA walkthroughs/solr_cluster_failure.html",
    "title": "Failure of the Solr cluster",
    "keywords": "Failure of the Solr cluster This section demonstrates how a deployment of i2 Analyze responds to the failure of all Solr nodes. This section also describes how to monitor and detect failure and ensure the recovery was successful. Before you begin the walkthrough, there a number of concepts that it is useful to understand: How Solr is deployed for high availability. For more information, see Solr . How the Solr status is reported in the component availability log in Liberty. For more information, see Monitor the system availability . In a containerized deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The solrClusterFailureWalkthrough.sh scripts simulates a Solr cluster failure and recovery. Simulating Solr Cluster failure To simulate the cluster failure, remove all the Solr containers. For example, run: docker stop solr2 solr1 See the Simulating the cluster failure section of the walkthrough script. Detecting failure The component availability log in Liberty is used to monitor and determine the status of the Solr cluster. When the Solr cluster is unavailable, the status is reported as DOWN . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'DOWN'\" Reinstating high availability To reinstate high availability, restart the failed Solr containers. In this example, restart both Solr containers by running the following command: docker start solr2 solr1 After the failed nodes recover, Liberty reports the changes to the cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. The reinstating high availability section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is active: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'ALL_REPLICAS_ACTIVE'\""
  },
  "content/HA walkthroughs/solr_node_failure.html": {
    "href": "content/HA walkthroughs/solr_node_failure.html",
    "title": "Failure of a Solr node",
    "keywords": "Failure of a Solr node This walkthrough demonstrates losing a Solr node from a collection, and describes how to identify failure, continue operations, and reinstate high availability with your Solr nodes. Before you begin the walkthrough, there a number of concepts that it is useful to understand: How Solr is deployed for high availability. For more information, see Solr . How the Solr status is reported in the component availability log in Liberty. For more information, see Monitor the system availability . In a containerized deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The solrNodeFailureWalkthrough.sh script demonstrates how to monitor the Liberty logs to identify Solr node failure and recovery. In the example, each shard has 2 replicas and 1 replica is located on each Solr node. This means that the Solr cluster can continue to process requests when one Solr node is taken offline. Simulating Solr node failure To simulate a node failure, one of the Solr containers is stopped in the Stop the Solr container section. For example: docker stop solr2 See the Simulating the cluster failure section of the walkthrough script. Detecting failure The component availability log in Liberty is used to monitor and determine the status of the Solr cluster. When the Solr Node is unavailable, the status is reported as DEGRADED . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'DEGRADED'\" Reinstating high availability To reinstate high availability, restart the failed Solr containers. In this example, restart the Solr by running the following command: docker start solr1 After the failed node recovers, Liberty reports the changes to the cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. The reinstating high availability section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is active: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'ALL_REPLICAS_ACTIVE'\""
  },
  "content/HA walkthroughs/zookeeper_quorum_failure.html": {
    "href": "content/HA walkthroughs/zookeeper_quorum_failure.html",
    "title": "Loss of the ZooKeeper quorum",
    "keywords": "Loss of the ZooKeeper quorum This walkthrough demonstrates losing more than 50% of the ZooKeeper servers from an ensemble, which causes the loss of the quorum. It also describes how to identify failure, and reinstate high availability with your ZooKeeper ensemble. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How ZooKeeper is deployed for high availability and how the ensemble functions when one or more ZooKeeper servers fail. For more information, see Configuring ZooKeeper for HADR . When the ZooKeeper quorum is lost, the Solr cluster also fails. To monitor that whether the ZooKeeper quorum is met or not the component availability log in Liberty is used. The status of Solr is used to determine the status of ZooKeeper. In a containerized deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. Simulating loss of quorum To simulate a loss of quorum, more than 50% of the ZooKeeper servers must be stopped. For example, to stop zk1 and zk2 , run: docker stop zk1 zk2 See the Simulating Zookeeper Quorum failure section of the walkthrough script. Detecting failure When the ZooKeeper quorum is lost, the Solr cluster also fails. The Solr status is reported as DOWN . For more information about losing Solr, see Failure of the Solr cluster . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep -q \"^.*[com.i2group.apollo.common.toolkit.internal.ConsoleLogger] - (opal-services) - '.*', .*'DOWN'\" For more information see the Detecting failure section of the walkthrough script. Reinstating high availability To reinstate high availability to the deployment, restart the failed ZooKeeper servers. In this example, restart the ZooKeeper containers by running the following command: docker start zk1 zk2 When enough ZooKeeper servers are up to achieve at least a DEGRADED quorum, the status of the i2 Analyze services is ACTIVE and Liberty reports the changes to the Solr cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. See the Reinstating high availability section of the walkthrough script."
  },
  "content/HA walkthroughs/zookeeper_server_failure.html": {
    "href": "content/HA walkthroughs/zookeeper_server_failure.html",
    "title": "Failure of a ZooKeeper server",
    "keywords": "Failure of a ZooKeeper server This walkthrough demonstrates losing a single ZooKeeper server from an ensemble, and describes how to identify failure, continue operations, and reinstate high availability with your ZooKeeper ensemble. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How ZooKeeper is deployed for high availability. For more information, see Multi-server setup . The ZooKeeper AdminServer is used to monitor the status of the ZooKeeper ensemble The AdminServer . In the zookeeperServerFailureWalkthrough.sh script demonstrates stopping one of the ZooKeeper containers, monitoring the status, and reinstating high availability. Simulating a server failure To simulate a server failure in the ensemble, one of the ZooKeeper servers is stopped. For example, to stop zk1 , run: docker stop zk1 See the Simulating ZooKeeper server failure section of the walkthrough script. Detecting failure When one ZooKeeper server goes offline, the other servers can still make a quorum and remain active. Because the ensemble can sustain only one more server failure, the state is defined as DEGRADED . In the walkthrough, the getZkQuorumEnsembleStatus function is used to monitor and determine the ensemble status by calling the commands/srvr resource on each ZooKeeper servers's admin endpoint and reports the status as DEGRADED when one of the servers is unavailable. See the Detecting failure section of the walkthrough script. Restoring high availability To restore high availability to the ensemble, restart the failed ZooKeeper server. In this example, restart the ZooKeeper container by running the following command: docker start zk1 When the ZooKeeper server is up again, the status of the ensemble is ACTIVE . In the walkthrough, the getZkQuorumEnsembleStatus function is used again to determine the ensemble status. See the Reinstating high availability section of the walkthrough script."
  },
  "content/images and containers/etl_client.html": {
    "href": "content/images and containers/etl_client.html",
    "title": "ETL Client",
    "keywords": "ETL Client An ETL Client container is an ephemeral container that is used to run ETL tasks. Building an ETL Client image The ETL Client image is built from the Dockerfile in images/etl_client . It uses the i2 Analyze Tools image as the base image. Docker build command The following docker build command builds the ETL Client image: docker build -t \"etlclient_redhat:4.4.0\" \"images/etl_client\" \\ --build-arg BASE_IMAGE=\"i2a_tools_redhat:4.4.0\" The --build-arg flag is used to provide your local user ID to the Docker image when it is built. The value of $USER comes from your shell. For examples of the build commands, see the buildImages.sh script. Running an ETL Client container A ETL Client container uses the ETL Client image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs the ETL Client container: docker run --rm \\ --name \"etl_client\" \\ --network \"eia\" \\ -v \"examples/pre-prod/configuration/logs:/opt/configuration/logs\" \\ -v \"prereqs/i2analyze/toolkit/examples/data:/var/i2a-data\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=\"1433\" \\ -e DB_NAME=\"ISTORE\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_OS_TYPE=\"UNIX\" \\ -e DB_INSTALL_DIR=\"/opt/mssql-tools\" \\ -e DB_LOCATION_DIR=\"/var/opt/mssql/data\" \\ -e ETL_TOOLKIT_JAVA_HOME=\"/opt/java/openjdk/bin\" \\ -e DB_USERNAME=\"i2etl\" \\ -e DB_PASSWORD=\"DB_PASSWORD\" \\ -e DB_SSL_CONNECTION=\"true\" \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"etlclient_redhat:4.4.0\" \"$@\" For an example of the docker run command, see runEtlToolkitToolAsi2ETL function in the clientFunctions.sh script. For an example of how to use runEtlToolkitToolAsi2ETL function, see runEtlToolkitToolAsi2ETL . Bind mounts Logs : The ETL Client container generates logs when running ingestion commands. To make these logs easily accessible, the directory where they are generated must be mounted into the container. In the example, the logs can be accessed in the examples/pre-prod/configuration/logs directory. Data : The ETL Client container requires access to the data directory to run the ingestion scripts. To access the data, the data directory must be mounted into the container. Environment variables The ETl Client is built on top of the SQL Client. Any environment variables referenced in the SQL Client can be used in the ETL Client. Additional Environment variables Environment Variable Description DB_DIALECT The database dialect. Currently only sqlserver is supported DB_OS_TYPE The Operating System that the database is on. Can be UNIX , WIN , or AIX . DB_INSTALL_DIR Specifies the database CMD location. DB_LOCATION_DIR Specifies the location of the database. ETL_TOOLKIT_JAVA_HOME Specifies the location of Java bin directory. Useful links Defining an ingestion source"
  },
  "content/images and containers/grafana.html": {
    "href": "content/images and containers/grafana.html",
    "title": "Grafana",
    "keywords": "Grafana In a containerized deployment, Grafana is configured and run from a Grafana image. Configuring Grafana Grafana is configured by environment variables used in the docker run command. For more information about the configuration, see Configure a Grafana Docker image . Running a Grafana container A Grafana container uses the out-of-the-box Grafana image. In the docker run command, you can use -e to pass environment variables to Grafana on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Grafana container: docker run -d \\ --name \"grafana\" \\ --net \"eia\" \\ --net-alias \"grafana.eia\" \\ -p \"3500:3000\" \\ -v \"grafana_data:/var/lib/grafana\" \\ -v \"grafana_dashboards:/etc/grafana/dashboards\" \\ -v \"grafana_provisioning:/etc/grafana/provisioning\" \\ -v \"grafana_secrets:/run/secrets\" \\ -e GF_SECURITY_ADMIN_USER=\"grafana\" \\ -e GF_SECURITY_ADMIN_PASSWORD=\"GF_SECURITY_ADMIN_PASSWORD\" \\ -e GF_SERVER_PROTOCOL=\"https\" \\ -e GF_SERVER_CERT_FILE=\"/run/secrets/server.cer\" \\ -e GF_SERVER_CERT_KEY=\"/run/secrets/server.key\" \\ -e PROMETHEUS_URL=\"https://prometheus.eia:9090\" \\ -e PROMETHEUS_USERNAME=\"prometheus\" \\ -e PROMETHEUS_PASSWORD=\"prometheus\" \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"grafana/grafana-oss:8.5.3\" For an example of the docker run command, see serverFunctions.sh . The runGrafana function does not take any arguments. Volumes A named volume or a bind mount can be used to persist data and logs that are generated and used in the Grafana container. To configure the Grafana container to use the volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For Grafana, the directory that must be mounted is /var/lib/grafana . For example: -v \"grafana_data:/var/lib/grafana\" \\ -v \"grafana_dashboards:/etc/grafana/dashboards\" \\ -v \"grafana_provisioning:/etc/grafana/provisioning\" \\ -v \"grafana_secrets:/run/secrets\" \\ For more information, see Run Grafana container with persistent storage . Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access prometheus and certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as GF_SECURITY_ADMIN_USER__FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure Grafana, you can provide environment variables to the Docker container in the docker run command. Grafana environment variables All the configuration options in Grafana can be overridden by environment variables like GF_<SectionName>_<KeyName> . For more information about Grafana environment variables, see Override configuration with environment variables . Environment variable Description GF_SECURITY_ADMIN_USER The Grafana administrator user name. GF_SECURITY_ADMIN_PASSWORD The Grafana administrator password. Grafana SSL The following environment variables enable you use SSl with Grafana Environment variable Description GF_SERVER_PROTOCOL The Grafana server protocol. GF_SERVER_CERT_FILE The Grafana administrator password. GF_SERVER_CERT_KEY The Grafana administrator password. Prometheus authentication The following environment variables are used to configure Grafana to connect to Prometheus as a client: Environment variable Description PROMETHEUS_USERNAME The Prometheus username. PROMETHEUS_PASSWORD The Prometheus password. SSL_CA_CERTIFICATE See Secure Environment variables ."
  },
  "content/images and containers/i2analyze_tool.html": {
    "href": "content/images and containers/i2analyze_tool.html",
    "title": "i2 Analyze Tool",
    "keywords": "i2 Analyze Tool An i2 Analyze Tool container is an ephemeral container that is used to run the i2 Analyze tools. For more information about the tools, see i2 Analyze tools . Building the i2 Analyze Tool image The i2 Analyze Tool image is built from the Dockerfile in images/i2a_tools . The image contains the i2-tools & scripts folder from the toolkit. Docker build command The following docker build command builds the i2 Analyze Tool image: docker image build -t \"i2a_tools_redhat:4.4.0\" \"images/i2a_tools\" \\ --build-arg USER_UID=\"$(id -u \"${USER}\")\" The --build-arg flag is used to provide your local user ID to the Docker image when it is built. The value of $USER comes from your shell. The local user ID is required so that a user is created in the Docker container with the same user ID as the local user. The user is required to ensure that the local user can access any files that are generated on the container and mounted to the host via a bind mount. For examples of the build commands, see buildImages.sh script. Running an i2 Analyze Tool container An i2 Analyze Tool container uses the i2 Analyze Tool image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs an i2 Analyze Tool container: docker run \\ --rm \\ --name \"i2atool\" \\ --network \"eia\" \\ --user \"$(id -u \"${USER}\"):$(id -u \"${USER}\")\" \\ -v \"examples/pre-prod/configuration:/opt/configuration\" \\ -v \"examples/pre-prod/database-scripts/generated:/opt/databaseScripts/generated\" \\ -e LIC_AGREEMENT=\"ACCEPT\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_NAME=\"ISTORE\" \\ -e CONFIG_DIR=\"/opt/configuration\" \\ -e GENERATED_DIR=\"/opt/databaseScripts/generated\" \\ -e DB_USERNAME=\"dba\" \\ -e DB_PASSWORD=\"DBA_PASSWORD\" \\ -e DB_OS_TYPE=\"UNIX\" \\ -e DB_INSTALL_DIR=\"/opt/mssql-tools\" \\ -e DB_LOCATION_DIR=\"/var/opt/mssql/data\" \\ -e SOLR_ADMIN_DIGEST_USERNAME=\"solr\" \\ -e SOLR_ADMIN_DIGEST_PASSWORD=\"SOLR_ADMIN_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD=\"ZOO_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD=\"ZOO_DIGEST_READONLY_PASSWORD\" \\ -e DB_SSL_CONNECTION=\"true\" \\ -e SOLR_ZOO_SSL_CONNECTION=\"true\" \\ -e SSL_PRIVATE_KEY=\"SSL_PRIVATE_KEY\" \\ -e SSL_CERTIFICATE=\"SSL_CERTIFICATE\" \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"i2a_tools_redhat:4.4.0\" \"$@\" Bind mounts Configuration : The i2 Analyze Tool container requires the i2 Analyze configuration. To access the configuration, the configuration directory must be mounted into the container. The CONFIG_DIR environment variable must specify the location where the configuration is mounted. Generated scripts directory : Some of the i2 Analyze tools generate scripts to be run against the Information Store database, or run scripts that were generated by other i2 Analyze tools. For the i2 Analyze Tool container to interact with these scripts, the directory where they are generated must be mounted into the container. In the example scripts, this is defaulted to /database-scripts/generated . The GENERATED_DIR environment variable must specify the location where the generated scripts are mounted. This directory must be created with your local user before mounting it to the container, otherwise docker will create this directory as a root. Environment variables To configure the i2 Analyze Tool container, you provide environment variables to the Docker container in the docker run command. The following table describes the supported environment variables that you can use: Environment variable Description LIC_AGREEMENT The license agreement for use the i2Analyze tools. ZK_HOST The connection string the ZooKeeper client to connect to. DB_DIALECT See i2a Tools Environment Variables . DB_SERVER See i2a Tools Environment Variables . DB_PORT See i2a Tools Environment Variables . DB_NAME See i2a Tools Environment Variables . CONFIG_DIR The root location of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. CLASSES_PATH The location to the files required by the i2 Analyze tools. The files are built into the image. DB_USERNAME See i2a Tools Environment Variables . DB_PASSWORD See i2a Tools Environment Variables . SOLR_ADMIN_DIGEST_USERNAME The name of the administrator user for performing administration tasks. SOLR_ADMIN_DIGEST_PASSWORD The password for the administrator user. ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. The following environment variables enable you to use SSL: Environment variable Description SERVER_SSL See Secure Environment variables . DB_SSL_CONNECTION See Secure Environment variables . SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SSL_PRIVATE_KEY See Secure Environment variables . SSL_CERTIFICATE See Secure Environment variables . SSL_CA_CERTIFICATE See Secure Environment variables . GATEWAY_SSL_CONNECTION See Secure Environment variables . SSL_OUTBOUND_PRIVATE_KEY See Secure Environment variables . SSL_OUTBOUND_CERTIFICATE See Secure Environment variables . NOTE: when you set SERVER_SSL and SSL_CA_CERTIFICATE environment variables, the CA.cer certificate will be located at /tmp/i2acerts/CA.cer . when you set GATEWAY_SSL_CONNECTION , SSL_OUTBOUND_PRIVATE_KEY and SSL_OUTBOUND_CERTIFICATE environment variables, the certificated will be located at /tmp/i2acerts/i2Analyze.pem ."
  },
  "content/images and containers/images.html": {
    "href": "content/images and containers/images.html",
    "title": "Images and containers",
    "keywords": "Images and containers The documentation in this section describes the images and containers that are required for a deployment of i2 Analyze in a containerized environment."
  },
  "content/images and containers/liberty.html": {
    "href": "content/images and containers/liberty.html",
    "title": "Liberty",
    "keywords": "Liberty In a containerized deployment, you configure the i2 Analyze application and Liberty in an image that is layered on top of the liberty_ubi_base image. The liberty_ubi_base contains static configuration and application jars that are required by i2 Analyze and should not be changed. The liberty_ubi_base image is built on top a Liberty image maintained by i2 Group on Docker Hub. Building a Liberty base image The Liberty base image for i2 Analyze is built on top of a Liberty image maintained by i2 Group on Docker Hub. Docker build command The base image is built from the Dockerfile in images/liberty_ubi_base . The following docker build command builds the Liberty base image: docker build -t \"liberty_ubi_base:4.4.0\" images/liberty_ubi_base \\ --build-arg I2ANALYZE_VERSION=\"4.4.0\" For examples of the build commands, see buildImages.sh script. Configuring the Liberty server Liberty is configured by exception. The runtime environment operates from a set of built-in configuration default settings, and you only need to specify configuration that overrides those default settings. You do this by editing either the server.xml file or another XML file that is included in server.xml at run time. In a containerized deployment of i2 Analyze, a server.xml file is provided for you. To provide or modify any values, you specify a number of environment variables when you run a Liberty container. Additionally, you can extend the server.xml by using the provided server.extensions.xml in the i2 Analyze configuration. Any elements that you add to the extensions file are included in the server.xml when you run a Liberty container. Configuring the i2 Analyze application The contents of the configuration directory must be copied into the images/liberty_ubi_combined/classes directory. The contents of the classes directory is added to the configured Liberty image when the image is built. If you make changes to the configuration, you must copy the changes to the classes directory and rebuild the configured image. Note: The system match rules are configured differently. The application is updated to use the system-match-rules.xml from the Solr client command line. For more information about updating the system match rules, see Updating the system match rules . Building a configured Liberty image The configured image is built from the base image. The configured image contains the i2 Analyze application and Liberty configuration that is required to start the i2 Analyze application. When you change the configuration, the configured image must be rebuilt to reflect the changes. Docker build command The configured image is built from the Dockerfile in images/liberty_ubi_combined . The following docker build command builds the configured image: docker build -t \"liberty_configured_redhat:4.4.0\" \"images/liberty_ubi_combined\" \\ --build-arg BASE_IMAGE=\"liberty_redhat:4.4.0\" An example of providing the configuration to the classes directory and building the image is included in the buildLibertyConfiguredImage function in the serverFunctions.sh script. Running a Liberty container A Liberty container uses the configured image. In the docker run command, you can use -e to pass environment variables to Liberty on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Liberty container: docker run -m 1g -d \\ --name \"liberty1\" \\ --network \"eia\" \\ --net-alias \"liberty1.eia\" \\ -p \"9045:9443\" \\ -v \"liberty1_secrets:/run/secrets\" \\ -v \"liberty1_data:/data\" \\ -e LICENSE=\"accept\" \\ -e FRONT_END_URI=\"https://liberty.eia:9045/opal\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_USERNAME=\"i2analyze\" \\ -e DB_PASSWORD_FILE=\"/run/secrets/DB_PASSWORD\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_PASSWORD\" \\ -e SOLR_HTTP_BASIC_AUTH_USER=\"liberty\" \\sp -e SOLR_HTTP_BASIC_AUTH_PASSWORD_FILE=\"/run/secrets/SOLR_APPLICATION_DIGEST_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ -e SSL_CA_CERTIFICATE_FILE=\"/run/secrets/CA.cer\" \\ -e GATEWAY_SSL_CONNECTION=true \\ -e SSL_OUTBOUND_PRIVATE_KEY_FILE=\"/run/secrets/gateway_user.key\" \\ -e SSL_OUTBOUND_CERTIFICATE_FILE=\"/run/secrets/gateway_user.cer\" \\ -e SSL_OUTBOUND_CA_CERTIFICATE_FILE=\"/run/secrets/outbound_CA.cer\" \\ -e LIBERTY_HADR_MODE=1 \\ -e LIBERTY_HADR_POLL_INTERVAL=1 \\ \"liberty_configured_redhat\" For an example of the docker run command, see serverFunctions.sh . The runLiberty function takes the following arguments to support running multiple Liberty containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container and the Solr host. VOLUME - the name for the named volume of the Liberty container. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. KEY_FOLDER - The folder with keys and certificates for the container. For more information, see Security . An example of running Liberty container by using runLiberty function: runLiberty liberty1 liberty1.eia liberty1_data 9045 liberty1 Volumes A named volume or a bind mount can be used to persist data, which is generated and used in the Liberty container, outside of the container. To configure the Liberty container to use the volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For Liberty, the directory that is mounted must be /data , this directory folder stores: jobs, record groups and charts. For example: -v liberty_data:/data \\ -v liberty1_secrets:/run/secrets Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access ZooKeeper, the database, and the certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure the Liberty server, you provide environment variables to the Docker container in the docker run command. The following table describes the supported environment variables that you can use: Environment variable Description FRONT_END_URI The URI that clients use to connect to i2 Analyze. For more information, see Specifying the connection URI . DB_DIALECT Specifies which database management system to configure i2 Analyze for. In this release, it can be set to sqlserver . For more information, see properties.microsoft.sqlserver . DB_SERVER Specifies the fully qualified domain name of the database server to connect to. The value populates the serverName attribute in the Liberty server configuration. For more information, see properties.microsoft.sqlserver . DB_PORT Specifies the port number of the SQL Server database to connect to. The value populates the portNumber attribute in the Liberty server configuration. You can specify DB_PORT or DB_INSTANCE . For more information, see properties.microsoft.sqlserver . DB_USERNAME The database user that is used by Liberty to connect to the database. DB_PASSWORD The database user password. ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated. The connection string must be in the following format: <hostname>:<port>,<hostname>:<port> . SOLR_HTTP_BASIC_AUTH_USER The Solr user that Liberty uses to connect to Solr. This is not an administrator user. SOLR_HTTP_BASIC_AUTH_PASSWORD The Solr user password. ZOO_DIGEST_USERNAME The ZooKeeper user that is used by Liberty to connect to ZooKeeper. ZOO_DIGEST_PASSWORD The ZooKeeper user password. The following environment variables enable you to use SSL: Environment variable Description DB_SSL_CONNECTION See Secure Environment variables . SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . GATEWAY_SSL_CONNECTION See Secure Environment variables . SSL_OUTBOUND_PRIVATE_KEY_FILE See Secure Environment variables . SSL_OUTBOUND_CERTIFICATE_FILE See Secure Environment variables . SSL_OUTBOUND_CA_CERTIFICATE_FILE See Secure Environment variables . Liberty HADR You can run Liberty in an active/active configuration with multiple Liberty containers. In an active/active configuration, multiple instance of the i2 Analyze application run concurrently on multiple Liberty containers. One instance of the i2 Analyze application is determined to be the leader at any given time. The following tables describes the environment variables that you can use to configure HADR: Environment variable Description LIBERTY_HADR_MODE Can be set to 1 or 0. If set to 1, Liberty starts in HADR mode. The default is 0. LIBERTY_HADR_POLL_INTERVAL The interval in minutes to poll for liberty leadership status. The default is 5. LIBERTY_HADR_MAX_ERRORS The maximum number of errors allowed before Liberty initiates a leadership poll. The time span in which the errors can occur is determined by LIBERTY_HADR_ERROR_TIME_SPAN . The default is 5. LIBERTY_HADR_ERROR_TIME_SPAN The time span in seconds for the LIBERTY_HADR_MAX_ERRORS to occur within. The default is 30. <!-- markdownlint-configure-file { \"MD013\": false } -->"
  },
  "content/images and containers/prometheus.html": {
    "href": "content/images and containers/prometheus.html",
    "title": "Prometheus",
    "keywords": "Prometheus In a containerized deployment, Prometheus is configured and run from a Prometheus image. Configuring Prometheus Prometheus is configured by the prometheus.yml file. A prometheus.yml template is shipped with the product. To modify the prometheus.yml , you can modify the template inside your configuration folder. For more information about the file, see Configuration file . Running a Prometheus container A Prometheus container uses a Prometheus image maintained by i2 Group on Docker Hub. For more information about the command, see docker run reference . Docker run command The following docker run command runs a Prometheus container: docker run -d \\ --name \"prometheus\" \\ --net \"eia\" \\ --net-alias \"prometheus.eia\" \\ -p \"9090:9090\" \\ -v \"examples/pre-prod/prometheus/web-config.yml:/etc/prometheus/web-config.yml\" \\ -v \"examples/pre-prod/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml\" \\ -v \"prometheus_data:/prometheus\" \\ -v \"prometheus_secrets:/run/secrets\" \\ \"i2group/i2eng-prometheus:2.36\" For an example of the docker run command, see serverFunctions.sh . The runPrometheus function does not take any arguments. Volumes A named volume or a bind mount can be used to persist data and logs that are generated and used in the Prometheus container. To configure the Prometheus container to use the volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For Prometheus, the directory that must be mounted is /prometheus . For example: -v prometheus_data:/prometheus \\ -v prometheus_secrets:/run/secrets For more information, see Volumes & bind mount . Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access liberty and certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. In a production environment, the orchestration environment needs to provide the secrets to the file system. The mechanism that is used here simulates the orchestration system providing the secrets as files. Liberty authentication The prometheus.yml file contains a scrape config section with the user, password and certificates to communicate with Liberty. In our example the job name is liberty and the user Jenny is the administrator. For more information about configuring Prometheus to scrape targets, see <scrape_config> Prometheus SSL The web-config.yml file contains the configuration to secure Prometheus. For more information about configuring HTTPS and authentication in Prometheus, see HTTPS and authentication . Environment variables The following table describes the supported environment variables that you can use: Environment variable Description PROMETHEUS_USERNAME The Prometheus username. PROMETHEUS_PASSWORD The password used by the Prometheus user. LIBERTY_ADMIN_USERNAME The user that is used to access Liberty. LIBERTY_ADMIN_PASSWORD The password for the Liberty user. LIBERTY_SCHEME The URL scheme used to connect to Liberty. E.g. http or https . PROMETHEUS_SCHEME The URL scheme used to connect to Prometheus. E.g. http or https . Security environment variables: Environment variable Description SSL_PRIVATE_KEY See Secure Environment variables . SSL_CERTIFICATE See Secure Environment variables . SSL_CA_CERTIFICATE See Secure Environment variables ."
  },
  "content/images and containers/solr.html": {
    "href": "content/images and containers/solr.html",
    "title": "Solr",
    "keywords": "Solr In a containerized deployment, Solr is configured and run from a Solr image. Configuring Solr Solr is configured by the solr.xml file. A default solr.xml configuration is generated by the Solr container. To modify the solr.xml , you can modify the Dockerfile to build an image with your solr.xml file. The Dockerfile in images/solr_redhat contains a commented out example COPY command that copies a solr.xml into the image. By copying a solr.xml file into the image, the container does not generate a default solr.xml . For more information about the file, see Format of solr.xml . Building a Solr image The Solr image for i2 Analyze is built on top of a Solr image maintained by i2 Group on Docker Hub. The Dockerfile is modified to configure Solr for use with i2 Analyze. Docker build command The Solr image is built from the Dockerfile in images/solr_redhat . The following docker build command builds the Solr image: docker build -t \"solr_redhat:4.4.0\" images/solr_redhat \\ --build-arg I2ANALYZE_VERSION=\"4.4.0\" For examples of the build commands, see buildImages.sh script. Running a Solr container A Solr container uses the Solr image. In the docker run command, you can use -e to pass environment variables to Solr on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Solr server container: docker run -d \\ --name \"sol1\" \\ --net \"eia\" \\ --net-alias \"solr1.eia\" \\ --init \\ -p 8983:8983 \\ -v \"solr1_data:/var/solr\" \\ -v \"solr1_secrets:/run/secrets\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e SOLR_HOST=\"solr1.eia\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_READONLY_PASSWORD\" \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ -e SSL_CA_CERTIFICATE_FILE=\"/run/secrets/CA.cer\" \\ \"solr_redhat:4.4.0\" For an example of the docker run command, see serverFunctions.sh . The runSolr function takes the following arguments to support running multiple Solr containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container and the Solr host. VOLUME - The name for the named volume of the Solr container. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. An example of running Solr container by using runSolr function: runSolr solr1 solr1.eia solr1_data 8983 Volumes A named volume or a bind mount can be used to persist data and logs that are generated and used in the Solr container, as well as a separate volume for backups, outside of the container. To configure the Solr container to use the volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For Solr, the directory that must be mounted is /var/solr . For example: -v solr1_data:/var/solr \\ -v solr_backup:/backup \\ -v solr1_secrets:/run/secrets A unique volume name must be used for each Solr container. For more information, see How the image works . Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure Solr, you can provide environment variables to the Docker container in the docker run command. Solr environment variables The following table describes the supported environment variables that you can use for Solr: Environment variable Description SOLR_HOST Specifies the fully qualified domain name of the Solr container. ZooKeeper authentication The following environment variables are used to configure Solr to connect to ZooKeeper as a client: Environment variable Description ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated list. The connection string must be in the following format: <hostname>:<port>,<hostname>:<port> . ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. For more information about configuring Solr to connect to ZooKeeper, see: Client Configuration Parameters . ZooKeeper Access Control Solr SSL The following environment variables enable you use SSl with Solr Environment variable Description SOLR_ZOO_SSL_CONNECTION See below. SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . For more information about the Solr SSL configuration, see Set Common SSL-Related System Properties . For more information about the ZK SSL configuration, see Client Configuration Parameters . Solr Environment Variable Mapping The above environment variables are either passed through to the standard Solr launch script or used to construct the following Solr environment variables. For exact details see the Docker image. SOLR_ZK_CREDS_AND_ACLS SOLR_OPTS"
  },
  "content/images and containers/solr_client.html": {
    "href": "content/images and containers/solr_client.html",
    "title": "Solr Client",
    "keywords": "Solr Client A Solr Client container is an ephemeral container that is used to run Solr commands. Building a Solr Client container The Solr Client uses the same image as the Solr Server container. For more information about building the Solr image, see Solr . Running a Solr Client container A Solr Client container uses the Solr image. In the docker run command, you can use -e to pass environment variables to Solr on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Solr Client container: docker run --rm \\ --net \"eia\" \\ -v \"examples/pre-prod/configuration:/opt/configuration\" \\ -e SOLR_ADMIN_DIGEST_USERNAME=\"solr\" \\ -e SOLR_ADMIN_DIGEST_PASSWORD=\"SOLR_ADMIN_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD=\"ZOO_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD=\"ZOO_DIGEST_READONLY_PASSWORD\" \\ -e SECURITY_JSON=\"SECURITY_JSON\" \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SSL_PRIVATE_KEY=\"SSL_PRIVATE_KEY\" \\ -e SSL_CERTIFICATE=\"SSL_CERTIFICATE\" \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"solr_redhat:4.4.0\" \"$@\" For an example of the docker run command, see runSolrClientCommand function in clientFunctions.sh script. For an example of how to use runSolrClientCommand function, see runSolrClientCommand . Bind mounts Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Configuration : The Solr client requires the i2 Analyze configuration to perform some Solr operations. To access the configuration, the configuration directory must be mounted into the container. Environment variables To configure the Solr client, you can provide environment variables to the Docker container in the docker run command. Environment variable Description SOLR_ADMIN_DIGEST_USERNAME For usage see Command Parsing SOLR_ADMIN_DIGEST_PASSWORD For usage see Command Parsing ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. SECURITY_JSON The Solr security.json. Solr Basic Authentication SOLR_ZOO_SSL_CONNECTION See Secure Environment Variables . SERVER_SSL See Secure Environment Variables . SSL_PRIVATE_KEY See Secure Environment Variables . SSL_CERTIFICATE See Secure Environment Variables . SSL_CA_CERTIFICATE See Secure Environment Variables . Command parsing When commands are passed to the Solr client by using the \"$@\" notation, the command that is passed to the container must be escaped correctly. On the container, the command is run using docker exec \"$@\" . Because the command is passed to the docker run command using bash -c , the command must be maintained as a double quoted string. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\\\" -H Content-Type:text/xml --data-binary \\\"<delete><query>*:*</query></delete>\\\"\" Different parts of the command must be escaped in different ways: \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" Because the curl command uses the container's local environment variables to obtain the values of SOLR_ADMIN_DIGEST_USERNAME and SOLR_ADMIN_DIGEST_PASSWORD , the $ is escaped by a \\ . The \" around both of the variables are escaped with a \\ to prevent the splitting of the command, which means that the variables are evaluated in the container's environment. \\\"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\\\" The URL is surrounded in \" because the string contains a variable. The \" are escaped with a \\ . Because the SOLR1_FQDN variable is evaluated before it is passed to the container, the $ is not escaped. \\\"<delete><query>*:*</query></delete>\\\" The data portion of the curl command is escaped with \" because it contains special characters. The \" are escaped with a \\ ."
  },
  "content/images and containers/sql_client.html": {
    "href": "content/images and containers/sql_client.html",
    "title": "SQL Server Client",
    "keywords": "SQL Server Client An SQL Server Client container is an ephemeral container that is used to run the sqlcmd commands to create and configure the database. Building an SQL Server Client image The SQL Server Client is built from a Dockerfile that is based on Microsoft SQL Server . The SQL Server Client image is built from the Dockerfile in images/sql_client . Docker build command The following docker build command builds the SQL Server Client image: docker build -t \"sqlserver_client_redhat:4.4.0\" images/sql_client Running a SQL Server Client container An SQL Server Client container uses the SQL Server Client image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command runs a SQL Server Client container: docker run \\ --rm \\ --name \"sqlclient\" \\ --network \"eia\" \\ -v \"pre-reqs/i2analyze/toolkit:/opt/toolkit\" \\ -v \"examples/pre-prod/database-scripts/generated:/opt/databaseScripts/generated\" \\ -e SQLCMD=\"/opt/mssql-tools/bin/sqlcmd\" \\ -e SQLCMD_FLAGS=\"-N -b\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_NAME=\"ISTORE\" \\ -e GENERATED_DIR=\"/opt/databaseScripts/generated\" \\ -e DB_USERNAME=\"dba\" \\ -e DB_PASSWORD=\"DBA_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"sqlserver_client_redhat:4.4.0\" \"$@\" For an example of the docker run command, see runSQLServerCommandAsETL function in clientFunctions.sh script. For an example of how to use runSQLServerCommandAsETL function, see runSQLServerCommandAsETL . Note: you can run SQL Server Client container as different users, see runSQLServerCommandAsDBA , runSQLServerCommandAsSA Bind mounts Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access ZooKeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_CA_CERTIFICATE_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Toolkit : For the SQL Server Client to use the tools in /opt/toolkit/i2-tools/scripts , the toolkit must be mounted into the container. In the example scripts, this is defaulted to /opt/toolkit . Generated scripts directory : Some of the i2 Analyze tools generate scripts to be run against the Information Store database. For the SQL Server Client to run these scripts, the directory where they are generated must be mounted into the container. In the example scripts, this is defaulted to /database-scripts/generated . The GENERATED_DIR environment variable must specify the location where the generated scripts are mounted. Environment variables Environment Variable Description DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_USERNAME The user. DB_PASSWORD The password. GENERATED_DIR The root location where any generated scripts are created. The following environment variables enable you use SSL Environment variable Description DB_SSL_CONNECTION See Secure Environment variables . SSL_CA_CERTIFICATE See Secure Environment variables . Command parsing When commands are passed to the Solr client by using the \"$@\" notation, the command that is passed to the container must be escaped correctly. On the container, the command is run using docker exec \"$@\" . Because the command is passed to the docker run command using bash -c , the command must be maintained as a double quoted string. For example: runSQLServerCommandAsETL bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -S \\${DB_SERVER},${DB_PORT} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} -Q \\\"BULK INSERT ${STAGING_SCHEMA}.${table_name} FROM '/var/i2a-data/${BASE_DATA}/${csv_and_format_file_name}.csv' WITH (FORMATFILE = '/var/i2a-data/${BASE_DATA}/sqlserver/format-files/${csv_and_format_file_name}.fmt', FIRSTROW = 2)\\\"\" Different parts of the command must be escaped in different ways: \\${DB_SERVER} , \\${DB_USERNAME} , \\${DB_PASSWORD} , and \\${DB_NAME} Because the command uses the container's local environment variables to obtain the values of these variables, the $ is escaped by a \\ . ${DB_PORT} is not escaped because this is an environment variable available to the script calling the client function. \\\"BULK INSERT ${STAGING_SCHEMA}.${table_name} ... FIRSTROW = 2)\\\" The string value for the -Q argument must be surrounded by \" when it is run on the container. The surrounding \" are escaped with \\ . The variables that are not escaped in the string are evaluated outside of the container when the function is called."
  },
  "content/images and containers/sql_server.html": {
    "href": "content/images and containers/sql_server.html",
    "title": "SQL Server",
    "keywords": "SQL Server In a containerized deployment, the database is located on a SQL Server container. Building a SQL Server image SQL Server is built from a Dockerfile that is based on the Dockerfile from Microsoft SQL Server . The SQL Server image is built from the Dockerfile in images/sql_server . Docker build command The following docker build command builds the SQL Server image: docker build -t \"sqlserver_redhat:4.4.0\" images/sqlserver For examples of the build commands, see buildImages.sh script. Running a SQL Server container The SQL Server container uses the SQL Server image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command runs a SQL Server container: docker run -d \\ --name \"sqlserver\" \\ --network \"eia\" \\ --net-alias \"sqlserver.eia\" \\ -p \"1433:1433\" \\ -v \"sqlserver_data:/var/opt/mssql\" \\ -v \"sqlserver_sqlbackup:/backup\" \\ -v \"sqlserver_secrets:/run/secrets/\" \\ -v \"prereqs/i2analyze/toolkit/examples/data:/var/i2a-data\" \\ -e ACCEPT_EULA=\"Y\" \\ -e MSSQL_AGENT_ENABLED=true \\ -e MSSQL_PID=\"Developer\" \\ -e SA_PASSWORD_FILE=\"/run/secrets/SA_PASSWORD_FILE\" \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ \"sqlserver_redhat:4.4.0\" For an example of the docker run command, see serverFunctions.sh . The runSQLServer does not take any arguments. Volumes Named volumes are used to persist data and logs that are generated and used in the SQL Server container, as well as a separate volume for backups, outside of the container. Note: It is good practice to have a separate volume for the backup from the database storage. For more information, see SQL Server Backup best practices . To configure the SQL Server container to use these volumes, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For SQL Server, the path to the directory that must be mounted is /var/opt/mssql . For example: -v sqlvolume:/var/opt/mssql -v sqlserver_sqlbackup:/backup For more information, see Use Data Volume Containers . Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_PRIVATE_KEY_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Data : The SQL Server container requires access to the data directory to run the ingestion scripts. To access the data, the data directory must be mounted into the container. Environment variables Environment Variable Description ACCEPT_EULA Set to Y to confirm your acceptance of the End-User Licensing Agreement . MSSQL_AGENT_ENABLED For more information see Configure SQL Server settings with environment variables on Linux MSSQL_PID For more information see Configure SQL Server settings with environment variables on Linux SA_PASSWORD The administrator user's password. The following environment variables enable you to use SSL: Environment variable Description SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . For more information about the SSL in SQLServer, see Specify TLS settings ."
  },
  "content/images and containers/zookeeper.html": {
    "href": "content/images and containers/zookeeper.html",
    "title": "ZooKeeper",
    "keywords": "ZooKeeper In a containerized deployment, ZooKeeper is configured and run from a ZooKeeper image maintained by i2 Group on Docker Hub. Running a ZooKeeper container A ZooKeeper container uses the ZooKeeper image. In the docker run command, you can use -e to pass environment variables to ZooKeeper on the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command starts a ZooKeeper container: docker run --restart always -d \\ --name \"zk1\" \\ --net \"eia\" \\ --net-alias \"zk1.eia\" \\ -p \"8080:8080\" \\ -p \"2181:2181\" \\ -p \"2281:2281\" \\ -p \"3888:3888\" \\ -p \"2888:2888\" \\ -v \"zk1_data:/data\" \\ -v \"zk1_datalog:/datalog\" \\ -v \"zk1_logs:/logs\" \\ -v \"zk1_secrets:/run/secrets\" \\ -e \"ZOO_SERVERS=server.1=zk1.eia:2888:3888 server.2=zk2.eia:2888:3888 server.3=zk3.eia:2888:3888\" \\ -e \"ZOO_MY_ID=1\" \\ -e \"ZOO_SECURE_CLIENT_PORT=2281\" \\ -e \"ZOO_CLIENT_PORT=2181\" \\ -e \"ZOO_4LW_COMMANDS_WHITELIST=ruok, mntr, conf\" \\ -e \"SERVER_SSL=true\" \\ -e \"SSL_PRIVATE_KEY_FILE=/run/secrets/server.key\" \\ -e \"SSL_CERTIFICATE_FILE=/run/secrets/server.cer\" \\ -e \"SSL_CA_CERTIFICATE_FILE=/run/secrets/CA.cer\" \\ \"i2eng/i2eng-zookeeper:3.6\" Note: SERVER_SSL variable is set based on the SOLR_ZOO_SSL_CONNECTION switch, see Environment variables . ZooKeeper Service Ports Default ports used by ZooKeeper are: 8080 - By default, the server is started on port 8080, and commands are issued by going to the URL \"/commands/[command name]\", e.g., http://localhost:8080/commands/stat . 2181 - The port at which the clients will connect (non-secure). This is defined by setting ZOO_CLIENT_PORT . 2281 - The port at which the clients will connect (secure). This is defined by setting ZOO_SECURE_CLIENT_PORT . 3888 - Port used by ZooKeeper peers to talk to each other. 2888 - Port used by ZooKeeper peers to talk to each other. For more information, see ZooKeeper Service Ports . For an example of the docker run command, see serverFunctions.sh . The runZK function takes the following arguments to support running multiple ZooKeeper containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container. DATA_VOLUME - The name for the data named volume. For more information, see Volumes . DATALOG_VOLUME - The name for the datalog named volume. For more information, see Volumes . LOG_VOLUME - The name for the log named volume. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. ZOO_ID - An identifier for the ZooKeeper server. For more information, see Environment variables . An example of running Zookeeper container using runZK function: runZK zk1 zk1.eia zk1_data zk1_datalog zk1_logs 8080 1 Volumes A named volume or a bind mount can be used to persist data and logs that are generated and used in the ZooKeeper container, outside of the container. For more information, see Where to store data . Named Volumes To configure the ZooKeeper container to use the named volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For ZooKeeper, the directories that must be mounted are /data , /datalog , /logs . For example: -v zk1_data:/data \\ -v zk1_datalog:/datalog \\ -v zk1_log:/logs \\ -v zk1_secrets:/run/secrets A unique volume name must be used for each ZooKeeper container. A bind mount can be used instead of the named volume: For example: -v /var/zk/data:/data \\ -v /var/zk/datalog:/datalog \\ -v /var/zk/logs:/logs \\ A unique bind mount must be used for each ZooKeeper container. Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_PRIVATE_KEY_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure ZooKeeper, you can provide environment variables to the Docker container in the docker run command. The zoo.cfg configuration file for ZooKeeper is generated from the environment variables passed to the container. The following table describes the mandatory environment variables for running ZooKeeper in replicated mode: Environment variable Description ZOO_SERVERS Specified the list of ZooKeeper servers in the ZooKeeper ensemble. Servers are specified in the following format: server.id=<address1>:<port1>:<port2>;<client port> . ZOO_MY_ID An identifier for the ZooKeeper server. The identifier must be unique within the ensemble. ZOO_CLIENT_PORT Specifies the port number for client connections. Maps to the clientPort configuration parameter. ZOO_4LW_COMMANDS_WHITELIST A list of comma separated Four Letter Words commands that user wants to use. A valid Four Letter Words command must be put in this list else ZooKeeper server will not enable the command. By default the whitelist only contains \"srvr\" command which zkServer.sh uses. The rest of four letter word commands are disabled by default. For more information, see ZooKeeper Docker hub . The following table described the security environment variables: Environment variable Description ZOO_SECURE_CLIENT_PORT Specifies the port number for client connections that use SSL. Maps to the secureClientPort configuration parameter. SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . For more information about securing ZooKeeper, see Encryption, Authentication, Authorization Options . The following table describes the environment variables that are supported: Environment variable Description ZOO_TICK_TIME The length of a single tick, which is the basic time unit used by ZooKeeper, as measured in milliseconds. Maps to the tickTime configuration parameter. The default value is 2000 . ZOO_INIT_LIMIT Amount of time, in ticks, to allow followers to connect and sync to a leader. Increase this value as needed, if the amount of data managed by ZooKeeper is large. Maps to the initLimit configuration parameter. The default value is 10 . ZOO_SYNC_LIMIT Amount of time, in ticks, to allow followers to sync with ZooKeeper. If followers fall too far behind a leader, they will be dropped. Maps to the syncLimit configuration parameter. The default value is 5 . ZOO_AUTOPURGE_PURGEINTERVAL The time interval in hours for which the purge task has to be triggered. Set to a positive integer (1 and above) to enable the auto purging. Maps to the autopurge.purgeInterval configuration parameter. The default value is 24 . ZOO_AUTOPURGE_SNAPRETAINCOUNT When auto purge is enabled, ZooKeeper retains the specified number of most recent snapshots and the corresponding transaction logs in the dataDir and dataLogDir respectively and deletes the rest. Maps to the autopurge.snapRetainCount setting. The default value is 3 . ZOO_MAX_CLIENT_CNXNS Limits the number of concurrent connections (at the socket level) that a single client, identified by IP address, may make to a single member of the ZooKeeper ensemble. Maps to the maxClientCnxns configuration parameter. The default value is 60 . ZOO_STANDALONE_ENABLED When set to true , if ZooKeeper is started with a single server the ensemble will not be allowed to grow, and if started with more than one server it will not be allowed to shrink to contain fewer than two participants. Maps to the standaloneEnabled configuration parameter. The default value is true . ZOO_ADMINSERVER_ENABLED Enables the AdminServer. The AdminServer is an embedded Jetty server that provides an HTTP interface to the four letter word commands. Maps to the admin.enableServer configuration parameter. The default value is true . ZOO_DATA_DIR The location where ZooKeeper stores in-memory database snapshots. Maps to the dataDir configuration parameter. The default value is /data . ZOO_DATA_LOG_DIR The location where ZooKeeper writes the transaction log. Maps to the dataLogDir configuration parameter. The default value is /datalog . ZOO_CFG_EXTRA You can add arbitrary configuration parameters, that are not exposed as environment variables in ZooKeeper, to the Zookeeper configuration file using this variable. ZOO_CONF_DIR Specifies the location for the ZooKeeper configuration directory. The default value is /conf . ZOO_LOG_DIR Specifies the location for the ZooKeeper logs directory. The default value is /logs . For more information about configuring ZooKeeper, see: Configuration Parameters ZooKeeper Docker hub . Note: Values that are specified in the environment variables override any configuration that is included in the ZOO_CFG_EXTRA block."
  },
  "content/ingest_config_dev.html": {
    "href": "content/ingest_config_dev.html",
    "title": "Ingesting development data into the Information Store",
    "keywords": "Ingesting development data into the Information Store When you are developing a configuration, ingest a small amount of representative test data into the system to ensure the schema is suitable for your data and you can configure i2 Analyze to meet your requirements. For more information about ingesting data, see Ingesting data into the Information Store . Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to a pattern that includes the Information Store. For example: DEPLOYMENT_PATTERN=\"i2c_istore\" Process overview: Provide a data set Create the ingestion sources Provide and run scripts to complete the ingestion process If you have deployed with the law enforcement schema, complete the steps in Example ingestion process to ingest example data into your Information Store. Data sets The /i2a-data directory is used to contain the data sets that you ingest into the Information Store of a deployment. The data that you ingest into the Information Store must conform to the Information Store schema. However, one data set can be ingested with different configs. There is not a 1-to-1 mapping between data sets and configs. Each data set must contain at least one ingestion script. This script contains the functions that populate the staging tables with your data and calls the ETL toolkit tools that ingest the data. The expected directory structure is as follows: - i2a-data - <data_set> - scripts - <script1> - <script2> Ingesting data into the config dev environment The manageData.sh script is used to manage the ingestion process in the dev environment. To ingest data into the Information Store, you must create scripts that call the ETL tools that complete the actions required by i2 Analyze for you ingest data. For more information, see: - ETL tools - Ingesting data into the Information Store You can find example scripts in the examples/ingestion/scripts directory. These scripts demonstrate how to create staging tables, populate them, and ingest the data into the Information Store. To run scripts, the manageData.sh script is called as follows: ./manageData.sh -c <config_name> -t ingest -d <data_set> -s <script_name> Where: <config_name> is the name of the config that is currently deployed and running in the config dev environment <data_set> is the name of a directory in i2a-data <script_name> is the name of a script in the directory specified for <data_set> Creating ingestion sources The ingestion sources for a config are contained in the configuration. Ingestion sources are defined in <config_name>/configuration/ingestion/scripts/createIngestionSources.sh . Copy the examples/ingestion/scripts/createIngestionSources.sh file to the <config_name>/configuration/ingestion/scripts/ directory. In the script, the INGESTION_SOURCES array contains the name and description of 2 example sources. INGESTION_SOURCES=( [Example Ingestion Source 1]=EXAMPLE_1 [Example Ingestion Source 2]=EXAMPLE_2 ) You can modify or add to the array of ingestion sources. To create the ingestion sources in the array, the manageData.sh script is called as follows: ./manageData.sh -c <config_name> -t sources Example ingestion process The i2 Analyze minimal toolkit contains the example law-enforcement-data-set-1 data that can be ingested when the example law enforcement schema ( law-enforcement-schema.xml ) is deployed. This contains a number of CSV files that contain the data, and a mapping.xml file. For more information about the mapping file, see Ingestion mapping files . Before you can ingest the law enforcement example data, complete the following steps to provide the data set and scripts: Copy the pre-reqs/i2analyze/toolkit/examples/data/law-enforcement-data-set-1 directory to the i2a-data directory. Copy the examples/ingestion/scripts directory to the i2a-data/law-enforcement-data-set-1 directory. The directory structure is as follows: - i2a-data - law-enforcement-data-set-1 - scripts - ingestLawEnforcementDataSet1.sh - createStagingTables.sh Copy the examples/ingestion/scripts/createIngestionSources.sh file to the <config_name>/configuration/ingestion/scripts/ directory. Use the manageData.sh to create the ingestion sources defined in the example createIngestionSources.sh . For example: ./manageData.sh -c config-development -t sources The example scripts separate the creation of the staging tables from the ingestion of data. To ingest the example data into the config-development config, run the following commands: ./manageData.sh -c config-development -t ingest -d law-enforcement-data-set-1 -s createStagingTables.sh ./manageData.sh -c config-development -t ingest -d law-enforcement-data-set-1 -s ingestLawEnforcementDataSet1.sh The manageData.sh script The scripts/manageData.sh script is used to manage data in an environment. It can be used to run scripts that use the ETL toolkit tools, or to remove all data from the Information Store. The following usage and help is provided for the manageData.sh script: Usage: manageData.sh -c <config_name> -t {ingest} -d <data_set> -s <script_name> [-v] manageData.sh -c <config_name> -t {sources} [-s <script_name>] [-v] manageData.sh -c <config_name> -t {delete} [-v] manageData.sh -h Options: -c <config_name> Name of the config to use. -t {delete|ingest|sources} The task to run. Either delete or ingest data, or add ingestion sources. Delete permanently removes all data from the database. -d <data_set> Name of the data set to ingest. -s <script_name> Name of the ingestion script file. -v Verbose output. -h Display the help. After you add data to your environment, you can configure the rest of the configuration ."
  },
  "content/manage_backup_restore.html": {
    "href": "content/manage_backup_restore.html",
    "title": "Back up and restore a development database",
    "keywords": "Back up and restore a development database When you are developing a configuration that uses an Information Store, you might populate the Information Store with demonstration or test data. You can use the tooling provided to back up the Information Store that is associated with a config. The Information Store is contained in a Docker volume. Because Docker is not designed for permanent data storage, you can back up an Information Store to your local file system. Backup location The backups are stored in the /backups directory. A sub-directory is created for each config name, with another sub-directory for each backup name. The backup file is called ISTORE.bak . For example: - backups - <config_name> - <backup_name> - ISTORE.bak - <backup_name> - ISTORE.bak Creating a backup Use the deploy script to create your backup. The following usage pattern shows how you create a backup: ./deploy.sh -c <config_name> -t backup -b <backup_name> If you do not provide a backup name, the backup is created in a directory called default . For example, to create a backup called test-1 for the config-development config: ./deploy.sh -c config-development -t backup -b test-1 Restoring from backup Use the deploy script to restore from a backup. The following usage pattern shows how you restore from a backup: ./deploy.sh -c <config_name> -t restore -b <backup_name> For example, to restore a backup called test-1 for the config-development config: ./deploy.sh -c config-development -t restore -b test-1"
  },
  "content/manage_update_images.html": {
    "href": "content/manage_update_images.html",
    "title": "Updating the Docker images",
    "keywords": "Updating the Docker images The update task pulls the latest images from Docker Hub and rebuilds the local images. Use this task to update the images in your environments. Images on Docker Hub are updated with security fixes and other updates. To ensure that you are up-to-date with the latest images, run the update task on a regular basis. For example, once every two weeks or once a month. Run the following command to update the images: ./manageEnvironment.sh -t update If your images are already up-to-date, a message is displayed and you do not need to update your configs. You do not need to complete the following steps. If your images are updated, complete the following steps to update your configs to use the latest images, clean and recreate your deployments. Before you clean your deployments, ensure that you have backed up any data. To back up all of the configs in your environment, use the manageEnvironment.sh script. For example: ./manageEnvironment.sh -t backup -b update-20220623 This creates a backup named update-20220623 for all configs in the environment. Clean all of the configs by running the following command: ./manageEnvironment.sh -t clean Redeploy each config by running the following command: ./deploy.sh -c <config_name> Restore any data to each config. For example, to restore data from the update-20220623 backup, run: ./deploy.sh -c <config_name> -t restore -b update-20220623 For more information, see Back up and restore a development database ."
  },
  "content/managing_config_dev.html": {
    "href": "content/managing_config_dev.html",
    "title": "Managing configurations",
    "keywords": "Managing configurations Configs The /configs directory contains all of your configs. The name of the directory for each config is used to identify it when you run the deploy script. You can have as many different configs in the configs directory, however you can only have one deployed and running in the environment at any time. When you run the deploy.sh script and specify the name of a config that is already deployed, the running deployment is updated with any changes you have made to the configuration or the referenced connectors and extensions. When you run the deploy.sh script and specify the name of a config that is not deployed, the containers in the current environment are stopped and a new environment is deployed with the specified config. The deploy.sh script The scripts/deploy.sh script is used to deploy configs, assets and manage your environment. The following usage and help is provided for the deploy.sh script: Usage: deploy.sh -c <config_name> [-t {clean}] [-v] [-y] deploy.sh -c <config_name> [-t {backup|restore} [-b <backup_name>]] [-v] [-y] deploy.sh -h Options: -c <config_name> Name of the config to use. -t {clean} Clean the deployment. Will permanently remove all containers and data. -t {backup} Backup the database. -t {restore} Restore the database. -b <backup_name> Name of the backup to create or restore. If not specified, the default backup is used. -v Verbose output. -y Answer 'yes' to all prompts. -h Display the help."
  },
  "content/managing_connectors.html": {
    "href": "content/managing_connectors.html",
    "title": "Managing connectors",
    "keywords": "Managing connectors The /connector-images directory contains the files that are required to build each connector image in your config dev environment. The complete contents of each connector directory depends on the template that you used to create it. All the templates include a folder that you use to place the connector source code, the files used to build the Docker images and run the Docker containers, and JSON files that you use to provide information about the connector. For instructions about how to add connectors to your environment, see Adding connectors to your development environment . Connector versioning You specify the version and tag of a connector in the connector-version.json file of each connector. version The version value is used as a version for your connector. If you are deploying an i2 Connect server connector, the version value has more requirements. For more information, see i2 Connect server connector versioning . tag The tag value is used in the image name, host name, and secrets for your connector container. For example: { \"version\": \"0.0.1\", \"tag\": \"1-0-0\" } Building connectors Use the deploy.sh script to generate secrets and build the images for connectors referenced in that config. For example, to build and run all of the connectors referenced in the <config-name> config, run: ./deploy.sh -c <config_name> To build the images for all connectors in your environment, see Managing the environment . Connector secrets Before you can use a connector, you must generate secrets for it. For more information about the secrets used in the environment, see Managing container security . The secrets that are generated for a particular connector are tied to the tag, if you change the tag you must to regenerate the secrets for that connector. To regenerate the secrets for a connector after you change the tag, complete the following steps: Remove the dev-environment-secrets/<connector_name> directory for the connector that you changed the tag of. Run the deploy.sh script with a config that references the connector. ./deploy.sh -c <config_name> Note: To secure the connection to an external connector, you must install the following certificates where the connector is running. At this release, your connector must be configured to use the certificates signed by the config development environment CA. dev-environment-secrets/generated-secrets/certificates/<connector-name>/server.key dev-environment-secrets/generated-secrets/certificates/<connector-name>/server.cer dev-environment-secrets/generated-secrets/certificates/CA/CA.cer i2 Connect server connectors If you are deploying one or more connectors that were developed using the i2 Connect SDK. There are two versions that you must consider; the version of a connector and the version of the i2 Connect server that the connectors depends on. Connector version In the connector-version.json file, the version specifies the range of supported versions for the connector. This enables you to have more control of the version of a connector that is used in your deployment. You can specify the range of versions at the major, minor, or patch level. For example, if you specify a major version 1 in the connector-version.json file, any connector that has a major version of 1 is compatible. When the connector is developed, the version of the connector is provided in the package.json file by the developer. If the version of the connector specified in package.json does not match the version, or range of versions, that you specify in connector-version.json a warning is displayed when you deploy your environment. The following table demonstrates how you can specify the version ranges and their compatibility with connector versions: package.json version connector-version.json version Compatible? (Y/N) 1.0.0 1 Y 1.3.0 1 Y 1.4.0 1.3 N 1.4.6 1.4 Y 1.4.8 1.4.7 N 2.0.0 1 N i2 Connect server version i2 Connect server connectors depend on the i2 Connect server. When the connector is developed, the version of the i2 Connect server that the connector depends on is provided in the package.json file by the developer. In the connector-images/i2connect-server-version.json file, you specify the range of versions of the i2 Connect server that you want all i2 Connect server connectors to depend on. The versions in these files are specified in the npm semantic versioning syntax. For more information about the syntax, see semver - the semantic versioning for npm . If the version of the i2 Connect server that a connector depends on specified in package.json does not match the version, or range of versions, that you specify in i2connect-server-version.json a warning is displayed when you deploy your environment. The following table demonstrates how you can specify the version ranges and their compatibility with i2 Connect server versions: package.json version connector-version.json version Compatible? (Y/N) ^1.0.0 ^1.0.0 Y 1.3.2 ^1.0.0 Y 1.4.0 1.3.0 N 2.0.0 ^1.0.0 N i2 Connect server connector configuration (Optional) i2 Connect server connectors can have specific connector configuration that is defined in the settings.json file. When the connector is developed, a default file can be provided by the developer. The developer should also provide you with documentation that described the required configuration for the connector. After the first deployment of the connector, if a settings.json file is provided, the system extracts it to the connector-images/<connector-name>/app directory. The file is in the same directory structure as in the supplied connector. For example, <connector-name>/app/config/settings.json . To change the configuration, modify the settings.json and redeploy the environment. You might have to provide secrets, or references to secrets, in the settings.json file. To provide the secrets, you can use a value from the environment. This can either be an environment variable or a value from a .env file in the connector-image/<connector-name> directory. Note: If you use a .env file, it must not be checked in to source control or distributed to prevent compromising your secrets. An example settings.json file might look like: { \"data_source_url\": \"https://data-source/data.json\", \"data_source_API_key\": \"${env.API_KEY}\" } Where API_KEY is an environment variable or a key in a .env file with the value of the API Key. For more information about populating the settings.json file, see Making a connector configurable ."
  },
  "content/managing_environment.html": {
    "href": "content/managing_environment.html",
    "title": "Managing the environment",
    "keywords": "Managing the environment The manageEnvironment.sh script is used to manage the dev environment rather than specific configs. The actions that the script can complete include; backing up and restoring all configs, upgrading configs to a later version, and building connectors. i2 Analyze version In a the config dev environment and a containerized deployment, the images that are used for the containers are built for a specific version on i2 Analyze. To identify the version of i2 Analyze that a particular image is built for, inspect the the image. To inspect the images, use the Docker extension in VS Code. The liberty_configured_redhat & solr_redhat image includes the version number in the tag. For example: config-development-4.4.0.0 Each config includes a version file that reports the i2 Analyze version for that config. This value does not include the fix pack version number. For example, the version file contains: SUPPORTED_I2ANALYZE_VERSION=4.4.0 The manageEnvironment.sh script The scripts/manageEnvironment.sh script is used to manage to development environment rather than specific configs. The following usage and help is provided for the manageEnvironment.sh script: Usage: manageEnvironment.sh -t backup [-b <backup_name>] [-p <path>] [-i <config_name>] [-e <config_name>] [-y] manageEnvironment.sh -t copy -p <path> [-i <config_name>] [-e <config_name>] [-y] manageEnvironment.sh -t upgrade -p <path> [-y] manageEnvironment.sh -t update [-y] manageEnvironment.sh -t clean [-i <config_name>] [-e <config_name>] [-y] manageEnvironment.sh -t connectors [-i <connector_name>] [-e <connector_name>] [-y] manageEnvironment.sh -t extensions [-i <extension_name>] [-e <extension_name>] [-y] manageEnvironment.sh -h Options: -t {backup} Backup the database for a config. -t {copy} Copy the dependencies for a config from the specified path, to the current analyze-containers project. -t {upgrade} Upgrade all configurations from the specified path. -t {update} Pulls the latest base images from Docker Hub and rebuilds the local images. -t {clean} Clean the deployment for a config. Will permanently remove all containers and data. -t {connectors} Build all connector images. -t {extensions} Build all extensions. -i <config_name|extension_name|connector_name> Name of the config, connector or extension to include for the task. To specify multiple values, add additional -i options. -e <config_name|extension_name|connector_name> Name of the config, connector or extension to exclude for the task. To specify multiple values, add additional -e options. -b <backup_name> Name of the backup to create or restore. If not specified, the default backup is used. -p <path> Path to the root of an analyze-containers project. Defaults to the current project path. -y Answer 'yes' to all prompts. -v Verbose output. -h Display the help. Backup and restore The backup and restore tasks allow you to back up and restore a number of configurations by using one command. To back up a single config, use the deploy.sh script that is described in back up and restore . To back up all of the configs in your environment, run: ./manageEnvironment.sh -t backup -b backup-202206 This creates a backup named backup-202206 for all configs in the environment. Updating the Docker images The update task pulls the latest images from Docker Hub and rebuilds the local images. Use this task to update the images in your environments. Images on Docker Hub are updated with security fixes and other updates. For more information, see Update the Docker images . Upgrading The upgrade task allows you to upgrade the configs in your environment to a later version of Analyze containers or i2 Analyze. For more information, see Upgrading . Copying configurations The copy task allows you to copy configs from one development environment to another. To mount the directory in the dev container, open the devcontainer.json file inside .devcontainer directory. In the mounts object, add the path to another analyze-containers directory as the source and target. For example: \"mounts\": [ ... \"source=/home/<user-name>/analyze-containers,target=/home/<user-name>/analyze-containers,type=bind,consistency=cached\" ] Press F1 (or Cmd+Shift+P in MacOS) and type Remote-Containers: Rebuild Container and select it. Run the following command from the scripts directory: ./manageEnvironment.sh -p <path> -t copy Where <path> is the absolute path to the root of another analyze-containers repository. This script copies the following directories: backups configs connector-images dev-environment-secrets gateway-schemas i2a-data i2a-extensions If you specify to include or exclude any configs by using the -i and -e options, only the directories that are referenced by the configs are copied. Building connectors The connectors that are referenced by a config are built and deployed when that config is deployed. You can optionally build all connectors or a subset of the connectors by using the manageEnvironment.sh script with the connectors task. To build all of the connector images, run: ./manageEnvironment.sh -t connectors To build a subset of the connector images, you can use of the include ( -i ) and exclude ( -e ) options. For example: ./manageEnvironment.sh -t connectors -i connector_name_1 -i connector_name_2 This command only builds images for connector_name_1 and connector_name_2 . ./manageEnvironment.sh -t connectors -e <connector_name_1> -e <connector_name_2> This command builds the images for all connectors except connector_name_1 and connector_name_2 . Building extensions The extensions that are referenced by a config are built and deployed when that config is deployed. You can optionally build all extensions or a subset of the extensions by using the manageEnvironment.sh script with the extensions task. To build all of the extensions, run: ./manageEnvironment.sh -t extensions To build a subset of the extensions, you can use the include ( -i ) and exclude ( -e ) options. For example: ./manageEnvironment.sh -t extensions -i extension_1 -i extension_2 This command only builds extension_1 and extension_2 . ./manageEnvironment.sh -t extensions -e <extension_1> -e <extension_2> This command builds all of the extensions except extension_1 and extension_2 ."
  },
  "content/managing_toolkit_configuration.html": {
    "href": "content/managing_toolkit_configuration.html",
    "title": "Synchronize configurations",
    "keywords": "Synchronize configurations You can synchronize configurations between the i2 Analyze deployment toolkit and the configuration development environment. This enables you to develop the i2 Analyze configuration by using the config development environment, and deploy with it as part of the i2 Analyze deployment toolkit. This can be useful in the following situations: You want to create a new deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to develop the configuration for it. You have an existing deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to further develop the configuration. When you synchronize a configuration, you must do so with a complete i2 Analyze deployment toolkit. You can synchronize from an existing deployment toolkit configuration (import) or from an existing configuration development config (export). The configuration from the i2 Analyze deployment toolkit is the source of truth. When you have an existing deployment toolkit, the instructions direct you to back up the configuration before you synchronize it with the config development environment. You complete the synchronization by using the manageToolkitConfiguration.sh script. Examples The following use cases demonstrate the process of synchronizing configurations between the config development environment and an i2 Analyze deployment toolkit: Use case 1 : You want to create a new deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to develop the configuration for it. Use case 2 : You have an existing deployment that uses the i2 Analyze deployment toolkit and you want to convert to use the config development environment to further develop the configuration. After you develop the configuration, you will still deploy i2 Analyze using the deployment toolkit. Use case 1 Use the config development environment to create and develop a config. For more information, see Configuration development environment . When you are happy with the developed configuration, you can complete the process that enables you to use it with the i2 Analyze deployment toolkit. Install the i2 Analyze deployment toolkit. For more information, see Installing i2 Analyze . To mount the directory where you installed the i2 Analyze toolkit to the dev container, open the devcontainer.json file inside .devcontainer directory. In the mounts object, add the path to the toolkit directory as the source and target. On Windows, you must use forward slashes ( / ). If your toolkit is on the Windows filesystem, access it by using the /mnt directory. For example: \"mounts\": [ ... \"source=/mnt/c/i2/i2analyze/toolkit,target=/mnt/c/i2/i2analyze/toolkit,type=bind,consistency=cached\" ] Press F1 (or Cmd+Shift+P in MacOS) and type Remote-Containers: Rebuild Container and select it. Export the configuration from the config development environment to the i2 Analyze deployment toolkit. Run the manageToolkitConfiguration.sh script with the create task to create a base configuration in the i2 Analyze deployment toolkit. ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/i2/i2analyze/toolkit -t create Where: -c is the name of the config. -p is the absolute path to the root of the i2 Analyze deployment toolkit that was mounted into the dev container. Note: The base configuration that is created is suitable for the deployment pattern that is specified in the variables.sh file of the specified config. Run the manageToolkitConfiguration.sh script with the export task to export the config dev configuration into the deployment toolkit. ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/i2/i2analyze/toolkit -t export Where: -c is the name of the config to export. -p is the absolute path to the root of the i2 Analyze deployment toolkit that was mounted into the dev container. You can then deploy your i2 Analyze deployment toolkit configuration. For more information about deploying i2 Analyze, see Deploying i2 Analyze . Start at step 3 from the linked documentation. There are a number of environment specific configuration files that are not populated in the config development environment. Use case 2 This section assumes that you are starting with a configuration from an i2 Analyze deployment toolkit. For more information about deploying i2 Analyze, see Deploying i2 Analyze . The config development environment uses fixed names for schemas and configuration files. Before you can import your deployment toolkit configuration into the config development environment, your configuration must be updated to use the expected file names. The manageToolkitConfiguration.sh script can rename the files for you and update any references to them. You can also decide whether or not to use the config development environment configuration set in your deployment toolkit configuration. If you decide to use the new config set, all settings for the deployment must be specified in the analyze-settings.properties file. If you choose not to use the new configuration set, any additions that you make to the analyze-settings file in the config development environment are not exported. Back up your i2 Analyze deployment toolkit configuration. Fore more information about backing up your configuration, see Back up and restore the configuration . Use the config development environment to create an empty config to import your configuration in to. For more information, see Configuration development environment . You do not need to specify any schema files or deploy the config. To mount the directory where your i2 Analyze toolkit is installed to the dev container, open the devcontainer.json file inside .devcontainer directory. In the mounts object, add the path to the toolkit directory as the source and target. On Windows, you must use forward slashes ( / ). If your toolkit is on the Windows filesystem, access it by using the /mnt directory. For example: \"mounts\": [ ... \"source=/mnt/c/i2/i2analyze/toolkit,target=/mnt/c/i2/i2analyze/toolkit,type=bind,consistency=cached\" ] Run the manageToolkitConfiguration.sh script with the prepare task to rename the schema and configuration files and update the references: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/i2/i2analyze/toolkit -t prepare Where: -c is the config name you are working with. -p is the absolute path to the root of the i2 Analyze deployment toolkit that was mounted into the dev container. To use the config development environment configuration set, complete the following 2 steps. Otherwise, move to step 5. Copy the contents of templates/toolkit-config-mod directory to the /toolkit/configuration/fragments/common/WEB-INF/classes directory of the i2 Analyze deployment toolkit. Review the settings that are used in the DiscoClientSettings and DiscoServerSettingsCommon properties files of i2 Analyze deployment toolkit and move any settings that you want to continue using into the /toolkit/configuration/fragments/common/WEB-INF/classes/analyze-settings.properties file. You can remove the DiscoClientSettings and DiscoServerSettingsCommon properties files after you move any settings. Run the manageToolkitConfiguration.sh script with the import task to import the configuration into the config development environment: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/i2/i2analyze/toolkit -t import Where: -c is the config name to import the configuration into. -p is the absolute path to the root of the i2 Analyze deployment toolkit that was mounted into the dev container. Use the config development environment to develop the configuration. For more information, see Developing the configuration . In the config development environment, a user.registry.xml is required. You can start from the example provided in pre-reqs/i2analyze/toolkit/examples/security and align it to your security schema or copy in an existing one. If you want to use your developed configuration in your i2 Analyze deployment toolkit, you can use the export task to export the configuration into your toolkit. Otherwise, continue to use the config development environment. Run the manageToolkitConfiguration.sh script with the export task. For example: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/i2/i2analyze/toolkit -t export Where: -c is the config name you have created to synchronize the configuration to. -p is the absolute path to the root of the i2 Analyze deployment toolkit that was mounted into the dev container. You must update the i2 Analyze deployment toolkit deployment with the changes to the configuration. Connectors In the config development environment, i2 Connect connectors are run on Docker images and can only be access from within the Docker network. This means that the connectors and URL references to the connectors cannot be synchronized between configurations. The connector IDs are displayed when you run the import and export tasks to enable you to provide the connectors in each environment. For more information about deploying connectors, see: Adding connectors to your config development environment Adding connectors to your i2 Analyze deployment toolkit configuration Extensions In the config development environment, i2Analyze extensions are built and deployed using Maven. To add an extension to your i2 Analyze deployment toolkit, copy the JAR from i2a-extensions/<extension-name>/target/<extension-name>-<version>.jar and follow the DevEssentials deployment steps. For more information, see Configuring the 'group-based default security dimension values' example project for an example of the steps to take. Prometheus and Grafana In the config development environment, Prometheus and Grafana are run on Docker images. You can modify the configs/<config-name>/configuration/prometheus/prometheus.yml file and the Grafana dashboards in configs/<config-name>/configuration/grafana/dashboards folder to further develop your metrics configuration and once finished this can be copied to the locations that are required by Prometheus and Grafana. For more information, see Configure server monitoring . manageToolkitConfiguration.sh script Usage: manageToolkitConfiguration.sh -c <config_name> -p <toolkit_path> -t { create | prepare | import | export } [-v] Options: -c <config_name> Name of the config to use. -p <toolkit_path> The absolute path to the root of an i2 Analyze deployment toolkit. -t {create} Creates a configuration in the i2 Analyze deployment toolkit that can be imported into the config development environment. -t {prepare} Prepares an existing i2 Analyze deployment toolkit configuration to be imported into the config development environment. -t {export} Export a config development environment configuration to an i2 Analyze deployment toolkit configuration. -t {import} Import an i2 Analyze deployment toolkit configuration to a config development environment configuration. -v Verbose output. -h Display the help."
  },
  "content/managing_update_env.html": {
    "href": "content/managing_update_env.html",
    "title": "Updating to the latest version of the analyze-containers repository",
    "keywords": "Updating to the latest version of the analyze-containers repository To update your config development environment to use the latest version of the analyze-containers repository, update the images, secrets, and deploy your configs. Before you update your environment: Back up any databases that are associated with your configs. For more information, see Back up and restore a development database . Back up the following directories from your existing copy of the repository: backups configs connector-images dev-environment-secrets gateway-schemas i2a-data Download the tar.gz or pull the latest version of the analyze-containers repository from https://github.com/i2group/analyze-containers . If you download the tar.gz , extract the contents and overwrite your existing copy of the analyze-containers repository. Then, copy your backed up directories from step 1.1 over the new version of the repository. Update the command line tools in your environment. The latest version of the analyze-containers repository requires XMLStarlet. For more information about installing XMLStarlet, see Command line tools . To update the images and secrets, run the createDevEnvironment.sh script from the scripts directory. ./createDevEnvironment.sh Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT variable to ACCEPT . The variable is in the licenses.conf file. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The variables are in the licenses.conf file. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y To update any deployed configs, run the deploy.sh script from the scripts directory with your config name. You must run the script twice, once with the clean task. For example: ./deploy.sh -c <config_name> -t clean ./deploy.sh -c <config_name>"
  },
  "content/managing_upgrade_env.html": {
    "href": "content/managing_upgrade_env.html",
    "title": "Upgrading",
    "keywords": "Upgrading Upgrade your config development environment to use the latest version of the analyze-containers repository, or i2 Analyze update the images, secrets, and deploy your configs. The process of upgrading the config development environment is completed in a different directory from your current environment. Prerequisites Download the tar.gz from https://github.com/i2group/analyze-containers/releases/tag/v2.3.0 . Extract the tar.gz file by using the following steps: On Windows, copy the analyze-containers-2.3.0.tar.gz file to your WSL filesystem. For example: Z:\\home\\<user> Extract the tar.gz file (On Windows, run the commands from a WSL terminal. Start > wsl ): cd /home/<user> tar -xvzf analyze-containers-2.3.0.tar.gz mv analyze-containers-2.3.0/ analyze-containers-230 Download the i2 Analyze V4.4.0 Minimal for Linux. To download i2 Analyze, follow the procedure described in Where can I download the latest i2 Products? Populate the subject of the form with Request for i2 Analyze 4.4.0 minimal toolkit for Linux . Rename the downloaded file to i2analyzeMinimal.tar.gz , then copy it to the analyze-containers-230/pre-reqs directory. Download the Microsoft JDBC Driver 9.4.1 for SQL Server from https://github.com/microsoft/mssql-jdbc/releases/tag/v9.4.1 by clicking the mssql-jdbc-9.4.1.jre11.jar asset. Copy the mssql-jdbc-9.4.1.jre11.jar file to the pre-reqs/jdbc-drivers directory. Open the devcontainer.json file inside analyze-containers-230/.devcontainer directory and in the mounts section add the path to your previous analyze-containers version. For both the source and target. For example: \"mounts\": [ ... \"source=/home/<user-name>/analyze-containers,target=/home/<user-name>/analyze-containers,type=bind,consistency=cached\" ] Open a new VS Code window, press F1 (or Cmd+Shift+P in MacOS) and type Remote-Containers: Open Folder in Container and select it. In the file explorer, navigate to your newly created analyze-containers-230 directory. You might see a prompt is displayed, this means a new dev environment is available, click rebuild to rebuild the remote dev container. To create the configuration development environment and template config, in the analyze-containers-230/scripts directory run: ./createDevEnvironment.sh The script performs the following actions: Extracts the required files from the i2 Analyze deployment toolkit Builds the required Docker images for the development environment Generates the secrets that are used in the environment Creates the configuration template The configuration template is located in /templates/config-development . Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT variable to ACCEPT . The variable is in the licenses.conf file. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The variables are in the licenses.conf file. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y There are 2 options that you can choose from when you upgrade your config development environment. Option 1 : Upgrade all of your configs to use the latest version of i2 Analyze. Option 2 : Upgrade a subset of your configs to use the latest version of i2 Analyze, and keep the remaining configs using the previous version. Regardless of the option that you choose, you will have 2 directories for each version of the analyze-containers project. If you choose option 1 and you no longer need to previous version, the manageEnvironment script can delete the analyze-containers directory and rename your analyze-containers-230 directory for you. Option 1 If you want to upgrade all of the configs in your environment to use the latest version, run the following command from the analyze-containers-230/scripts directory: ./manageEnvironment.sh -p <path-to-previous-project> -t upgrade Where <path-to-previous-project> is the absolute path to the root of your previous analyze-containers repository. This script completes the following actions: Creates a backup of the ISTORE database for each config that contains one Copies the following directories from the previous project to the current project: backups configs connector-images dev-environment-secrets gateway-schemas i2a-data i2a-extensions Upgrades the configs to the latest version of i2 Analyze and analyze-containers If you no longer need to previous version, answer yes to the prompt from the manageEnvironment.sh script. To start the deployment at the latest version with a config, run: ./deploy.sh -c <config-name> Option 2 If you want to upgrade a subset of configs to use the latest version, run the following commands from the analyze-containers-230/scripts directory: Backup the database for the configs you want to upgrade: ./manageEnvironment.sh -p <path-to-previous-project> -i <config-name> -b global-upgrade -t backup Where: <path-to-previous-project> is the absolute path to your previous analyze-containers repository <config-name> is the name of a config to backup. You can provide multiple -i options for each config you want to upgrade. Copy the configs and dependencies from the previous project to the current project: ./manageEnvironment.sh -p <path-to-previous-project> -i <config-name> -t copy Where: <path-to-previous-project> is the absolute path to your previous analyze-containers repository <config-name> is the name of a config to copy. You can provide multiple -i options for each config you want to upgrade. Run the createDevEnvironment.sh script from the analyze-containers-230/scripts directory to create the images for i2 Analyze 4.4.0. ./createDevEnvironment.sh Upgrades your configs to the latest version. For each config that you want to upgrade, run: ./deploy.sh -c <config-name> Where: <config-name> is the name of the config to upgrade."
  },
  "content/reference architecture/deploy_aws.html": {
    "href": "content/reference architecture/deploy_aws.html",
    "title": "Deploying the AWS reference architecture",
    "keywords": "Deploying the AWS reference architecture When you follow the instructions to deploy the AWS reference architecture, the scripts launch, configure, and run the AWS compute, network, storage, and other services for the i2 Analyze on AWS. The reference architecture includes AWS CloudFormation templates that automate the deployment and a deployment guide that describes the architecture. The deployment is running on AWS and you are charged by AWS for the infrastructure that is used. Consult the AWS pricing information before you run the scripts: ... By default, the AWS reference architecture is deployed on the following instances. For information about modifying these values before you deploy the reference architecture, see XXX. (TODO: populate resources and identify steps to modify the reference architecture). Prerequisites Before you can deploy the AWS reference architecture, you need to have installed and configured the following pre-requisites: Have an AWS account Install the AWS CLI v2 For more, see Installing, updating, and uninstalling the AWS CLI version 2 Configure the analyze-containers repository For more information, see Getting started with the analyze-containers repository . In your terminal, navigate to the examples/aws directory. Accepting the licenses Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT variable to ACCEPT . The variable is in the licenses.conf file. For example: LIC_AGREEMENT=ACCEPT Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The variables are in the licenses.conf file. For example: MSSQL_PID=Developer ACCEPT_EULA=Y Setting up AWS Variables Before you can deploy the AWS reference architecture, you must provide your AWS account information in the examples/aws/utils/simulated-external-variables.sh script. To set up the system to communicate with your AWS account, set the value of the ECR_BASE_NAME environment variable to your default private registry URL. For more information, see Amazon ECR private registries . Set the value of the AWS_REGION environment variable to the region name you want to deploy the architecture in. For example: ECR_BASE_NAME=\"#####.dkr.ecr.eu-west-2.amazonaws.com\" AWS_REGION=\"eu-west-2\" Creating and uploading resources to AWS The create-and-upload-aws-resources.sh script creates the configuration, database scripts, secrets, images, and runbooks then uploads them to AWS. For more information about what is created, see create-and-upload-aws-resources.sh . Run create-and-upload-aws-resources.sh : ./create-and-upload-aws-resources.sh Deploying the i2 Analyze on AWS The deploy-aws.sh script deploys i2 Analyze on AWS. For more information about what is deployed, see deploy-aws.sh . Run deploy-aws.sh : ./deploy-aws.sh"
  },
  "content/reference architecture/deploy_pre_prod.html": {
    "href": "content/reference architecture/deploy_pre_prod.html",
    "title": "Pre-production example environment",
    "keywords": "Pre-production example environment Prerequisites Before you create an example pre-production environment, you must configure the analyze-containers repository. For more information, see Getting started with the analyze-containers repository . Creating a containerized deployment After you have all of the prerequisites in place, use the example scripts and artifacts in the examples/pre-prod directory to create the reference pre-production containerized deployment. Creating the environment and configuration The createEnvironment.sh script performs a number of actions that ensure all of the artifacts for a deployment are created and in the correct locations. These actions include: Extracting the i2 Analyze minimal toolkit to the pre-reqs/i2analyze directory. Creating the i2 Analyze Liberty application Creating and populating the configuration directory structure For more information about the what the script does, see: Create environment To create the environment and configuration, run the following commands: ./createPreProdEnvironment.sh The configuration directory is created in the examples/pre-prod directory. By default, the environment is created for an Information Store and i2 Connect gateway deployment. Accepting the licenses Before you can use i2 Analyze and the tools, you must read and accept the license agreement and copyright notices in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT variable to ACCEPT . The variable is in the licenses.conf file. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The variable is in the licenses.conf file. Running the containers and start i2 Analyze To deploy and start i2 Analyze, run the following command: ./deploy.sh For more information about the actions that are completed, see Deploying i2 Analyze . Installing Certificate To access the system, the server that you are connecting from must trust the certificate that it receives from the deployment. To enable trust, install the /dev-environment-secrets/generated-secrets/certificates/externalCA/CA.cer certificate as a trusted root certificate authority in your browser and operating system's certificate store. For information about installing the certificate, see: Install Certificates with the Microsoft Management Console Setting up certificate authorities in Firefox Set up an HTTPS certificate authority in Chrome Accessing the system To connect to the deployment, the URL to use is: https://i2analyze.eia:9046/opal/ What to do next To understand how the environment is created, you can review the documentation that explains the images, containers, tools, and functions: Images and containers Tools and functions To learn how to configure and administer i2 Analyze in a containerized environment, you can complete the walkthroughs that are included in the repository: Walkthroughs"
  },
  "content/reference architecture/reference_architectures.html": {
    "href": "content/reference architecture/reference_architectures.html",
    "title": "Containerized deployment reference architectures",
    "keywords": "Containerized deployment reference architectures Containerized deployment reference"
  },
  "content/reference architecture/understanding.html": {
    "href": "content/reference architecture/understanding.html",
    "title": "Understanding the reference architecture",
    "keywords": "Understanding the reference architecture The Analyze-Containers repository includes Dockerfiles and example scripts that provide a reference architecture for creating a containerized deployment of i2 Analyze. The scripts demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. How to use the reference architecture? The repository is designed to be used with the i2 Analyze minimal toolkit. The minimal toolkit is similar to the standard i2 Analyze deployment toolkit, except that it only includes the minimum amount of application and configuration files. The i2 Analyze minimal toolkit is used to provide the artifacts that are required to build the images and provide the configuration for the deployment. Bash scripts are then used to build the images and run the containers. To demonstrate creating an example containerized deployment, complete the actions described in Pre-production example environment . The minimal toolkit also contains the tools that are used by the bash scripts to deploy, configure, and administer your deployment of i2 Analyze. The tools are in the form of JAR files that are called from shell scripts. For more information about the tools that are available and their usage, see i2 Analyze tools . Dockerfiles and images A deployment of i2 Analyze consists of the following components: Liberty Solr ZooKeeper Prometheus Grafana Optionally a database management system The Analyze-Containers repository contains the Dockerfiles that are used to build the images for each component. For Liberty, SQL Server, Prometheus and Grafana the image provided by each vendor is used. For Solr and ZooKeeper, the repository contains custom Dockerfiles that were created from the ones provided by Solr and ZooKeeper. For more information about the images and containers, see images and containers . Scripts The Analyze-Containers repository provides example scripts that you can use and leverage for your own deployment use cases. The Analyze-Containers repository contains a number of scripts that are designed to be used at various stages when working towards creating a containerized deployment. The repository also includes example artifacts that are used with the scripts. These artifacts include an example certificate authority and certificates, secrets and keys to be used with i2 Analyze, and utilities that are used by the example scripts. Walkthroughs A number of walkthroughs are provided that demonstrate how to complete configuration and administration tasks in a containerized deployment. The walkthroughs consist of a reference script that demonstrates how to complete the action, and a document that explain the process in more detail. What is deployed? When you run the provided scripts to create the example deployment, i2 Analyze is deployed in the following topology: The deployment includes: A load balancer container, using HAProxy Two Liberty containers configured for high availability A Solr cluster with two Solr containers A ZooKeeper ensemble with three ZooKeeper containers A SQL Server container A Prometheus container A Grafana container A number of \"client\" ephemeral containers are used to complete a single actions. The following client containers are used: SQL Server client Solr client For more information about the images and containers, see images and containers ."
  },
  "content/reference architecture/understanding_aws.html": {
    "href": "content/reference architecture/understanding_aws.html",
    "title": "Understanding the AWS reference architecture",
    "keywords": "Understanding the AWS reference architecture The analyze-containers repository includes yaml files and example scripts that provide a reference architecture for creating an AWS deployment of i2 Analyze. The scripts demonstrate how to upload resources, create CloudFormation Stacks and enable you to deploy, configure, and run i2 Analyze on AWS. How to use the reference architecture? You should already have an understanding of the Containerized deployment reference before starting with the AWS reference architecture. This architecture uses the same Dockerfiles and images used in the containerized deployment reference which the exception of the database management system (server) and the load balancer. To create the example AWS deployment, complete the actions described in AWS example environment . Scripts The analyze-containers repository provides example scripts that you can use and leverage for your own deployment use cases. The analyze-containers repository contains a number of scripts that are designed to be used at various stages when working towards creating an AWS deployment. The repository also includes example artifacts that are used with the scripts. These artifacts include an example certificate authority and certificates, secrets and keys to be used with i2 Analyze, CloudFormation templates, runbooks, and utilities. Walkthroughs A number of walkthroughs are provided that demonstrate how to complete configuration and administration tasks in an AWS deployment. The walkthroughs consist of a reference script that demonstrates how to complete the action, and a document that explain the process in more detail. What is deployed? When you run the provided scripts to create the example deployment, i2 Analyze is deployed in the following topology: TODO! The deployment includes: A load balancer (AWS ELB) Liberty containers running in ECS as Fargate A Solr cluster with two Solr containers deployed in EC2 instances A ZooKeeper ensemble with three ZooKeeper containers deployed in EC2 instances SQL Server running in EC2 with EBS A number of \"client\" ephemeral containers are used to complete a single actions. The following client containers are used: SQL Server client Solr client For more information about the artifacts that are used, see: Stacks Runbooks AWS Tools"
  },
  "content/runbooks/helper_runbooks.html": {
    "href": "content/runbooks/helper_runbooks.html",
    "title": "Helper runbooks",
    "keywords": "Helper runbooks i2a-UpdateScripts Description This runbook is an automation pulling down i2a-scripts on all required EC2 instances Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group updateScripts Pull down and untar i2a-scripts.tar.gz N/A update-tools/update"
  },
  "content/runbooks/runbooks.html": {
    "href": "content/runbooks/runbooks.html",
    "title": "Runbooks",
    "keywords": "Runbooks The documentation in this section describes the runbooks that are used to deploy i2 Analyze in an AWS environment."
  },
  "content/runbooks/solr_runbooks.html": {
    "href": "content/runbooks/solr_runbooks.html",
    "title": "Solr runbooks",
    "keywords": "Solr runbooks The following runbooks are used to deploy, start, and stop Solr and ZooKeeper. i2a-SolrFirstRun Description Initialize, configure, and start Solr and ZooKeeper Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group runZookeeper1 Start ZooKeeper container on the zk1 instance run-zk.sh solr-first-run/run-zk runZookeeper2 Start ZooKeeper container on the zk2 instance run-zk.sh solr-first-run/run-zk runZookeeper3 Start ZooKeeper container on the zk3 instance run-zk.sh solr-first-run/run-zk configureZkForSolr Create is_cluster and upload solr/security.json configure-zk-for-solr.sh solr-first-run/configure-zk-for-solr runSolr Start Solr containers on all available solr instances run-solr.sh solr-first-run/run-solr configureSolr Create and configure Solr collections configure-solr.sh solr-first-run/configure-solr i2a-SolrStart Description Start previously created ZooKeeper and Solr containers Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group runZookeeper1 Start ZooKeeper container on the zk1 instance run-zk.sh solr-start/run-zk runZookeeper2 Start ZooKeeper container on the zk2 instance run-zk.sh solr-start/run-zk runZookeeper3 Start ZooKeeper container on the zk3 instance run-zk.sh solr-start/run-zk RUN_SOLR Start Solr containers on all available solr instances run-solr.sh solr-start/run-solr <!-- markdownlint-configure-file { \"MD024\": false } -->"
  },
  "content/runbooks/sqlserver_runbooks.html": {
    "href": "content/runbooks/sqlserver_runbooks.html",
    "title": "SQL Server runbooks",
    "keywords": "SQL Server runbooks i2a-SqlServerFirstRun Description This runbook is an automation for initializing and configuring the Information Store in SQL Server Parameters Name Description DeploymentName The name of the deployment"
  },
  "content/security and users/aws_security.html": {
    "href": "content/security and users/aws_security.html",
    "title": "",
    "keywords": ""
  },
  "content/security and users/db_users.html": {
    "href": "content/security and users/db_users.html",
    "title": "Database roles, users, and logins",
    "keywords": "Database roles, users, and logins During the deployment, administration, and use of i2 Analyze, a number of different actions are completed against the Information Store database. These actions can be separated into different categories that are usually completed by users with differing permissions. In SQL Server, you can create a number of database roles and assign users to roles. In the example deployment, a number of different roles and users are used to demonstrate the types of roles that might complete each action. Database roles In the example, the following roles are used: Role Description DBA_Role The DBA_Role is used to perform database administrative tasks. External_ETL_Role The External_ETL_Role is used to move data from an external system into the staging tables in the Information Store database. i2_ETL_Role The i2_ETL_Role is used to read data from the staging tables and ingest it into - and delete it from - the Information Store database. i2analyze_Role The i2analyze_Role is used to complete actions required by the Liberty application. For example, returning results for Visual Queries. Database role permissions Each database role requires a specific set of permissions to complete the actions attributed to them. DBA_Role The DBA_Role requires permissions to: Set up and maintain the database management system and Information Store database. Create and modify the database management system objects. For example, bufferpools, tablespoons, and filegroups. Create and modify database objects. For example, tables, views, indexes, sequences. Troubleshoot performance or other issues. For example, has all privileges on all tables. This can be restricted in some environments. Configure high availability. Manage backup and recovery activities. Additionally, the role requires access to two roles in the msdb database: SQLAgentUserRole - for more information, see SQLAgentUserRole Permissions db_datareader - for more information, see Fixed-Database Roles These roles are required for database creation, to initialize the deletion-by-rule objects, and to create the SQL Server Agent jobs for the deletion rules. Note: The configureDbaRolesAndPermissions.sh script is run during deploy to grant the correct permissions. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes All CREATE TABLE, CREATE VIEW, CREATE SYNONYM Required to create the database objects. All ALTER, SELECT, UPDATE, INSERT, DELETE, REFERENCES Required to make changes for maintaining the database. IS_Core EXECUTE Required for deletion-by-rule and database configuration. IS_Public EXECUTE Required to run the stored procedures for deletion-by-rule. The following table provides an overview of the permissions required on the schemas in the msdb database: Schema Permissions Notes dbo SQLAgentUserRole Required to create the deletion jobs during deployment, and to manage the deletion job schedule. dbo db_datareader Required to create the deletion job schedule. The following table provides an overview of the permissions required on the schemas in the master database: Schema Permissions Notes All VIEW SERVER STATE Required for deletion-by-rule automated jobs via the SQL Server Agent. sys EXECUTE ON fn_hadr_is_primary_replica Required for deletion-by-rule automated jobs. The configureDbaRolesAndPermissions.sh script is used to configure the DBA user with all the required role memberships and permissions. External_ETL_Role The External_ETL_Role requires permissions to move data from external systems into the Information Store staging tables. For example, it can be used by an ETL tool - such as DataStage or Informatica - to move and transform data that results in populated staging tables in the Information Store staging schema. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging SELECT, UPDATE, INSERT, DELETE Required to populate the staging tables with date to be ingested or deleted. In addition to these permissions, in an environment running SQL server in a Linux container, users with this role must also be a member of the sysadmin group in order to perform BULK INSERT into the external staging tables. The addEtlUserToSysAdminRole.sh script is used to make the etl user a member of the sysadmin fixed-server role. i2_ETL_Role The i2_ETL_Role requires permissions to use the i2 Analyze ingestion tools to ingest data from the staging tables into the Information Store. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging ALTER, SELECT, UPDATE, INSERT, DELETE Required by the ingestion tools to create and modify objects during the ingestion process. IS_Stg ALTER, SELECT, UPDATE, INSERT, DELETE Required by the ingestion tools to create and modify objects during the ingestion process. IS_Meta SELECT, UPDATE, INSERT UPDATE and INSERT are required to update the ingestion history table. SELECT is required to read the schema meta data. IS_Data ALTER, SELECT, UPDATE, INSERT, DELETE ALTER is required to drop and create indexes and update statistics as part of the ingestion process. IS_Public ALTER, SELECT ALTER is required to delete and create synonyms when enabling merged property views. IS_Core SELECT, EXECUTE Required to check configuration of the database. i2analyze_Role The i2analyze_Role requires permissions to complete actions required by the Liberty application. These actions include: Visual Query, Find Path, Expand, Add to chart, Upload, and Online upgrade. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging ALTER, SELECT, UPDATE, INSERT, DELETE Required to run deletion-by-rule jobs. IS_Stg ALTER, SELECT, UPDATE, INSERT, DELETE Required to upload and delete records via Analyst's Notebook Premium and to run deletion-by-rule jobs. IS_Meta SELECT, UPDATE, INSERT, DELETE DELETE is required to process the ingestion history queue. IS_Data SELECT, UPDATE, INSERT, DELETE UPDATE, INSERT, and DELETE are required by deletion-by-rule and to upload and delete records via Analyst's Notebook Premium. IS_Core SELECT, UPDATE, INSERT, DELETE Required for online upgrade. IS_VQ SELECT, UPDATE, INSERT, DELETE Required to complete Visual Queries. IS_FP SELECT, INSERT, EXEC Required to complete Find Path operations. IS_WC SELECT, UPDATE, INSERT, DELETE Required to work with Web Charts. IS_PUBLIC EXEC Grants execute permission to stored procedures in the IS_PUBLIC schema. Required to create and send Alerts. The database backup operator role The example also demonstrates how to perform a database backup. The dbb user will perform this action and is a member of the SQL Server built-in role, db_backupoperator . This gives this user the correct permissions for performing a backup and nothing else. For more information, see Fixed-Database Roles for more details. Database users and logins In the example, a user is created for each role described previously. These users are then used throughout the deployment and administration steps to provide a reference for when each role is required. The following users and logins are used in the example: User and login Description Secrets sa The system administrator user. The sa user has full permissions on the database instance. This user creates the Information Store database, roles, users, and logins. The password is in the sa_PASSWORD file in the dev-environment-secrets/generated-secrets/sqlserver directory. i2analyze The i2analyze user is a member of the i2analyze_Role . The password is in the i2analyze_PASSWORD file in the dev-environment-secrets/generated-secrets/sqlserver directory. etl The etl user is a member of External_ETL_Role . The password is in the etl_PASSWORD file in the dev-environment-secrets/generated-secrets/sqlserver directory. i2etl The i2etl user is a member of i2_ETL_Role . The password is in the i2etl_PASSWORD file in the dev-environment-secrets/generated-secrets/sqlserver directory. dba The dba user is a member of DBA_Role . The password is in the dba_PASSWORD file in the dev-environment-secrets/generated-secrets/sqlserver directory. dbb The dbb user is the database backup user, it is a member of the SQL Server built in role: db_backupoperator . The password is in the dbb_PASSWORD file in the dev-environment-secrets/generated-secrets/sqlserver directory. The sa user and login exists on the base SQL Server image. The sa user is used to create the following artifacts: Database: ISTORE Roles: i2analyze_Role , External_ETL_Role, , i2_ETL_Role and DBA_Role Logins: i2analyze , etl , i2etl , dba , and dbb Users: i2analyze , etl , i2etl , dba , and dbb The roles and users must be created after the Information Store database is created. Creating the roles The sa user is used to run the createDbRoles.sh client function that creates the i2Analyze_Role , External_ETL_Role , i2_ETL_Role , and DBA_Role roles. To create the roles, the createDbRoles.sh script is run using the runSQLServerCommandAsSA client function. This function uses an ephemeral SQL Server client container to create the database roles. For more information about the client function, see: runSQLServerCommandAsSA createDbRoles.sh In the example, the createDbRoles.sh script is called in deploy.sh . Note: All the secrets required at runtime by the client container can be made available by providing a file path to the secret which is converted to an environment variable by the docker container. For example, to provide the SA_USERNAME environment variable to the client container, a file containing the secret is declared in the docker run command: -e \"SA_USERNAME_FILE=${CONTAINER_SECRETS_DIR}/SA_USERNAME_FILE\" The file name can be anything, but the environment variable is fixed. For more information see, managing container security Create the login and user Use the sa user to create the login and the user on the ISTORE , and make the user a member of the role. You can use an ephemeral SQL Client container to create the login and the user. The createDbLoginAndUser.sh script in /images/sql_client/db-scripts is used to create the login and user. The scripts are called from the deploy.sh scripts. The createDbLoginAndUser function The createDbLoginAndUser function uses an ephemeral SQL Client container to create the database administrator login and user. The login and user are created by the sa user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The createDbLoginAndUser.sh script is used to create the login and user. The function requires the following environment variables to run: Environment variable Description SA_USERNAME The sa username. SA_PASSWORD The sa user password. DB_USERNAME The database user name. DB_PASSWORD The database user password. DB_SSL_CONNECTION Whether to use SSL for connection. SSL_CA_CERTIFICATE The path to the CA certificate. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_ROLE The name of the role that user will be added to. It has to be one of the roles from this list . Changing SA password In a Docker environment, you must start the SQL Server as the existing sa user before you can modify the password. The changeSAPassword function The changeSAPassword function uses an ephemeral SQL Client to change the sa user password. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The changeSAPassword.sh script is used to change the password. The function requires the following environment variables to run: Environment variable Description SA_USERNAME The sa username SA_OLD_PASSWORD The current sa password. SA_NEW_PASSWORD The new sa password. DB_SSL_CONNECTION Whether to use SSL for connection. SSL_CA_CERTIFICATE The path to the CA certificate. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store."
  },
  "content/security and users/security.html": {
    "href": "content/security and users/security.html",
    "title": "Managing container security",
    "keywords": "Managing container security SSL certificates in the deployment The example deployment is configured to use SSL connections for communication between clients and i2 Analyze, and between the components of i2 Analyze. To achieve this, the appropriate certificate authorities and certificates are used. The generateSecrets.sh script is used to simulate the process of creating the required keys and acquiring certificate authority-signed certificates. Note: The keys and certificates used are set to expire after 90 days. To use the certificates for longer than this, you must run the generateSecrets.sh script again. Certificate Authorities (CA) In the example, two CAs are used to provide trust: Internal CA The internal CA is to provide trust for the containers that are used for the components of i2 Analyze. Each container's certificates are signed by the internal CA. External CA The external CA is to provide trust for external requests to the i2 Analyze service via the load balancer. In our example, the external certificate authority is generated for you. However, in production a real certificate should be used. Container certificates To communicate securely using TLS each container requires the following certificates: Private key Certificate key Internal certificate authority The containers will generate truststores and keystores based on the keys provided to the container. For more information about how the keys are passed to the containers securely please see Secure Environment variables . Secure communication between containers When the components communicate, the CA certificate is used to establish trust of the container certificate that is received. Each container has its own private key ZooKeeper requires client authentication to initiate communication. The i2 Analyze, i2 Analyze Tool, and Solr client containers require container certificates to authenticate with ZooKeeper. Creating keys and certificates The following diagram shows a simplified sequence of creating a container certificate from the certificate authority and using it to establish trust: The certificate authority's certificate is distributed to the client. The private key is generated on the server. In the generateSecrets.sh script, the key is created by: openssl genrsa -out server.key 4096 The public part of the private key is used in a Certificate Signing Request (CSR). In the generateSecrets.sh script, this is completed by: openssl req -new -key server.key -subj \"/CN=solr1.eia\" -out key.csr The common name that is used for the certificate is the server's fully qualified domain name. The CSR is sent to the certificate authority (CA). The CA signs and returns a signed certificate for the server. In the generateSecrets.sh script, the CA signing the certificate is completed by: openssl x509 -req -sha256 -CA CA.cer -CAkey CA.key -days 90 -CAcreateserial -CAserial CA.srl -extfile x509.ext -extensions \"solr\" -in key.csr -out server.cer When communication is established, the container certificate is sent to the client. The client uses it's copy of the CA certificate to verify that the container certificate was signed by the same CA. Password generation The example simulates secrets management performed by various secrets managers provided by cloud vendors. The generateSecrets.sh generates these secrets and populates the dev-environment-secrets/simulated-secret-store with secrets required for each container. The docker desktop does not support secrets, but the example environment example simulates this by mounting the secrets folder. For more information see Manage sensitive data with Docker secrets . After these passwords have been generated, they can be uploaded to a secrets manager. Alternatively you can use a secrets manager to generate your passwords. Solr Basic Authentication Solr authentication can be enabled using the BasicAuthPlugin. The basic auth plugin defines users, user roles and passwords for users. For the BasicAuthPlugin to be enabled, solr requires a security.json file to be uploaded. In our example the security.json file is created by the generateSecrets.sh and located in dev-environment-secrets/generated-secrets/solr/security.json . For more information about solr authentication see Basic Authentication Plugin Prometheus Basic Authentication Prometheus authentication can be enabled using the web-config.yml. The YAML file defines users and passwords for users. In our example the password is the same as the username. Grafana Basic Authentication Grafana authentication can be enabled by passing GF_SECURITY_ADMIN_USER and GF_SECURITY_ADMIN_PASSWORD environment variables to the container. In our example the password is created by generateSecrets.sh and located in dev-environment-secrets/generated-secrets/grafana/admin_PASSWORD . Secure Environment variables In general secrets used by a particular container can be supplied via an environment variable containing the path to a file containing the secret, or an environment variable specifying the literal secret value, for example: Note: Secrets can be passwords, keys or certificates. docker run --name solr1 -d --net eia --secret source=SOLR_SSL_KEY_STORE_PASSWORD,target=SOLR_SSL_KEY_STORE_PASSWORD \\ -e SOLR_SSL_KEY_STORE_PASSWORD_FILE=\"/run/secrets/SOLR_SSL_KEY_STORE_PASSWORD_FILE\" or docker run --name solr1 -d --net eia -e SOLR_SSL_KEY_STORE_PASSWORD=\"jhga98u43jndfj\" The docker files in the analyze-containers repository have been modified to accept either. The convention is that the environment variable must match the property being set with \"_file\" appended if the secret is in a file, and without if a literal value is being used instead. In the example scripts, each container gets the relevant key stores mounted along with the correct secrets files in a secrets directory. NOTE: By default this is set as CONTAINER_SECRETS_DIR=\"/run/secrets\" in the common variables file. All the containers use the same environment variables to define the location of certificates. These are then used to generate appropriate artifacts for the particular container. There is also a standard way of turning on and off server SSL. Security switching variables Environment variable Description SERVER_SSL Can be set to true or false . If set to true , the container is configured to use encrypted connections. LIBERTY_SSL_CONNECTION Can be set to true or false . If set to true , connections to the Liberty container use an encrypted connection. DB_SSL_CONNECTION Can be set to true or false . If set to true , connections to the database use an encrypted connection SOLR_ZOO_SSL_CONNECTION Can be set to true or false . If set to true , connections to ZooKeeper and Solr use an encrypted connection. GATEWAY_SSL_CONNECTION Can be set to true of false . If set to true , connections to i2 Connect connectors use an encrypted connection. SSL_ENABLED Can be set to true or false . If set to true , the connector container communicates with the i2 Connect gateway using an encrypted connection. Security environment variables Environment variable Description SSL_PRIVATE_KEY The private key for the container certificate. SSL_CERTIFICATE The container certificate. SSL_CA_CERTIFICATE The Certificate Authority certificate. SSL_OUTBOUND_PRIVATE_KEY The private key for the Liberty container, which is used for outbound connections. SSL_OUTBOUND_CERTIFICATE The private certificate for the Liberty container, which is used for outbound connections. SSL_OUTBOUND_CA_CERTIFICATE_FILE The certificate authority used for verifying outbound connections. <!-- cspell:ignore jhga98u43jndfj -->"
  },
  "content/security and users/security_users.html": {
    "href": "content/security and users/security_users.html",
    "title": "Security and users",
    "keywords": "Security and users The documentation in this section describes how security and users are configured for a deployment of i2 Analyze in a containerized environment."
  },
  "content/stacks/solr_stack.html": {
    "href": "content/stacks/solr_stack.html",
    "title": "The Solr stack",
    "keywords": "The Solr stack Solr CloudFormation template in the i2a-stack-solr.yaml file creates: 3 ZooKeeper instances Solr Launch template 2 Solr instances SSM parameters for ZooKeeper and Solr ZooKeeper stack The ZooKeeper stack is based on the i2a-stack-docker-ec2.yaml CloudFormation template and uses a launch template to complete the following actions: Downloads i2a-scripts from the S3 resources bucket Installs Docker Downloads the zk image from AWS ECR Sets up CloudWatch ZooKeeper stack tags Tags: - Key: \"instance-name\" Value: \"zk1\" - Key: \"stack-name\" Value: !Ref DeploymentName ZooKeeper stack security groups Each ZooKeeper instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) ZkSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId Solr stack Solr Launch Template This Launch templates specifies the Solr instance configuration information, such as security groups and the commands required to initialize each Solr instance. EBS Volume Each Solr instance has a dedicated EBS Volume. The EBS Volume is in the /dev/sdh directory. (TODO: Add more info what is this for) EFS Volume Each Solr instance has dedicated EFS Storage to store data. EFS Storage is in the /var/solr directory. (TODO: Add more info what is this for) Solr stack security groups Each Solr instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) SolrSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId EFSSecurityGroupId Solr stack tags Tags: - Key: \"Name\" Value: !Sub \"${DeploymentName}-Solr-Instance\" - Key: \"stack-name\" Value: !Ref DeploymentName - Key: \"instance-name\" Value: \"solr\" Parameters DeploymentName: Type: String Description: Deployment Name PrivateSubnetAId: Type: AWS::EC2::Subnet::Id Description: Private subnet id to use for the instance IntraSubnetEncryption: Type: String Description: When set to true, uses an encrypted connection AllowedValues: - \"false\" - \"true\" Default: \"false\" ResourcesS3Bucket: Type: String Description: The AWS S3 bucket for CloudFormation templates ZkInstanceType: Type: String Description: EC2 instance type to use for ZooKeeper AllowedValues: - t3a.medium - t3a.large - t3.medium - t3.large Default: \"t3a.medium\" ConstraintDescription: Must be a valid EC2 instance type SolrInstanceType: Type: String Description: EC2 instance type to use for Solr AllowedValues: - r5.large - r5.xlarge - r5.2xlarge - r5b.large - r5b.xlarge - r5b.2xlarge - r5d.large - r5d.xlarge - r5d.2xlarge Default: \"r5.large\" ConstraintDescription: Must be a valid EC2 instance type SolrEBSVolumeSize: Type: Number Description: Volume size to use for Solr, in GiBs Default: 8 SolrEBSVolumeType: Type: String Description: Volume type to use for Solr AllowedValues: - gp3 - io1 - io2 Default: 'gp3' SolrEBSVolumeIops: Type: Number Description: > The number of I/O operations per second provisioned for the volume to use for Solr The following are the supported values for each volume type: gp3: 3,000-16,000 IOPS io1: 100-64,000 IOPS io2: 100-64,000 IOPS' Default: 3000 LatestAmiId: Type: \"AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>\" Description: This gets the latest AMI image ID. Do NOT change Default: \"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\" IamInstanceProfile: Type: String Description: The name of the shared instance template stack Default: \"SSMInstanceProfile\" (TODO: do we need to add info about all ssm params being created as a part of this stack?)"
  },
  "content/stacks/sqlserver_stack.html": {
    "href": "content/stacks/sqlserver_stack.html",
    "title": "The SQL Server stack",
    "keywords": "The SQL Server stack SQL Server CloudFormation template in the i2a-sqlserver.yaml file creates: SQL Server instance SSM parameters for SQL Server SQL Server stack The SQL Server stack uses the latest ami image and uses a launch template to complete the following actions: Installs jQuery Installs Amazon EFS client Creates a mount directory Mounts EFS file system Sets up SQL Server SQL Server stack tags Tags: - Key: Name Value: !Join [\"\", [!Ref \"AWS::StackName\", -SQLServer2019-Instance]] - Key: \"instance-name\" Value: \"sqlserver\" - Key: \"stack-name\" Value: !Ref DeploymentName SQL Server stack security groups The SQL Server instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) SQLServerSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId EFSSecurityGroupId EBS Volume The SQL Server instance has a dedicated EBS Volume. The EBS Volume is in the /dev/sdh directory. (TODO: Add more info what is this for) Parameters Parameters: DeploymentName: Description: Deployment Name Type: String PrivateSubnetAId: Description: Private subnet id to use for the instance Type: AWS::EC2::Subnet::Id InstanceType: Description: EC2 instance type to use for SQL Server Type: String AllowedValues: - m5.xlarge - m5.2xlarge - m5.4xlarge - m5.8xlarge - m5.12xlarge - m5.16xlarge - m5.24xlarge - r5.xlarge - r5.2xlarge - r5.4xlarge - r5.8xlarge - r5.12xlarge - r5.16xlarge - r5.24xlarge - i3.xlarge - i3.2xlarge - i3.4xlarge - i3.8xlarge - i3.16xlarge Default: m5.xlarge # Vendor recommended ConstraintDescription: must be a valid EC2 instance type supported by SQL Server. LatestAmiId: Description: This gets the latest SQL Server AMI image ID. Do NOT change Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id> Default: /aws/service/ami-windows-latest/amzn2-x86_64-SQL_2019_Standard DBEBSVolumeSize: Description: Volume size to use for database, in GiBs Type: Number Default: 50 DBEBSVolumeType: Description: Volume type to use for database Type: String AllowedValues: - gp3 - io1 - io2 Default: gp3 DBEBSVolumeIops: Description: 'The number of I/O operations per second provisioned for the volume to use for the database. The following are the supported values for each volume type: gp3: 3,000-16,000 IOPS io1: 100-64,000 IOPS io2: 100-64,000 IOPS' Type: Number Default: 3000 IamInstanceProfile: Description: The name of the shared instance template stack Type: String Default: SSMInstanceProfile (TODO: do we need to add info about all ssm params being created as a part of this stack?)"
  },
  "content/stacks/stacks.html": {
    "href": "content/stacks/stacks.html",
    "title": "Stacks",
    "keywords": "Stacks The documentation in this section describes the stacks that are used to deploy i2 Analyze in an AWS environment."
  },
  "content/tools and functions/aws_tools.html": {
    "href": "content/tools and functions/aws_tools.html",
    "title": "AWS tools",
    "keywords": "AWS tools create-and-upload-aws-resources.sh deploy-aws.sh create-and-upload-aws-resources.sh script The create-and-upload-aws-resources.sh script creates the configuration, secrets, images, and runbooks then uploads them to AWS. The following functions are used when you run the script: Set the dependency tag The setDependenciesTag function sets the value of the dependency tag to the I2ANALYZE_VERSION defined in the version file. This ensures all the core images are labeled in AWS ECR for that version of the product and any other deployment can reuse them. (TODO: populate with more info for CIR-781) Creates the configuration The AWS reference architecture uses an example configuration. The configuration is created by the createBaseConfiguration function. The generateSolrConfiguration function generates the configuration for Solr from the base configuration. Creates the S3 buckets The createS3Bucket function creates two AWS S3 buckets using the AWS CLI: analyze-containers-aws-ref-arch-config stores the configuration. analyze-containers-aws-ref-arch-resources stores other resources. You can use the S3 management console in AWS to view the contents of the buckets. Creates the secrets The createSecrets function generates all the secrets and passwords that are required for the deployment and pushes them to the secret store (TODO - any doc/reference for this) and shreds the local copy of these secrets. Creates the images The createImages function builds the Docker images that are required for the deployment and tags them with the I2ANALYZE_VERSION defined in the version file. The built images are uploaded to your AWS Elastic Container Registry. To see your AWS ECR, go to ECR console The function uses the utils/buildImages.sh script to build the images, and the examples/aws/utils/push-images.sh script to push the images to AWS. Upload artifacts The uploadArtifacts function is used to upload the configuration to the analyze-containers-aws-ref-arch-config S3 bucket. The function also uploads the CloudFormation templates and i2a-scripts to the analyze-containers-aws-ref-arch-resources bucket. Create runbooks The createRunbooks function creates the runbooks that are required to start and stop the system and uploads them to AWS System Manager . For more information about the runbooks, see Runbooks . deploy-aws.sh script The script creates all the stacks that are required for the i2 Analyze deployment. The stacks are created in the following order: vpc security storage launch-templates sqlserver solr admin-client For more information about the stacks, see Stacks . After the stacks are created, the following runbooks are used to start the system: i2a-UpdateScripts i2a-SolrFirstRun i2a-SqlServerFirstRun For more information about the runbooks, see Runbooks ."
  },
  "content/tools and functions/client_functions.html": {
    "href": "content/tools and functions/client_functions.html",
    "title": "Client utilities",
    "keywords": "Client utilities The clientFunctions.sh file contains functions that you can use to perform actions against the server components of i2 Analyze. Secrets utilities The getSecret function gets a secret such as a password for a user. Status utilities The status utilities report whether a component of i2 Analyze is live. waitForIndexesToBeBuilt The waitForIndexesToBeBuilt function sends a request to the admin/indexes/status endpoint. If the response indicates no indexes are still BUILDING the function returns and prints a message to indicate the indexes have been built. If the indexes are not all built after 5 retries the function will print an error and exit. waitForConnectorToBeLive This function takes (1) the fully qualified domain name of a connector and (2) the port of the connector as its arguments. The waitForConnectorToBeLive function sends a request to the connector's /config endpoint. If the response is 200 , the connector is live. If the connector is not live after 50 tires the function will print an error and exit. getAsyncRequestStatus This function takes (1) the request id of the asynchronous request to get the status of. The getAsyncRequestStatus function makes a request to the Asynchronous Collection API and check the state is marked as completed in the JSON response returned. If the state is not marked as completed , the function returns the response message which contains any error messages that are reported with the asynchronous request. For more information about the Asynchronous Collection API, see REQUESTSTATUS: Request Status of an Async Call . getSolrStatus This function takes (1) a timestamp that is used to specify the point in the logs after which the logs are monitored. The getSolrStatus function used the logs from the Liberty container, and uses a grep command to find specific component availability messages. The function returns the entries in the logs that match message in the grep command. If no matching message is found, the function prints the following error and exits: \"No response was found from the component availability log (attempt: ${i}). Waiting...\" Database Security Utilities changeSAPassword The changeSAPassword function uses the generated secrets to call the changeSAPassword.sh with the initial (generated) sa password and the new (generated) password. For more information, see changeSAPassword . createDbLoginAndUser The createDbLoginAndUser function takes a user and a role as its arguments. The function creates a database login and user for the provided user , and assigns the user to the provided role . Execution utilities The execution utilities enable you to run commands and tools from client containers against the server components of i2 Analyze. runSolrClientCommand The runSolrClientCommand function uses an ephemeral Solr client container to run commands against Solr. For more information about the environment variables and volume mounts that are required for the Solr client, see Running a Solr client container . The runSolrClientCommand function takes the command you want to run as an argument. For example: runSolrClientCommand \"/opt/solr/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd clusterprop -name urlScheme -val https For more information about commands you can execute using the Solr zkcli , see Solr ZK Command Line Utilities runi2AnalyzeTool The runi2AnalyzeTool function uses an ephemeral i2 Analyze Tool container to run the i2 Analyze tools. For more information about the environment variables and volume mounts that are requires for the i2Analyze tool, see Running an i2 Analyze Tool container The runi2AnalyzeTool function takes the i2 tool you want to run as an argument. For example: runi2AnalyzeTool \"/opt/i2-tools/scripts/updateSecuritySchema.sh\" runi2AnalyzeToolAsExternalUser The runi2AnalyzeToolAsExternalUser function uses an ephemeral i2 Analyze Tool container to run commands against the i2 Analyze service via the load balancer as an external user. The container contains the required secrets to communicate with the i2 Analyze service from an external container. For example, if you would like to send a Curl request to the load balancer stats endpoint, run: runi2AnalyzeToolAsExternalUser bash -c \"curl \\ --silent \\ --cookie-jar /tmp/cookie.txt \\ --cacert /tmp/i2acerts/CA.cer \\ --request POST \\\"${FRONT_END_URI}/j_security_check\\\" \\ --header 'Origin: ${FRONT_END_URI}' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'j_username=Jenny' \\ --data-urlencode 'j_password=Jenny' \\ && curl \\ --silent \\ --cookie /tmp/cookie.txt \\ --cacert /tmp/i2acerts/CA.cer\\ \\\"${FRONT_END_URI}/api/v1/admin/indexes/status\\\"\" runSQLServerCommandAsETL The runSQLServerCommandAsETL function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the etl user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsETL function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsETL bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -S \\${DB_SERVER} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} -Q \\\"BULK INSERT IS_Staging.E_Person FROM '/var/i2a-data/law-enforcement-data-set-2-merge/person.csv' WITH (FORMATFILE = '/var/i2a-data/law-enforcement-data-set-2-merge/sqlserver/format-files/person.fmt', FIRSTROW = 2)\\\"\" runSQLServerCommandAsi2ETL The runSQLServerCommandAsi2ETL function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the i2etl user, such as executing generated drop/create index scripts, created by the ETL toolkit. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsi2ETL function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsi2ETL bash -c \"${SQLCMD} ${SQLCMD_FLAGS} \\ -S \\${DB_SERVER},${DB_PORT} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} \\ -i /opt/database-scripts/ET5-drop-entity-indexes.sql\" runSQLServerCommandAsFirstStartSA The runSQLServerCommandAsFirstStartSA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the sa user with the initial SA password. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . runSQLServerCommandAsSA The runSQLServerCommandAsSA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the sa user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsSA function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsSA \"/opt/i2-tools/scripts/database-creation/runStaticScripts.sh\" runSQLServerCommandAsDBA The runSQLServerCommandAsDBA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the dba user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsDBA function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsDBA \"/opt/i2-tools/scripts/clearInfoStoreData.sh\" runSQLServerCommandAsDBB The runSQLServerCommandAsDBB function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the dbb (the backup operator) user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsDBB function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsDBA \"runSQLServerCommandAsDBB bash -c \" \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"USE ISTORE; BACKUP DATABASE ISTORE TO DISK = '/backup/istore.bak' WITH FORMAT;\\\"\"\" runEtlToolkitToolAsi2ETL The runEtlToolkitToolAsi2ETL function uses an ephemeral ETL toolkit container to run ETL toolkit tasks against the Information Store using the i2 ETL user credentials. For more information about running the ETL Client container and the environment variables required for the container, see ETL Client . For more information about running the ETL toolkit container and the tasks that you can run, see ETL The runEtlToolkitToolAsi2ETL function takes command that you want to run as an argument. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/addInformationStoreIngestionSource --ingestionSourceName EXAMPLE_1 --ingestionSourceDescription EXAMPLE_1\" runEtlToolkitToolAsDBA Some ETL tasks must be performed by the DBA. The runEtlToolkitToolAsDBA function used the same ephemeral ETL toolkit container but uses the DBA user instead of the ETL user. For more information about running the ETL Client container and the environment variables required for the container, see ETL Client . For more information about running the ETL toolkit container and the tasks that you can run, see ETL The runEtlToolkitToolAsDBA function takes command that you want to run as an argument. For example: runEtlToolkitToolAsDBA bash -c \"/opt/i2/etltoolkit/enableMergedPropertyValues --schemaTypeId ET5\" Volume utilities getVolume The getVolume function uses an ephemeral Red Hat UBI Docker image to update a local directory with the contents of a specified volume. The getVolume function takes the local directory, volume name, and a directory in that volume as arguments. For example, to get the contents from the run/secrets directory in the liberty1_secrets volume into your local /dev-environment-secrets/simulated-secret-store directory, run: getVolume \"/dev-environment-secrets/simulated-secret-store\" \"liberty1_secrets\" \"/run/secrets\" updateVolume The updateVolume function uses an ephemeral Red Hat UBI Docker image to update a volume with the contents of a local directory. The updateVolume function takes the local directory, volume name, and a directory in that volume as arguments. For example, to update the the run/secrets directory in the liberty1_secrets volume with the content of the local /dev-environment-secrets/simulated-secret-store directory, run: updateVolume \"/dev-environment-secrets/simulated-secret-store\" \"liberty1_secrets\" \"/run/secrets\""
  },
  "content/tools and functions/common_variables.html": {
    "href": "content/tools and functions/common_variables.html",
    "title": "Common Variables",
    "keywords": "Common Variables All scripts in the containerized environment use variables to configure the system such as location of file paths, host names, port etc These variables are all stored in a central script in utils/commonVariables.sh script. The following descriptions explains the variables in the commonVariables.sh script in more detail. Image names This section contains the image names for all the images built by the buildImages.sh script. These variables are also used by the clientFunctions.sh and severFunctions.sh scripts. Container names This section contains the container names for all the containers run in the environment. User names This section contains all the usernames used by the servers in the containerized environment. Liberty variables This section contains the liberty variables for the chosen deployment pattern. Gateway variables This section contains the variables definition for the i2Connect gateway configuration. The values of these variables are generated during deployment. User variables This section contains user variables."
  },
  "content/tools and functions/create_configuration.html": {
    "href": "content/tools and functions/create_configuration.html",
    "title": "Creating the configuration",
    "keywords": "Creating the configuration Before you can deploy i2 Analyze, the configuration for the deployment must exist. You can create the configuration by running the utils/createConfiguration.sh script. The following descriptions explain what the createConfiguration.sh script does in more detail. Creates the base configuration The configuration is created in examples/pre-prod/ . The configuration is created from the all-patterns base configuration in the minimal toolkit. The base configuration can be used for any of the deployment patterns, however some properties are for specific patterns. By default, a schema and security schema are specified. The schemas that are used are based on the law-enforcement-schema.xml . Copies the JDBC drivers The JDBC drivers are copied from the pre-reqs/jdbc-drivers directory to the configuration/environment/common/jdbc-drivers directory in your configuration. Configures Form Based Authentication The createConfiguration.sh scripts configures form based authentication in the examples/pre-prod/configuration/fragments/common/WEB-INF/classes/server.extensions.xml ."
  },
  "content/tools and functions/create_environment.html": {
    "href": "content/tools and functions/create_environment.html",
    "title": "Creating the environment",
    "keywords": "Creating the environment Before you can deploy i2 Analyze in a containerized environment, the local environment must be created. The local environment requires the prerequisites to be in the correct locations, and copies some artifacts and tools from the i2 Analyze toolkit. You can create the local environment by running the utils/createEnvironment.sh script. The following descriptions explain what the createEnvironment.sh script does in more detail. Extracts the i2 Analyze minimal toolkit The script extracts the toolkit tar.gz located in the pre-reqs directory into the the pre-reqs/i2Analyze directory. Populate image clients The createEnvironment.sh script creates populate client images with the contents of the i2-tools & scripts folder of the toolkit. ETL toolkit The createEnvironment.sh script creates the ETL toolkit that is built into the etl toolkit image. The etl toolkit is created in images/etl_client/etltoolkit . Example connector application The createEnvironment.sh script creates the example connector that is built into the connector image. The application consists of the contents of i2analyze/toolkit/examples/connectors/example-connector . The connector application is created in images/example_connector/app . Liberty application The createEnvironment.sh script creates the i2 Analyze liberty application that is built into the Liberty base image. The application consists of files that do not change based on the configuration. After the application is created, you do not need to modify it. The application is created in images/liberty_ubi_base/application ."
  },
  "content/tools and functions/deploy.html": {
    "href": "content/tools and functions/deploy.html",
    "title": "Deploy and start i2 Analyze",
    "keywords": "Deploy and start i2 Analyze This topic describes how to deploy and start i2 Analyze in a containerized environment. For an example of the activities described, see the deploy.sh Running Solr and ZooKeeper The running Solr and ZooKeeper section runs the required containers and creates the Solr cluster and ZooKeeper ensemble. Create Solr cluster The createSecureCluster function creates the secure Solr cluster for the deployment. The function includes a number of calls that complete the following actions: The runZK server function runs the ZooKeeper containers that make up the ZooKeeper ensemble. For more information about running a ZooKeeper container, see ZooKeeper . In deploy.sh , 3 ZooKeeper containers are used. The runSolrClientCommand client function is used a number of times to complete the following actions: Create the znode for the cluster. i2 Analyze uses a ZooKeeper connection string with a chroot. To use a chroot connection string, a znode with that name must exist. For more information, see SolrCloud Mode . Set the urlScheme to be https . Configure the Solr authentication by uploading the security.json file to ZooKeeper. For more information about the function, see runSolrClientCommand . The runSolr server function runs the Solr containers for the Solr cluster. For more information about running a Solr container, see Solr . In deploy.sh , 2 Solr containers are used. At this point, your ZooKeepers are running in an ensemble, and your Solr containers are running in SolrCloud Mode managed by ZooKeeper. Initializing the Information Store database The initializing the Information Store database section creates a persistent database backup volume, runs the database container and configures the database management system. The database backup volume is created first with the Docker command: docker volume create \"${SQL_SERVER_BACKUP_VOLUME_NAME}\" so that the volume will not be automatically deleted when the SQL Server container is remove. This helps maintain any backups created while running a SQL server container. For more information about docker storage, see Docker Storage . Running the database server container The runSQLServer server function creates the secure SQL Server container for the deployment. For more information about building the SQL Server image and running a container, see Microsoft SQL Server . Before continuing, deploy.sh uses the waitForSolrToBeLive and waitForSQLServerToBeLive common function to ensure that Solr and SQL Server are running. Configuring SQL Server The configureSecureSqlServer function uses a number of client and server functions to complete the following actions: The changeSAPassword client function is used to change the sa user's password. For more information, see changeSAPassword Generate the static Information Store database scripts. The runi2AnalyzeTool client function is used to run the generateStaticInfoStoreCreationScripts.sh tool. runi2AnalyzeTool Generate static database scripts tool Create the Information Store database and schemas. The runSQLServerCommandAsSA client function is used to run the runDatabaseCreationScripts.sh tool. runSQLServerCommandAsSA runDatabaseCreationScripts.sh Create the database roles, logins, and users. For more information about the database users and their permissions, see Database users . The runSQLServerCommandAsSA client function runs the createDbRoles.sh script. The createDbLoginAndUser client function creates the logins and users. The runSQLServerCommandAsSA client function runs the applyBuiltInSQLServerRoles.sh script. Grant the dba user the required permissions in the msdb and master databases, this grants the correct permissions for the Deletion by Rule feature of i2Analyze. The runSQLServerCommandAsSA client function runs the configureDbaRolesAndPermissions.sh script. Make the etl user a member of the SQL server sysadmin group to allow this user to perform bulk inserts into the external staging tables. The runSQLServerCommandAsSA client function runs the addEtlUserToSysAdminRole.sh script. Run the static scripts that create the Information Store database objects. The runSQLServerCommandAsSA client function is used to run the runStaticScripts.sh tool. runSQLServerCommandAsSA Run static database scripts tool Configuring Solr and ZooKeeper The configuring Solr and ZooKeeper sections creates Solr configuration and then configures the Solr cluster and creates the Solr collections. The generateSolrSchemas.sh i2-tool creates the solr directory in examples/pre-prod/configuration/solr/generated_config . This directory contains the managed-schema, Solr synonyms file and Solr config files for each index. The configureSecureSolr function uses the runSolrClientCommand client function to upload the managed-schema , solr.xml , and synonyms file for each collection to ZooKeeper. For example: runSolrClientCommand solr zk upconfig -v -z \"${ZK_HOST}\" -n daod_index -d /conf/solr_config/daod_index The setClusterPolicyForSecureSolr function uses the runSolrClientCommand client function to set a cluster policy such that each host has 1 replica of each shard. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer -X POST -H Content-Type:text/xml -d '{ \\\"set-cluster-policy\\\": [ {\\\"replica\\\": \\\"<2\\\", \\\"shard\\\": \\\"#EACH\\\", \\\"host\\\": \\\"#EACH\\\"}]}' \\\"${SOLR1_BASE_URL}/api/cluster/autoscaling\\\"\" For more information about Solr policies, see Autoscaling Policy and Preferences . The createCollectionForSecureSolr function uses the runSolrClientCommand client function to create each Solr collection. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/admin/collections?action=CREATE&name=main_index&collection.configName=main_index&numShards=1&maxShardsPerNode=4&rule=replica:<2,host:*\\\"\" For more information about the Solr collection API call, see CREATE: Create a Collection . Configuring the Information Store database The configuring the Information Store database section creates objects within in the database. Generate the dynamic database scripts that create the schema specific database objects. The runi2AnalyzeTool client function is used to run the generateDynamicInfoStoreCreationScripts.sh tool. runi2AnalyzeTool Generate dynamic Information Store creation scripts tool Run the generated dynamic database scripts. The runSQLServerCommandAsSA client function is used to run the runDynamicScripts.sh tool. runSQLServerCommandAsSA Run dynamic Information Store creation scripts tool Configuring the Example Connector The configuring example connector section runs the example connector used by the i2 Analyze application. The runExampleConnector server function runs the example connector application. The waitForConnectorToBeLive client function checks the connector is live before allowing the script to proceed. Configuring i2 Analyze The configuring i2 Analyze section runs the Liberty containers that run the i2 Analyze application. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . The runLiberty server function runs a Liberty container from the configured image. For more information, see Running a Liberty container In deploy.sh , 2 liberty containers are used. Starting the load balancer. The runLoadBalancer functions in serverFunctions.sh runs HAProxy as a load balancer in a Docker container. The load balancer configuration we use can be found in haproxy.cfg file and the variables are passed as environment variables to the Docker container. The load balancer routes requests to the application to both Liberty servers that are running. The configuration that used is a simplified configuration for example purposes and is not to be used in production. For more information about configuring a load balancer with i2 Analyze, see Load balancer . Before continuing, deploy.sh uses the waitFori2AnalyzeServiceToBeLive common function to ensure that Liberty is running. Deploy the system match rules. The runSolrClientCommand client function is used to run the runIndexCommand.sh tool. The tool is run twice, once to update the match rules file and once to switch the match indexes. For more information, see Manage Solr indexes tool . Running Prometheus and Grafana The runPrometheus server function creates the Prometheus container and the runGrafana server function creates the Grafana container. For more information about running a Prometheus container, see Prometheus . For more information about running a Grafana container, see Grafana . Before continuing, deploy.sh uses the waitForPrometheusServerToBeLive and waitForGrafanaServerToBeLive common functions to ensure that Prometheus and Grafana are running."
  },
  "content/tools and functions/etl_tools.html": {
    "href": "content/tools and functions/etl_tools.html",
    "title": "ETL Tools",
    "keywords": "ETL Tools This topic describes how to perform ETL tasks by using the ETL toolkit in a containerized deployment of i2 Analyze. All of the tools described here are located in the images/etl_client directory. The etl_client directory is populated when running createEnvironment.sh . The runEtlToolkitToolAsi2ETL client function is used to run the ETL tools described in this topic as the i2ETL user. For more information about this client function, see runEtlToolkitToolAsi2ETL Building an ETL Client image The ETL client image is built from the Dockerfile in images/etl_client . The following docker run command builds the configured image: docker build -t \"etl_client:4.4.0\" \"images/etl_client\" Add Information Store ingestion source The addInformationStoreIngestionSource tool defines an ingestion source in the Information Store. For more information about ingestion sources in the Information Store, see Defining an ingestion source . You must provide the following arguments to the tool: Argument Description Maximum characters n A unique name for the ingestion source 30 d A description of the ingestion source that might appear in the user interface 100 Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/addInformationStoreIngestionSource -n <> -d <> \" Create Information Store staging table The createInformationStoreStagingTable tool creates the staging tables that you can use to ingest data into the Information Store. For more information about creating the tables, see Creating the staging tables . You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the staging table for sn The name of the database schema to create the staging table in tn The name of the staging table to create Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/createInformationStoreStagingTable -stid <> -sn <> -tn <> \" Ingest Information Store records The ingestInformationStoreRecords is used to ingest data into the Information Store. For more information about ingesting data into the Information Store, see The ingestInformationStoreRecords toolkit task You can use the following arguments with the tool: Argument Description imf The full path to the ingestion mapping file. imid The ingestion mapping identifier in the ingestion mapping file of the mapping to use im Optional: The import mode to use. Possible values are STANDARD, VALIDATE, BULK, DELETE, BULK_DELETE or DELETE_PREVIEW. The default is STANDARD. icf Optional: The full path to an ingestion settings file il Optional: A label for the ingestion that you can use to refer to it later lcl Optional: Whether (true/false) to log the links that were deleted/affected as a result of deleting entities Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/ingestInformationStoreRecords -imf <> -imid <> -im <>\" Sync Information Store records The syncInformationStoreCorrelation tool is used after an error during correlation, to synchronize the data in the Information Store with the data in the Solr index so that the data returns to a usable state Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/syncInformationStoreCorrelation\" Duplicate provenance check The duplicateProvenanceCheck tool can be used for identifying records in the Information Store with duplicate origin identifiers. Any provenance that has a duplicated origin identifier is added to a staging table in the Information Store. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTool bash -c \"/opt/i2/etltoolkit/syncInformationStoreCorrelation\" Duplicate provenance delete The duplicateProvenanceDelete tool deletes (entity/link) provenance from the Information Store that has duplicated origin identifiers. The provenance to delete is identified in the staging tables created by the duplicateProvenanceCheck tool. You can provide the following argument to the tool: Argument Description stn The name of the staging table that contains the origin identifiers to delete. If no arguments are provided, duplicate origin identifiers are deleted from all staging tables. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/syncInformationStoreCorrelation\" Generate Information Store index creation scripts The generateInformationStoreIndexCreationScript tool generates the scripts that create the indexes for each item type in the Information Store. For more information, see database index management You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the index creation scripts for. op The location to create the scripts. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTask bash -c \"/opt/i2/etltoolkit/generateInformationStoreIndexCreationScript -op <> -stid <> \" Generate Information Store index drop scripts The generateInformationStoreIndexDropScript tool generates the scripts that drop the indexes for each item type in the Information Store. For more information, see database index management You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the index drop scripts for. op The location to create the scripts. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTask bash -c \"/opt/i2/etltoolkit/generateInformationStoreIndexDropScript --op <> -stid <> \" Delete orphaned database objects The deleteOrphanedDatabaseObjects tool deletes (entity/link) database objects that are not associated with an i2 Analyze record from the Information Store. You can provide the following arguments to the tool: Argument Description iti Optional: The schema type identifier of the item type to delete orphaned database objects for. If no item type id is provided, orphaned objects for all item types are removed Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/deleteOrphanedDatabaseObjects -iti <> \" Disable merged property values The disableMergedPropertyValues tool removes the database views used to define the property values of merged i2 Analyze records. You can provide the following arguments to the tool: Argument Description etd The location of the root of the etl toolkit. stid The schema type identifier to disable the views for. If no schema type identifier is provided, the views for all of the item types are be removed Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/disableMergedPropertyValues -etd <> -stid <>\" For more information about correlation, see Information Store data correlation Enable merge property values The enableMergedPropertyValues tool creates the database views used to define the property values of merged i2 Analyze records. You can provide the following arguments to the tool: Argument Description etd The location of the root of the etl toolkit. stid The schema type identifier to create the views for. If no schema type identifier is provided, the views for all of the item types are generated. If the views already exist, they are overwritten. Use the runEtlToolkitToolAsDBA client function to run the tool as the database administrator. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/i2/etltoolkit/enableMergedPropertyValues -etd <> -stid <> \" For more information about correlation, see Information Store data correlation"
  },
  "content/tools and functions/i2analyze_tools.html": {
    "href": "content/tools and functions/i2analyze_tools.html",
    "title": "i2 Analyze tools",
    "keywords": "i2 Analyze tools The i2 Analyze deployment toolkit includes a number of tools that enable you to deploy and administer i2 Analyze. Note the TOOLKIT_DIR environment variable is set by the i2Tools image and does not need to be passed to the container at runtime. Generate Information Store scripts The generateInfoStoreToolScripts.sh script is used to generate Information Store scripts from templates in i2-tools/scripts/database-template directory located inside the i2Tools image. The generated scripts end up in in the GENERATED_DIR . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. GENERATED_DIR The root location where any generated scripts are created. DB_TRUSTSTORE_LOCATION The location of the truststore.jks . Schema update tool The generateUpdateSchemaScripts.sh script is used to update the i2 Analyze schema in the Information Store. To update the schema, the tool generates a number of SQL scripts that must be run against the Information Store database. When you run the tool, it compares the schema file in the configuration directory against the schema that is stored in the IS_META.I2_SCHEMAS table in Information Store database. If they are different, the schema in the database is updated with the one from the configuration. Then, the IS_DATA table structure is compared with the schema IS_META.I2_SCHEMAS . If the IS_DATA objects are different, the scripts to update the objects are generated. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. /update is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration ./database/sqlserver/InfoStore/generated/update You can run the generated scripts against the database manually, or you can use the Run database scripts tool . Security schema update tool The updateSecuritySchema.sh script is used to update the security schema in a deployment of i2 Analyze. When you run the script, the security schema file in the configuration directory is compared against the security schema that is stored in the IS_META.I2_SCHEMAS table in Information Store database. If they are different, the security schema in the database is updated with the one from the configuration. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration Run database scripts tool You can run any generated scripts against the database manually, or you can use the runDatabaseScripts.sh script. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires the following environment variables to be set: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. Manage Solr indexes tool By using the indexControls.sh script, you can pause and resume indexing on any index, upload system match rules, and switch a standby index to be the live index. Note: Switching from a standby index to live is possible for the following index types: chart, match, and main. The script requires the following environment variables to be present: Environment Variable Description ZOO_USERNAME The name of the administrator user for performing administration tasks. ZOO_PASSWORD The password for the administrator user. ZK_HOST The connection string for each ZooKeeper server to connect to. ZOO_SSL_TRUST_STORE_LOCATION The location of the truststore.jks file to be used for SSL connections to ZooKeeper. ZOO_SSL_TRUST_STORE_PASSWORD The password for the truststore. ZOO_SSL_KEY_STORE_LOCATION The location of the keystore.jks file to be used for SSL connections to ZooKeeper. ZOO_SSL_KEY_STORE_PASSWORD The password for the keystore. CONFIG_DIR The location of your configuration. This is used to locate the system-match-rules.xml . TASK The index control task to perform. You can specify the following values for TASK : UPDATE_MATCH_RULES SWITCH_STANDBY_MATCH_INDEX_TO_LIVE CLEAR_STANDBY_MATCH_INDEX REBUILD_STANDBY_MAIN_INDEX SWITCH_STANDBY_MAIN_INDEX_TO_LIVE CLEAR_STANDBY_MAIN_INDEX REBUILD_STANDBY_CHART_STORE_INDEX SWITCH_STANDBY_CHART_STORE_INDEX_TO_LIVE CLEAR_STANDBY_CHART_STORE_INDEX PAUSE_INDEX_POPULATION RESUME_INDEX_POPULATION For more information about the index control tasks, see Index control toolkit tasks . Information Store database consistency tool The databaseConsistencyCheck.sh script checks that the database objects in the IS_DATA database schema are consistent with the information in IS_META . The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration If the database is a consistent state with the schema, the following message will be returned in the console The Database is in a consistent state. If the database is an inconsistent state with the schema, errors will be reported in the console where the tool was run. For example: The following changes to the Information Store database tables and/or views have been detected: MODIFIED ITEM TYPES Item type: Arrest ADDED PROPERTY TYPES Property type: Voided Item type: Arrest The following changes to the Information Store database tables and/or views have been detected: ADDED ITEM TYPES Item type: Vehicle > ERROR [DatabaseConsistencyCheckCLI] - Information Store database is inconsistent with the schema stored in the database Schema validation tool The validateSchemaAndSecuritySchema.sh script checks that the database objects in the IS_META database schema are consistent with the i2 Analyze schema and the security schema in the IS_META.I2_SCHEMAS . The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration If the schemas are valid no errors will be reported. However, if the schemas are the same as the version of the database this information will be returned to the console. If the schemas contain validation errors will be reported to the console where the tool was run. For example: VALIDATION ERROR: Schema contains duplicate type EntityType with ID 'ET5'. Generate static database scripts tool The generateStaticInfoStoreCreationScripts.sh script injects values from your configuration into the static database scripts. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. /static is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration ../scripts/database/{dialect}/InfoStore/generated/static The script creates the .sql files in the location of the GENERATED_DIR that was supplied to the container. Run static database scripts tool The runStaticScripts.sh script runs the static scripts that create the Information Store database. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the static scripts to be run. /static is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Generate dynamic Information Store creation scripts tool The generateDynamicInfoStoreCreationScripts.sh script creates the .sql scripts that are required to create the database objects that align to your schema in the configuration. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The location where any generated scripts are created. /dynamic is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the i2-tools/scripts directory: ../../toolkit ../configuration ../scripts/database/{database dialect}/InfoStore/generated/dynamic The script creates the .sql files in the location of the GENERATED_DIR that was supplied to the container. Run dynamic Information Store creation scripts tool The runDynamicScripts.sh script runs the dynamic scripts that create the Information Store database. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The runSQLServerCommandAsSA client function is used to run the runDynamicScripts.sh script that create the Information store and schemas. runSQLServerCommandAsSA The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the scripts to be run. /dynamic is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Run database creation scripts tool The runDatabaseCreationScripts.sh script runs the create-database-storage.sql & 0001-create-schemas.sql database creation scripts. These scripts create the Information Store database and schemas. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the scripts to be run. /dynamic is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Remove data from the Information Store tool The runClearInfoStoreData.sh script removes all the data from the Information Store database. The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The path to the root of the directory that contains the generated static scripts. /static is appended to the value of GENERATED_DIR . The GENERATED_DIR environment variable must be present and is not defaulted. For more information about clearing data from the Information Store, see Clearing Data Create static Liberty application tool The createLibertyStaticApp.sh creates the static application to be hosted in Liberty. The tool combines the contents of various folders to create the application, in a similar way to the deployLiberty toolkit task. To run the tool, you must provide a location to create the application files as an argument. Alternatively if you are running the tool from outside of the toolkit/i2-tools/scripts folder the following environment variable must be set: Environment Variable Description TOOLKIT_DIR The location of the toolkit. When you call the createLibertyStaticApp.sh script, you must pass the location where the Liberty application directory is created. Connection properties Some of the tools require database connection properties such as database username, database truststore location & password for example in order to communicate with the database so that the tool can run. These properties can be loaded from a file called Connection.properties that is located in your configuration in the following directory configuration/fragments/common/WEB-INF/classes/ . For example: DBName=ISTORE DBDialect=sqlserver DBServer=sqlserver.eia DBPort=1433 DBOsType=UNIX DBSecureConnection=true DBCreateDatabase=false db.installation.dir=/opt/mssql-tools db.database.location.dir=/opt/mssql In the docker environment, these properties can be alternatively loaded via environment variables passed into the container. The following environment variables are supported for supplying connection properties: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_DIALECT The dialect for the database. Can be sqlserver . DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_OS_TYPE The Operating System that the database is on. Can be UNIX , WIN , or AIX . DB_INSTALL_DIR Specifies the database CMD location. DB_LOCATION_DIR Specifies the location of the database. DB_TRUSTSTORE_LOCATION The location of the truststore.jks . * DB_TRUSTSTORE_PASSWORD The password for the truststore. * The security settings are only required if your database is configured to use a secure connection. Generate Solr Config The generateSolrSchemas.sh generates Solr Config by taking the template schema-template-definition.xml file from the configuration/solr and generating solr config per index. For each index, the tool will generate solrconfig.xml , managed_schema and synonyms-<LOCALE>.txt file. NOTE: to change SOLR locale uncomment the relevant section from the schema-template-definition.xml file before deploying. The script requires the following environment variables to be present: Environment Variable Description CONFIG_DIR The location of the root of the configuration directory."
  },
  "content/tools and functions/reset_repository.html": {
    "href": "content/tools and functions/reset_repository.html",
    "title": "Resetting the repository",
    "keywords": "Resetting the repository The local repository can be returned to its initial state by running the resetRepository.sh script. The following artifacts are removed by the resetRepository.sh : All Docker resources such as images, containers, volumes and networks. The simulated-secret-store directory. Resources that are copied to various image directories when the environment is created. Database scripts that are generated as part of the deployment process. The ETL toolkit that is located in images/etl_client/etltoolkit The example connector that is located in images/example_connector/app After you run the resetRepository.sh script, you must follow the getting started process from the Creating a containerized deployment section."
  },
  "content/tools and functions/tools.html": {
    "href": "content/tools and functions/tools.html",
    "title": "Tools and functions",
    "keywords": "Tools and functions The documentation in this section describes the tools and functions that are used to deploy i2 Analyze in a containerized environment."
  },
  "content/upgrade/upgrade.html": {
    "href": "content/upgrade/upgrade.html",
    "title": "Upgrading i2 Analyze Pre-prod Environment",
    "keywords": "Upgrading i2 Analyze Pre-prod Environment This section describes an example of the process to upgrade i2 Analyze in a containerized environment. Updating the configuration includes the following high-level steps: * Backing up Solr and the database * Rebuilding the Docker images for the new version of i2 Analyze * Create the upgrade change set * Remove the previous containers and volumes * Upgrading Solr, the database, the example connector, the Liberty application, Prometheus and Grafana * Restoring Solr and the database Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . The examples/pre-prod/walkthroughs/upgrade/upgradeWalkthrough.sh script is a worked example that demonstrates how to upgrade a deployment in a containerized environment. Creating the Solr backup In the upgradeWalkthrough.sh script, the Backing up Solr section demonstrates the commands that are used to back up the Solr collections and system match rules. For more information about backing up Solr, see Backing up Solr . Creating SQL Server backups In the upgradeWalkthrough.sh script, the Backing up SQL Server section demonstrates the commands that are used to back up the SQL Server Information Store database. For more information about backing up the database, see Backing up the Information Store database . Rebuilding the images In the upgradeWalkthrough.sh script, the Rebuilding images section demonstrates running the buildImages.sh script to build images with for the new version of i2 Analyze. Creating the change set In the upgradeWalkthrough.sh script, the Create change set section demonstrates running the createChangeSet.sh script that creates the upgrade change set. The change set is created in examples/pre-prod/change-sets and contains the database scripts that are required to upgrade the Information Store database. Removing the previous containers In the upgradeWalkthrough.sh script, the Removing the previous containers section demonstrates the commands to delete the previous containers and remove any Docker volumes. Upgrading Solr In the upgradeWalkthrough.sh script, the Upgrading Solr section demonstrates the commands that are used to run the new Solr and ZooKeeper containers. The 3 ZooKeeper containers and 2 Solr containers are started. The Solr collections are configured, including the new vq_index collection. Restoring the Solr collections In the upgradeWalkthrough.sh script, the Restore Solr section demonstrates the commands that are used to restore the Solr Collections. For more information about restoring Solr, see [Restoring Solr](../backup and restore/restore.md#RestoreSolr). Restoring the Information Store database In the upgradeWalkthrough.sh script, the Restoring the Information Store database section demonstrates the commands that are used to restore the database to a new SQL Server container. Upgrading the Information Store database In the upgradeWalkthrough.sh script, the Upgrading Information Store section demonstrates the commands that are used to run the generated database scripts that upgrade the Information Store database. Upgrading example connector In the upgradeWalkthrough.sh script, the Upgrading example connector section demonstrates the commands that are used to run the example connector. Upgrading Liberty In the upgradeWalkthrough.sh script, the Upgrading Liberty section demonstrates the commands that are used to build the new Liberty configured image. Two Liberty containers are started using the new configured image. Then, the load balancer container is started. Upgrading Prometheus In the upgradeWalkthrough.sh script, the Upgrading Prometheus section demonstrates the commands that are used to run Prometheus. Upgrading Grafana In the upgradeWalkthrough.sh script, the Upgrading Grafana section demonstrates the commands that are used to run Grafana."
  },
  "content/walkthroughs/add_connector.html": {
    "href": "content/walkthroughs/add_connector.html",
    "title": "Adding a new Connector",
    "keywords": "Adding a new Connector This section describes an example of the process of adding a new Connector. You can use the examples/pre-prod/walkthroughs/change-management/addExampleConnectorWalkthrough.sh script to add a new second example connector to the example deployment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Updating Connectors.json To make liberty aware of the new connector, you need to add a new connector definition to the connectors.json. See Add example Connector to connectors.json section of the walkthrough script. Running a new Connector container To add a connector in the Docker environment, you need to run a new Connector container. See the Add example Connector section of the walkthrough script. The runExampleConnector server function in the serverFunctions.sh is used to run a new Example connector container. Updating your configuration After the connectors.json has been updated and the example connector has been run, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImageForPreProd server function builds the configured Liberty image. For more information, see Building a configured Liberty image . The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "content/walkthroughs/add_solr_node.html": {
    "href": "content/walkthroughs/add_solr_node.html",
    "title": "Adding a new Solr node",
    "keywords": "Adding a new Solr node This section describes an example of the process of adding a new Solr Node to an existing Solr cluster and using the Solr Collections API to add a replica on the newly created Solr node. You can use the examples/pre-prod/walkthroughs/change-management/addSolrNodeWalkthrough.sh script to add a Solr node to the example deployment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Running a new Solr container To add a Solr node to an existing Solr collection in the Docker environment, you run a new Solr container. See the Running a new Solr container section of the walkthrough script. The runSolr server function in the addSolrNodeWalkthrough.sh is used to run a new Solr container with a node that is added to the existing cluster. For more information about running a Solr container, see Solr . The following environment variables are used to specify the hostname of the container and to add the node to the existing cluster: SOLR_HOST specifies the fully qualified domain name of the Solr container. ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated list. When the connection string connects to the existing ZooKeeper quorum, the new Solr node is automatically added to the Solr cluster. Adding a Solr replica To use the new Solr node, you must add a replica for a shard and create it on the new Solr node. See the Adding a Solr replica section of the walkthrough script. To add a replica, use the Solr Collections API. For more information about the API command, see ADDREPLICA: Add Replica The following curl command is an example that creates a replica for shard1 in the main_index collection on the new Solr node: curl -u \"${SOLR_ADMIN_DIGEST_USERNAME}\":\"${SOLR_ADMIN_DIGEST_PASSWORD}\" --cacert /CA/CA.cer \"${SOLR1_BASE_URL}/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard1&node=${SOLR3_FQDN}:8983_solr\" In the addSolrNodeWalkthrough.sh script, the runSolrClientCommand function contains an example of how to run the curl command in a containerized environment: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard1&node=${SOLR3_FQDN}:8983_solr\\\"\" For more information about command parsing, see Command parsing The example above uses ${SOLR3_FQDN} , but you can use the fully qualified domain name of any Solr node in the cluster."
  },
  "content/walkthroughs/clear_data.html": {
    "href": "content/walkthroughs/clear_data.html",
    "title": "Clearing data from a deployment",
    "keywords": "Clearing data from a deployment This section describes an example of the process to clear the data from your deployment in a Docker environment. Clearing the data from a deployment includes the following high-level steps: Removing the Liberty containers Clearing the search index Creating and running a delete query Removing the collection properties Clearing data from the Information Store Remove all of the data in the Information Store Running the Liberty containers The examples/pre-prod/walkthroughs/change-management/clearDataWalkthrough.sh script is a worked example that demonstrates how to clear the data from the Information Store in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you clear the data from a deployment, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Clearing the search index Creating and running a delete query To clear the search index, run a Solr delete query against your indexes via the Solr API. You can run the delete query by using a curl command. In Solr, data is stored as documents. You must remove every document from each collection in your deployment. The runSolrClientCommand client function is used to run the curl commands that remove the documents from each collection. For more information about the function, see runSolrClientCommand . The following curl command removes every document from a the main_index : curl -u \"${SOLR_ADMIN_DIGEST_USERNAME}:${SOLR_ADMIN_DIGEST_PASSWORD}\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\" -H Content-Type:\"text/xml\" --data-binary \"<delete><query>*:*</query></delete>\" See the Clearing the search index section of the walkthrough script. For more information about command parsing, see Command parsing Removing the collection properties The runSolrClientCommand client function is used to remove the file from ZooKeeper. For more information about the function, see runSolrClientCommand . The following zkcli call removes the collection properties for the main_index . zkcli.sh -zkhost \"${ZK_HOST}\" -cmd clear \"/collections/main_index/collectionprops.json\" See the Clearing the search index section of the walkthrough script. The collection properties must be removed for any main, match, or chart collections. The collectionprops.json file is recreated when the i2 Analyze application is started. Clearing data from the Information Store See the Clearing the Information Store database section of the walkthrough script. The runSQLServerCommandAsDBA client function is used to run the clearInfoStoreData.sh tool to remove the data from the Information Store. runSQLServerCommandAsDBA clearInfoStoreData Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "content/walkthroughs/correlated_data.html": {
    "href": "content/walkthroughs/correlated_data.html",
    "title": "Defining the property values of merged records",
    "keywords": "Defining the property values of merged records This section describes an example of the process to define how property values of merged records are calculated in a Docker environment. The process includes the following high-level steps: Enabling merged property values To define the view, enable and then modify it to meet your correlation requirements Updating property value definitions Ingest data that correlates and review its properties The examples/pre-prod/walkthroughs/change-management/mergedPropertyValuesWalkthrough.sh script is a worked example that demonstrates how to enable and modify the views in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Default merge property behavior To demonstrate the default behavior, use the examples/pre-prod/walkthroughs/change-management/ingestDataWalkthrough.sh script to ingest some correlated data. For more information about ingesting the data, see Ingesting data into the Information Store . During the ingestion walk-through, the default behavior is used to determine the property values of the correlated record. In the default behavior, the property values from the merge contributor with the most recent value for source_last_updated are used. For more information about the how the property values for merged records are calculated, see Define how property values of merged records are calculated After you ingest the data, in Analyst's Notebook Premium search for Julia Yochum and add the returned entity to the chart. Keep the chart open for the remainder of the walkthrough script. Enabling merged property values To inform i2 Analyze that you intend to define the property values of merged records, run the enableMergedPropertyValues.sh tool. You can take control of the property values for records of specific item types, or all item types in the i2 Analyze schema. Note that this operation must be performed by the database administrator. See the Enabling merged property values section of the walkthrough script. The runEtlToolkitToolAsDBA client function is used to run the enableMergedPropertyValues.sh tool. runEtlToolkitToolAsDBA enableMergedPropertyValues Defining the property values of merged i2 Analyze records In the mergedPropertyValuesWalkthrough.sh , the views are created for the Person item type. The enableMergedPropertyValues.sh tool is used in the Create the merged property views for the CORRELATED_SCHEMA_TYPE_IDS section. Updating property value definitions The walkthrough provides an example .sql script that drops the existing IS_Public.E_Person_MPVDV view and replaces it with another. The new view prioritizes property values from merge contributors that come from the ingestion source names EXAMPLE_1 over values from EXAMPLE_2 and any other sources. The createAlternativeMergedPropertyValuesView.sql script is in examples/pre-prod/walkthroughs/configurationChanges . After the views are enabled, the merged property values definition view ( Person_MPVDV ) is modified to change how the property values of correlated records are calculated. Note, this step is also performed by the database administrator. See the Updating property value definitions section of the walkthrough script. The runSQLServerCommandAsDBA client functions in used to run the createAlternativeMergedPropertyValuesView.sql script. runSQLServerCommandAsDBA The merged property values definition view Reingesting the data The property values of merged records do not update when the MPVDV views are modified. To update the values of existing records, you must reingest at least one of the merge contributors to the record. To do this, use the ingestDataWalkthrough.sh script to ingest some data the correlates. For more information about ingesting the data, see Ingesting data into the Information Store . See the Reingesting the data section of the walkthrough script. After the data is ingested, in the Analyst's Notebook Premium chart that you have open, select the Julia Yochum item and click Get changes . The name of the item changes to Julie Yocham , because the property values that make up the name are now from the merge contributor where the ingestion source name is EXAMPLE_1 ."
  },
  "content/walkthroughs/ingestion.html": {
    "href": "content/walkthroughs/ingestion.html",
    "title": "Ingesting data into the Information Store",
    "keywords": "Ingesting data into the Information Store The examples/pre-prod/walkthroughs/change-management/ingestDataWalkthrough.sh script is a worked example that demonstrates how to script the process of ingesting data into the Information Store. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . The example provides a scripted mechanism to ingest data by using ETL toolkit. The script ingests data with and without correlation identifiers using both STANDARD and BULK import modes. For more information about ingestion, see: Information Store data ingestion Information Store data correlation To script the ingestion process, the following information is stored within the script: The mapping of item type identifiers to staging tables The mapping of staging tables to CSV files and format files. A list of import mapping identifiers from the mapping file. The script uses the runSqlServerCommandAsETL client function to import the data into the staging tables, and runEtlToolkitToolAsi2ETL to run the ingestion commands. Note the external ETL user is performing the import into the staging tables, but the i2 Internal ETL user is executing ETL tasks such as creating the ingestion source or performing the import from the staging tables. The data and ingestion artifacts that are used in the example are in the examples/data directory of the minimal toolkit. Creating the ingestion sources The runEtlToolkitToolAsi2ETL client function is used to run addInformationStoreIngestionSource ETL toolkit tool to create the ingestion sources. runEtlToolkitToolAsi2ETL addInformationStoreIngestionSource For more information about ingestion sources in the Information Store, see Defining an ingestion source For an example of how to use the tool, see the Create the ingestion sources section in the ingestDataWalkthrough.sh Creating the staging tables The runEtlToolkitToolAsi2ETL client function is used to run createInformationStoreStagingTable ETL toolkit tool to create the staging tables. runEtlToolkitToolAsi2ETL createInformationStoreStagingTable For more information about creating staging tables in the Information Store, see Creating the staging tables To create all of the staging tables in the example, schema type identifiers are mapped to staging table names. For an example of how the mappings are used, see the Create the staging tables section in the ingestDataWalkthrough.sh Note: Because there are multiple staging tables for the LAC1 link type, a second loop is used to iterate through the staging table names of each schema type. Ingesting data The walkthrough demonstrates how to ingest both non-correlated and correlated data. For each type of data the staging tables and populated, the data ingestion task is executed, then the staging tables are cleaned. Ingesting non correlated data In this case, the SQL Server BULK INSERT command is used to insert the CSV data into the staging tables. Ingesting correlated data See the Ingesting correlated data section of the walkthrough script. The ingestion procedure The runSqlServerCommandAsETL client function is used to run the sql statement that inserts the data into the staging tables. runSqlServerCommandAsETL Populating the staging tables The example uses CSV and format files to insert the data. The files have the same name ( person.csv and person.fmt ). The staging table names are mapped to the CSV and format files. There is one mapping for base data ( BASE_DATA_TABLE_TO_CSV_AND_FORMAT_FILE_NAME ) and one for correlation data ( CORRELATED_DATA_TABLE_AND_FORMAT_FILE_NAME ). For an example of how the mappings are used, see the Insert the base data into the staging tables section in the ingestDataWalkthrough.sh The runEtlToolkitToolAsi2ETL client function is used to run ingestInformationStoreRecords ETL toolkit tool to ingest the data into the Information Store from the staging tables. runEtlToolkitToolAsi2ETL ingestInformationStoreRecords The ingestInformationStoreRecords toolkit task The ingestInformationStoreRecords tool is used with BULK and STANDARD import modes. Standard import mode is used to ingest the correlation data sets. Bulk import mode is used to ingest the base data set. The import mapping identifiers to use with the ingestInformationStoreRecords tool are defined in the IMPORT_MAPPING_IDS and BULK_IMPORT_MAPPING_IDS lists. A loop is used to ingest data for each mapping identifier in the lists."
  },
  "content/walkthroughs/reset_walkthroughs.html": {
    "href": "content/walkthroughs/reset_walkthroughs.html",
    "title": "Resetting your environment",
    "keywords": "Resetting your environment Resetting the deployment after you complete one of the walkthroughs is done in 2 steps: Resets the configuration directory to the base example configuration Runs the deploy script to redeploy the system with the base configuration Resetting the configuration The configuration directory is reset by calling the pre-prod/createPreProdEnvironment.sh script. This creates a clean configuration for your deployment. Redeploying the system The pre-prod/deploy.sh script is run to redeploy the system."
  },
  "content/walkthroughs/update_configuration.html": {
    "href": "content/walkthroughs/update_configuration.html",
    "title": "Updating the i2 Analyze application configuration",
    "keywords": "Updating the i2 Analyze application configuration This section describes an example of the process to update the configuration of a deployment in a Docker environment. After you update the configuration, rebuild the configured Liberty image. You must rebuild the image, because the configuration is part of the image. Updating the configuration includes the following high-level steps: Removing the Liberty containers Updating configuration Rebuilding the configured Liberty image with the modified configuration Running the Liberty containers When you start Liberty, the schema that it caches from the database is updated Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the application configuration, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Updating your configuration In the examples/pre-prod/walkthroughs/change-management/updateConfigurationWalkthrough.sh script, a modified geospatial-configuration.xml is being copied from the /walkthroughs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. For information about modifying the configuration, see Configuring the i2 Analyze application . See the Updating the configuration section of the walkthrough script. After you modify the configuration, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImageForPreProd server function builds the configured Liberty image. For more information, see Building a configured Liberty image . Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "content/walkthroughs/update_match_rules.html": {
    "href": "content/walkthroughs/update_match_rules.html",
    "title": "Updating the system match rules",
    "keywords": "Updating the system match rules This section describes an example of the process to update the system match rules of a deployment with the Information Store. Updating your match rules in your deployment includes the following high-level steps after you modify your system match rules file: Update the match rules file Upload the updated system match rules file to ZooKeeper Create a standby match index with the new system match rules, and wait for a response from the server to say that the standby match index is ready Switch your standby match index to live The examples/pre-prod/walkthroughs/change-management/updateMatchRulesWalkthrough.sh script is a worked example that demonstrates how to update the system match rules in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Updating your system match rules file In the updateMatchRulesWalkthrough.sh script, the modified system-match-rules.xml is copied from the examples/pre-prod/walkthroughs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. See the Updating system-match-rules.xml section of the walkthrough script. For more information about modifying the system match rules file, see Deploying system match rules . Uploading system match rules to ZooKeeper After you modify the system match rules file, use the update_match_rules function of the runIndexCommand.sh script to upload the match rules to ZooKeeper. For more information, see Manage Solr indexes tool . The runIndexCommand function with the update_match_rules argument in the updateMatchRulesWalkthrough.sh script contains the docker run command that uses an ephemeral Solr client container to upload the match rules. To use the tool in a Docker environment, the following prerequisites must be present: The Docker toolkit and your configuration must be available in the container. In the example, the toolkit and configuration are volume mounted in the docker run command. For example: -v \"pre-reqs/i2analyze/toolkit:/opt/toolkit\" \\ -v \"examples/pre-prod/configuration:/opt/\" Environment variables The tool requires a number of environment variables to be set. For the list of environment variables that you can set, see Manage Solr indexes tool . Java The container must be able to run Java executables. In the example, the container uses the adoptopenjdk image from DockerHub. adoptopenjdk/openjdk11:ubi-jre Checking the standby match index status When uploading match rules, the runIndexCommand.sh will wait for 5 minutes for a response from the server. If however, it takes longer to create the new match index a curl command can be run against liberty. The waitForIndexesToBeBuilt client function makes a request to the api/v1/admin/indexes/status endpoint and inspects the JSON response to see if the match index is built. Switching the standby match index to live After the standby index is ready, you can switch the standby index to live for resolving matches. Use the switch_standby_match_index_to_live function of the runIndexCommand.sh script to switch the indexes. The runIndexCommand function with the switch_standby_match_index_to_live argument in the updateMatchRulesWalkthrough.sh script contains the docker run command that uses an ephemeral Solr client container to switch the match indexes. The tool outputs a message when the action is completed successfully and exit code 0 is returned. For example: > INFO [IndexControlHelper] - The server processed the command successfully: > Switched the live Match index from 'match_index1' to 'match_index2'. If there are any errors, the error is displayed in the console."
  },
  "content/walkthroughs/update_schema.html": {
    "href": "content/walkthroughs/update_schema.html",
    "title": "Updating the schema",
    "keywords": "Updating the schema This section describes an example of the process to update the schema of a deployment in a Docker environment. To update the schema in a deployment, you need to update the Information Store database and restart the application server. Updating the schema includes the following high-level steps: Removing the Liberty containers Modifying the schema file Generating the database scripts to update the Information Store database Running the generated scripts against your Information Store database Running the Liberty containers When you start Liberty, the schema that it caches from the database is updated Validating database consistency The examples/pre-prod/walkthroughs/configuration-changes/updateSchemaWalkthrough.sh script is a worked example that demonstrates how to update the schema in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the schema in the Information Store, you remove the Liberty containers. To remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Modifying your schema In the updateSchemaWalkthrough.sh script, a modified schema.xml from the walkthroughs/configuration-changes directory is copied to the configuration/fragments/common/WEB-INF/classes/ directory. After you modify the configuration, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImageForPreProd server function builds the configured Liberty image. For more information, see Building a configured Liberty image . See the Modifying the schema section of the walkthrough script. To modify your schema, use i2 Analyze Schema Designer. For more information on installing Schema Designer please see: i2 Analyze Schema Designer . Validating your schema After you modify the schema, you can use the validateSchemaAndSecuritySchema.sh tool to validate it. If the schema is invalid, errors are reported. See the Validating the schema section of the walkthrough script. The runi2AnalyzeTool client function is used to run the validateSchemaAndSecuritySchema.sh tool. runi2AnalyzeTool validateSchemaAndSecuritySchema Generating the database scripts After you modify and validate the schema, generate the database scripts that are used to update the Information Store database to reflect the change. See the Generating update schema scripts section of the walkthrough script. The runi2AnalyzeTool client function is used to run the generateUpdateSchemaScripts.sh tool. runi2AnalyzeTool generateUpdateSchemaScripts Running the generated scripts After you generate the scripts, run them against the Information Store database to update the database objects to represent the changes to the schema. See the Running the generated scripts section of the walkthrough script. The runSQLServerCommandAsDBA client function is used to run the runDatabaseScripts.sh tool. runSQLServerCommandAsDBA runDatabaseScripts After the database scripts are run, the Information Store database is updated with any changes to the schema. Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running. Validating database consistency After the system has started, the dbConsistencyCheckScript.sh tool is used to check the state of the database after the Information Store tables are modified. See the Validating database consistency section of the walkthrough script. The runi2AnalyzeTool client function is used to run the dbConsistencyCheckScript.sh tool. runi2AnalyzeTool dbConsistencyCheckScript"
  },
  "content/walkthroughs/update_security_schema.html": {
    "href": "content/walkthroughs/update_security_schema.html",
    "title": "Updating the security schema",
    "keywords": "Updating the security schema This section describes an example of the process to update the security schema of a deployment in a Docker environment. To update the security schema in a deployment, you need to update the Information Store database and restart the application server. Updating the security schema includes the following high-level steps: Removing the Liberty containers Modifying the security schema file Starting Liberty containers When you start Liberty, the security schema that it caches from the database is updated Updating the Information Store Running the Liberty containers Validating database consistency The examples/pre-prod/walkthroughs/configuration-changes/updateSecuritySchemaWalkthrough.sh script is a worked example that demonstrates how to update the security schema in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the security schema in the Information Store, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Modifying your security schema In the examples/pre-prod/walkthroughs/configuration-changes/updateSecuritySchemaWalkthrough.sh script, the updateSecuritySchemaFile function copies a modified security-schema.xml from the walkthroughs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. For more information about the structure of the security schema, see The i2 Analyze security schema . The buildLibertyConfiguredImageForPreProd server function builds the configured Liberty image. For more information, see Building a configured Liberty image . See the Modifying the security schema section of the walkthrough script. Validating your schema After you modify the security schema, you can use the validateSchemaAndSecuritySchema.sh tool to validate it. If the security schema is invalid, errors are reported. See the Validating the security schema section of the walkthrough script. The runi2AnalyzeTool client function is used to run the validateSchemaAndSecuritySchema.sh tool. runi2AnalyzeTool validateSchemaAndSecuritySchema Updating the Information Store See the Updating the Information Store section of the walkthrough script. The runi2AnalyzeTool client function is used to run the updateSecuritySchema.sh tool. runi2AnalyzeTool updateSecuritySchema Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running. Validating database consistency After the system has started, the dbConsistencyCheckScript.sh tool is used to check the state of the database after the Information Store tables are modified. See the Validating database consistency section of the walkthrough script. The runi2AnalyzeTool client function is used to run the dbConsistencyCheckScript.sh tool. runi2AnalyzeTool dbConsistencyCheckScript"
  },
  "content/walkthroughs/walkthroughs.html": {
    "href": "content/walkthroughs/walkthroughs.html",
    "title": "Understanding the walkthroughs",
    "keywords": "Understanding the walkthroughs The walkthroughs are designed to demonstrate how you can configure and administer a deployment of i2 Analyze in a containerized environment. The walkthroughs consist of scripts that you can run and a document to provide further explanation. You can run the scripts against the reference architecture deployment. The walkthrough scripts use some of the server and client functions to reduce repetition. This is a pattern that can be copied and used in your own environment. The list of walkthroughs that are provided is: Adding a connector Adding a Solr node Updating the schema Updating the security schema Updating the i2 Analyze application configuration Updating the system match rules Ingesting data Ingesting correlated data Clearing data from a deployment"
  },
  "guidetoc/index.html": {
    "href": "guidetoc/index.html",
    "title": "Versions",
    "keywords": "Versions The release version of the analyze-containers repository varies independently from the i2 Analyze version. Each release works with a specific version of i2 Analyze, please refer to the compatibility table . Compatibility Each new version of i2 Analyze Containers adds features and functionality. The following table shows the relationship between a version of the analyze-containers repository and the version of i2 Analyze in which its new features are available: i2 Analyze Containers version i2 Analyze version 2.1.3 4.3.4 2.2.0 4.3.5 2.3.0 4.4.0 Support policy i2's policy for supporting releases of the analyze-containers repository is as follows: Major version change: Breaking change to the analyze-containers code i2 Analyze major version change Minor version change: i2 Analyze minor/patch/revision change. Patch version change: analyze-containers new functionality On releasing a new major version, we support the previous major version for a minimum of 2 years. We might deprecate versions given security issues or unsupported features (such as a vulnerable dependency) on a case-by-case basis."
  },
  "index.html": {
    "href": "index.html",
    "title": "i2 Analyze containerized deployment",
    "keywords": "i2 Analyze containerized deployment The analyze-containers repository provides a containerized configuration development environment that you can use to develop i2 Analyze configurations, and a reference architecture to demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. The repository is designed to be used with the i2 Analyze minimal toolkit. The minimal toolkit is similar to the standard i2 Analyze deployment toolkit, except that it only includes the minimum amount of application and configuration files. If you are already using the analyze-containers environment and want to upgrade to the latest versions, follow the instructions in Upgrading . Configuration development environment The process to deploy i2 Analyze in production is iterative, and includes a number of phases and environments. The containerized configuration development is designed so that it is easy to move between these stages. Because the environment is containerized, the provided scripts manage the creation of any containers that are required by the environment. For example, the database management system container is added to your environment when you specify a deployment pattern that includes it. You do not need to manually create it first. The following features of containerized configuration development environment are designed to enable you to develop i2 Analyze configurations: Simplified configuration directory The configuration directory structure is flattened to make it easier to locate files The directory includes placeholder configuration files so that you can add to existing files The XSD files are included to provide intellisense in VS Code Simplified deployment You can switch between i2 Analyze deployment patterns by changing a single setting. Use a single deploy command to deploy and update a deployment regardless of the deployment pattern used or any changes to the configuration Manage multiple configurations Develop and store multiple configurations Switch between configurations by using the deploy command Backup the database associated with a configuration Add connectors to the environment The environment can build images and run containers for your connectors To start developing configurations, see Creating your configuration development environment . Containerized deployment reference architecture The repository includes Dockerfiles and example scripts that provide a reference architecture for creating a containerized deployment of i2 Analyze. The scripts demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. You can run the scripts to create a local example containerized deployment. For more information about the reference architecture, see Understanding the reference architecture ."
  },
  "versions/2.1.3/content/add_data_config_dev.html": {
    "href": "versions/2.1.3/content/add_data_config_dev.html",
    "title": "Add data to your config development environment",
    "keywords": "Add data to your config development environment Developing the configuration is separated into two parts: enabling i2 Analyze to work with your data, and defining how analysts interact with your data. First, update the DEPLOYMENT_PATTERN variable in your <config_name>/utils/variables.sh file to your intended deployment pattern. For example, to deploy with the i2 Connect gateway and the Information Store set the following value: DEPLOYMENT_PATTERN=\"i2c_istore\" Depending on the composition of your deployment, there are three methods that you can use to get your data into i2 Analyze for analysis. To develop the process for ingesting data into the Information Store, refer to Ingesting data into the Information Store . To develop connections to external data sources, refer to Connecting to external data sources . Ensure that analysts can import and create representative records in Analyst's Notebook Premium. Then, if required, upload records to the Information Store. For more information, see Import data and Create i2 Analyze chart items . When you develop the process to get your data into i2 Analyze, you might realize that your schema or security schema are not correct for your data. You can update the deployed schemas to better represent your data and security model. If you need to make destructive changes, change the DEPLOYMENT_PATTERN variable in your <config_name>/utils/variables.sh file to schema_dev and redeploy. Then complete the process of developing schemas . After you add data to your environment, you can configure the rest of the configuration ."
  },
  "versions/2.1.3/content/backup and restore/backup.html": {
    "href": "versions/2.1.3/content/backup and restore/backup.html",
    "title": "Backup",
    "keywords": "Backup This section describes the process to back up the Solr collections and the Information Store database in SQL Server in an i2 Analyze deployment in a containerised environment. Understanding the back up process In a deployment of i2 Analyze, data is stored in the Information Store database and indexed in Solr collections. You must back up these components as a pair to enable you to restore the data in your system after a failure. When you back up the Solr collections, the configuration in ZooKeeper is also backed up. To ensure that data can be restored correctly, you must back up the components in the following order: Solr collections Information Store database If data is changed in the database after taking the Solr backup, Solr can update the index to reflect these changes when the collections and database are restored. When you create your backups, ensure that you store both backups so that you can identify the pair of backups that you must restore if required. Note: In the walkthrough, both the database and Solr backups are versioned using the variable backup_version . In the walkthrough the backup version is 1 . This is used to create a directory where the all of the backup files are stored for this pair. To create another backup pair, increment the backup_version so that the backup files are stored in a different directory. Backing up the Solr collections To back up Solr collections, you must have a shared filesystem available that is shared and accessible by all Solr nodes. In a containerised environment, a backup volume is shared between all Solr containers. Note: In the example, the backup volume is mounted to /backup in the Solr container. To ensure solr can write the backup file, you must make sure the solr process has permission to the backup folder. In order for solr to write to the folder the user solr and group 0 must have read and write permission to the backup folder. The following chown command is an example that gives the solr process permission to write to the backup location: chown -R solr:0 /backup The runSolrClientCommand client function is used to run the BACKUP API request. The backup operation must be performed for each non-transient collection. The non-transient collections are the main_index , match_index , and chart_index . The following curl command is an example that creates a backup of the main_index collection: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert /tmp/i2acerts/CA.cer https://solr1:8983//solr/admin/collections?action=BACKUP&async=main_index_backup&name=main_index_backup&collection=main_index&location=/backup/1/ \" To perform a backup operation, use the Solr Collections API. For more information about the backup API command, see BACKUP: Backup Collection Note: The BACKUP API request must be an asynchronous call otherwise the backup procedure will timeout. This is done by adding async flag with a corresponding id to the curl command. In the above example, this is &async=main_index_backup . See the Backing up Solr section of the walkthrough script. Determining status of Solr backup The Monitoring Solr backup process section of the walkthrough script runs a loop around the getAsycRequestStatus client function that reports the status of the Asynchronous backup request. For more information about the client function, see getAsycRequestStatus . See the Monitoring Solr backup progress section of the walkthrough script. Backing up the system match rules file The system match rules are stored in the ZooKeeper configuration, and can be backed up by using the Solr ZooKeeper Command Line Interface (zkcli) from the Solr client container. The runSolrClientCommand client function is used to run the zkcli command. The following command is an example of how to use the getfile command to retrieve the system match rules: runSolrClientCommand \"/opt/solr-8.8.2/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd getfile /configs/match_index1/match_index1/app/match-rules.xml /backup/1/system-match-rules.xml Note: The system match rules file is backed up to the /backup/1/ folder where the Solr backup is located. For more information about the ZooKeeper command line utilities, see Using Solrâ€™s ZooKeeper CLI . Backing up the Information Store database In the containerised environment, the mounted backup volume is used to store the backup file. Only the Information Store database is backed up. The SQL Server instance and system databases are not included in the backup. For more information about the backup volume, see Running a SQL Server . The backup is performed by a user that has the built-in db_backupoperator SQL Server role. In this case, that is the dbb user. Use the runSQLServerCommandAsDBB client function to run the following SQL command to create the backup file. For example: runSQLServerCommandAsDBB bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"USE ISTORE; BACKUP DATABASE ISTORE TO DISK = '/backup/1/IStore.bak' WITH FORMAT;\\\"\" For more information about backing up a SQL Server database, see: Backup SQL Documentation SQL Server Backup Permissions For more information about the dbb user and it's role, see: db users See the Backing up SQL Server section of the walkthrough script."
  },
  "versions/2.1.3/content/backup and restore/br.html": {
    "href": "versions/2.1.3/content/backup and restore/br.html",
    "title": "Backup and restore",
    "keywords": "Backup and restore The documentation in this section describes how to back up and restore a deployment of i2 Analyze in a containerised environment."
  },
  "versions/2.1.3/content/backup and restore/restore.html": {
    "href": "versions/2.1.3/content/backup and restore/restore.html",
    "title": "Restore",
    "keywords": "Restore This section describes the process to restore the Solr collections and Information Store database in SQL Server of an i2 Analyze deployment in a containerised environment. Understanding the restore process To restore the data and indexes for a deployment of i2 Analyze, restore the pair of Solr collection and Information Store database backups. To ensure that the data is restored correctly, restore the pair of backups in the following order: Information Store database Solr collections If any data has changed in the Information Store after the Solr backup, the Solr index is updated to reflect the contents of the Information Store database when Liberty is started. Preparing the environment Before you can restore the backups, clean down the Solr, ZooKeeper & SQL Server environment if you are not using a clean environment. See the Simulating Clean down section of the walkthrough script. Stop the liberty servers Before the restore process can begin you must stop the liberty servers. In a this can be done by stopping the containers The following command is an example of how to stop the liberty containers: docker container stop liberty1 liberty2 Restoring the Information Store database The process of restoring the Information Store database contains the following steps: Running a SQL Server container with a new instance of SQL Server Creating the Information Store database from the backup file in the new instance Recreating the required logins in the new SQL Server instance, and the users in the restored database Running a SQL Server container Run a container for the new instance of SQL Server, this will have the backup directory available. For more information about running a SQL Server container, see SQL Server . See the Running the SQL Server section of the walkthrough script. Creating the database from the backup file In a new instance of SQL Server, the only user is the sa user. Use the runSQLServerCommandAsSA client function to run the following SQL command as the sa user to create the ISTORE database from the backup file: runSQLServerCommandAsSA bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"RESTORE DATABASE ISTORE FROM DISK = '/backup/IStore.bak;'\" See the Restoring the ISTORE database section of the walkthrough script. Recreating the logins and users In SQL Server, a login is scoped to the Database Engine. To connect to a specific database, in this case the ISTORE database, a login must be mapped to a database user. Because the backup is completed for the ISTORE database only, the logins from the previous SQL Server instance cannot be restored. Additionally, the database users are restored but there are no logins mapped to them. For more information about SQL Server logins, see SQL Server Login documentation In this environment, create the required logins and users by dropping the users from the database and recreating the logins and the users by using the createDbLoginAndUser client function. The logins and users are created in the same way as when original SQL Server instance was configured. For more information about creating the required logins, users, and permissions, see Configuring SQL Server in the deployment documentation. See the Dropping existing database users and Recreating database logins, users, and permissions sections of the walkthrough script. Restoring the Solr collections Restoring the solr indexes includes the following high-level steps: Deploy a new Solr cluster and ZooKeeper ensemble Restore the non-transient Solr collections Monitor the restore process until completed Recreate transient Solr collections Restore system match rules Deploy a new Solr cluster and ZooKeeper ensemble To restore the Solr indexes, deploy a new Solr cluster & ZooKeeper ensemble. For more information about running a clean ZooKeeper & Solr environment, see Running Solr and ZooKeeper . Create Solr cluster . See the Deploying Clean Solr & Zookeeper section of the walkthrough script. Restoring the non-transient Solr collections The runSolrClientCommand client function is used to run the RESTORE API request. The restore operation must be performed for each non-transient collection that was backed up. The following curl command is an example that restores the main_index collection: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert /tmp/i2acerts/CA.cer https://solr1:8983/solr/admin/collections?action=RESTORE&async=main_index_backup&name=main_index_backup&collection=main_index&location=/backup/1\" \" To perform a restore operation, use the Solr Collections API. For more information about the Restore API command, see RESTORE: Restore Collection . Note: The restore API request must be an asynchronous call otherwise the restore procedure will timeout. This is done by adding async flag with a corresponding id to the curl command. In the above example, this is &async=main_index_backup . See the Restoring non-transient Solr collection section of the walkthrough script. Determining completion of Solr restore procedure The Monitoring Solr restore process section of the walkthrough script runs a loop around the getAsycRequestStatus client function that reports the status of the Asynchronous request. For more information about the client function, see getAsycRequestStatus . See the Monitoring Solr restore progress section of the walkthrough script. Recreate transient Solr collections Recreate the transient daod_index and highlightquery_index Solr collections. For more information about the creating Solr collections, see Configuring Solr and ZooKeeper . See the Recreating transient Solr collections section of the walkthrough script. Restore system match rules After the indexes are restored, upload the system match rules file. Use the Solr ZooKeeper Command Line Interface (zkcli) to create the directory in ZooKeeper for the system match rules file and to upload it. The runSolrClientCommand client function is used to run the zkcli request. The following command creates the directory in ZooKeeper where the system match rules file must be stored: runSolrClientCommand \"/opt/solr-8.8.2/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd makepath /configs/match_index1/match_index1/app Note: The path to where the system match rules file must be located is in the following format configs/<index name>/<index name>/app The following command uploads the system match rules file to the directory in ZooKeeper created earlier: runSolrClientCommand \"/opt/solr-8.8.2/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd putfile /configs/match_index1/match_index1/app/match-rules.xml /backup/1/system-match-rules.xml For more information about the ZooKeeper command line utilities, see Using Solrâ€™s ZooKeeper CLI . See the Restoring system match rules section of the walkthrough script. Start the Liberty containers After the Solr collections and Information Store database are restored, start the Liberty containers. The following command is an example of how to start the Liberty containers: docker container start liberty1 liberty2 After the Liberty containers have started, the waitFori2AnalyzeServiceToBeLive common function ensures that the i2 Analyze service is running. See the Restart Liberty containers section of the walkthrough script."
  },
  "versions/2.1.3/content/connector_config_dev.html": {
    "href": "versions/2.1.3/content/connector_config_dev.html",
    "title": "Adding connectors to your development environment",
    "keywords": "Adding connectors to your development environment To add a connector to your deployment of i2 Analyze, you must develop an i2 Connect connector. For more information about developing connectors, see i2 Connect gateway connector development . Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to a pattern that includes the i2 Connect gateway. For example: DEPLOYMENT_PATTERN=\"i2c_istore\" Process overview: Create a custom connector image Update the configuration to reference connectors Build connector images and redeploy Creating a connector image Create a connector image from one of the image templates. You can deploy connectors using the following templates: Spring Boot for Java based connectors i2 Connect server for Node based connectors developed using the i2 Connect server SDK NodeJS for Node based connectors External for connectors hosted outside of the config development environment The connector should be secured and run on port 3443 . For more information about securing your system, see Securing i2 Analyze . Spring Boot based connector image Copy the /templates/springboot-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the springboot-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - target - connector-definition.json - connector-version.json - ... Copy your connector code into the target directory of the connector directory. For example, copy your .jar file into the <connector_name>/target directory. Next, configure your connector . i2 Connect server based connector image Copy the /templates/i2connect-server-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the i2connect-server-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - app - connector-definition.json - connector-version.json - ... Copy the i2 Connect server connector distributable ( the .tar.gz or .tgz file) into the connector directory. Next, configure your connector . NodeJS based connector image Copy the /templates/node-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the node-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - app - connector-definition.json - connector-version.json - ... Copy your connector code into the app directory of the connector directory. For example, copy the files into the <connector_name>/app directory. By default, the image will start node with the file app.js . Next, configure your connector . External connector Copy the /templates/external-connector directory to the /connector-images directory. Rename the external-connector folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - connector-definition.json Next, configure your connector . Configuring a connector Populate the values in the <connector_name>/connector-definition.json file with information about your connector. Key Description id An identifier that is unique for all connectors that will be deployed. name A name for the connector that is displayed to users in the client. configurationPath The full path to the configuration endpoint of the connector. By default, it is /config . gatewaySchema The short name of an optional gateway schema. When no gateway schema is used, do not provide a value. type Used to identify the type of connector. For i2 Connect server connectors, set to i2connect-server . For external connectors, set to external . For any other connectors, the type key is not required. baseUrl Used only for connectors of type external . The baseUrl value is the URL address of the connector. For any other connectors, the baseUrl key is not required. For example: { \"id\": \"connector1\", \"name\": \"Connector 1\", \"description\": \"First connector\", \"configurationPath\": \"/config\", \"gatewaySchema\": \"template-schema\", \"type\": \"i2connect-server\" } or { \"id\": \"connector2\", \"name\": \"Connector 2\", \"description\": \"Second connector\", \"configurationPath\": \"/config\", \"gatewaySchema\": \"template-schema\", \"type\": \"external\", \"baseUrl\": \"https://example-connector:3443\" } Note: If your external connector is hosted in a Docker container on a different network to your config development environment, connect the Liberty container to the same network as your external connector container. For example, run docker network connect <connector-network-name> liberty1.<config-name> . If your connector is running on your host machine, use the IP address of your host machine in the baseUrl value. Provide the version of the connector in the <connector_name>/connector-version.json file. For external connectors, this file is not required. The version value specifies the version of the connector. For example: { \"version\": \"0.0.1\", \"tag\": \"0-0-1\" } For more information about configuring connectors, see Managing connectors . Adding connectors to a config In the configuration you want to deploy your connector with, update the /configs/<config_name>/configuration/connector-references.json file and add the directory name for your connector in the connectors array. For example, to add the connector in connector-images\\example-connector the connector-references.json file contains the following connectors array: { \"connectors\": [ { \"name\": \"example-connector\" } ], ... } Defining a gateway schema (Optional) You can develop the gateway schema and associated charting schemes by changing the deployment pattern to schema-dev and following the instructions in Developing schemas in your configuration development environment . After you develop the schemes, complete the following steps to add deploy them with your configuration: Copy the schema and charting scheme files to the /gateway-schemas directory. To define the short name of the gateway schema, the prefix of the file name is used. The convention is defined as: <short>-<name>-schema.xml and <short>-<name>-schema-charting-scheme.xml . For example, files with the names law-enforcement-schema.xml and law-enforcement-charting-schemes.xml have the short name law-enforcement . Add the short name of your schema to the /configs/<config_name>/configuration/connector-references.json file in the gatewaySchemas array. For example, to add a gateway schema with the short name law-enforcement , the file contains the following gatewaySchemas array: { ... ], \"gatewaySchemas\": [ { \"shortName\": \"law-enforcement\" } ] } Set the short name of your schema in the <connector_name>/connector-definition.json as the value for the gatewaySchema key of the connector that will use the gateway schema. Building the connector images and redeploying Run the ./deploy.sh script with your config name and the connectors task from the /scripts directory to generate the secrets for your connectors, build the connector images, and deploy your environment. ./deploy.sh -c <config_name> -t connectors After you add connectors to your environment, you can configure the rest of the configuration ."
  },
  "versions/2.1.3/content/deploy_config_dev.html": {
    "href": "versions/2.1.3/content/deploy_config_dev.html",
    "title": "Configuration development environment",
    "keywords": "Configuration development environment To create a production deployment of i2 Analyze, you must develop the i2 Analyze configuration. The production deployment process describes a process of using different development environments to produce your i2 Analyze configuration. For more information about the environments, see Deployment phases and environments . The analyze-containers repository enables you to create a development environment that comprises a containerised deployment of i2 Analyze. The containerised deployment enables you to switch between deployment patterns easily and redeploy changes in a consistent manner. Process overview: Install the prerequisites for the analyze-containers repository Create a config template Create a custom config that you can deploy into a development environment Use the environment to develop the i2 Analyze configuration If you have already run the createDevEnvironment script and you want to create a new custom config from the template, go to Creating a development config . Prerequisites Before you create the configuration development environment, you must configure the analyze-containers repository. For more information, see Getting started with the Analyze-Containers repository . Creating the configuration development environment To create the configuration development environment and template config, in the /scripts directory run: ./createDevEnvironment.sh The script performs the following actions: Extracts the required files from the i2 Analyze deployment toolkit Builds the required Docker images for the development environment Generates the secrets that are used in the environment Creates the configuration template The configuration template is located in /templates/config-development . Note: At this release, the config development environment is supported with Microsoft SQL Server only. The createDevEnvironment.sh script creates a number of IBM Db2 artifacts, however they cannot be used with the config development environment. Accepting the licences Before you can use i2 Analyze and the tools, you must read the licence agreement and copyright notices. The licence file is in the pre-reqs/i2analyze/license directory. To accept the licence agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the utils/simulatedExternalVariables.sh script. Before you can use Microsoft SQL Server, you must accept the licence agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the licence in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variables are in the utils/simulatedExternalVariables.sh script. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y Modifying the hosts file To enable you to connect to a deployment, the Solr Web UI, and the database, update your hosts file to include entries for the containers: 127.0.0.1 solr1.eia 127.0.0.1 sqlserver.eia 127.0.0.1 i2analyze.eia On Windows, edit the hosts file in C:\\Windows\\System32\\drivers\\etc and the WSL hosts file in /etc/hosts . You can use the integrated terminal in VS Code to edit the WSL hosts file. On MacOS, edit the hosts file in /etc/hosts . You must run Analyst's Notebook Premium on Windows. If you deploy on MacOS, you can use a Windows virtual machine to host Analyst's Notebook Premium. For your virtual machine to connect to i2 Analyze, complete the following: On your MacOS terminal, run ifconfig and identify the IP address for your virtual machine in a section such as vmnet1 . For example, 172.16.100.1 . Then, on your Windows virtual machine add the following line to the C:\\Windows\\System32\\drivers\\etc\\hosts file: 172.16.100.1 i2analyze.eia Installing the certificate To access the system, the server that you are connecting from must trust the certificate that it receives from the deployment. To enable trust, install the /dev-environment-secrets/generated-secrets/certificates/externalCA/CA.cer certificate as a trusted root certificate authority in your browser and operating system's certificate store. For information about installing the certificate, see: Install Certificates with the Microsoft Management Console Add certificates to a keychain using Keychain Access on Mac Creating a config Before you can deploy the environment, create a config from the template: Copy the config-development directory (including the sub-directories) from /templates to the /configs directory. You can rename the config-development folder to a custom config name or use the default value. The name of the directory is used to determine which config to deploy, it is recommended to use a short name with no spaces or special characters. The directory structure is: - configs - <config_name> - configuration - database-scripts - utils The /configs/<config_name>/utils/variables.sh file contains variables that are used when you deploy the environment with this config. You can specify values for the following variables: The DEPLOYMENT_PATTERN variable specifies the deployment pattern for the deployment of i2 Analyze in the environment. You can specify: schema_dev - An i2 Connect gateway only deployment that you can use to develop your data and security models istore - Information Store i2c - i2 Connect gateway only cstore - Chart Store i2c_istore - i2 Connect gateway and Information Store i2c_cstore - i2 Connect gateway and Chart Store For more information about the i2 Analyze deployment patterns, see: Components . The following variables specify the ports that are exposed to the host machine for each component of i2 Analyze. Each variable has a default value: HOST_PORT_SOLR specifies the port that you use to connect to the Solr Web UI from the host machine. By default, 8983 . HOST_PORT_DB specifies the port that you use to connect to the database from the host machine. By default, 1433 . HOST_PORT_I2ANALYZE_SERVICE specifies the port that you use to connect to the deployment from the host machine. By default, 9443 . For example, to deploy with the i2 Connect gateway only for schema development and to use the default ports: DEPLOYMENT_PATTERN=\"schema_dev\" HOST_PORT_SOLR=\"8983\" HOST_PORT_DB=\"1433\" HOST_PORT_I2ANALYZE_SERVICE=\"9443\" Specifying the schema and charting schemes files To deploy i2 Analyze, you must provide a schema and charting scheme. The i2 Analyze toolkit includes example files that you can use as a starting point, or you can use existing files. For more information about the example schemas, see Example schemas . Choose an example schema and associated charting scheme as your starting point from the pre-reqs/i2analyze/toolkit/examples/schemas directory. Copy the chosen schema and charting scheme to your configs/<config_name>/configuration directory. Remove the existing schema.xml and schema-charting-schemes.xml files. Then, rename your chosen schema file to schema.xml and your charting scheme to schema-charting-schemes.xml . Specifying the security schema, user registry, and command access control files To connect to an i2 Analyze deployment, you must provide a security schema and user registry. To control access to features and enable to use of the administrator endpoints, provide a command access control file. The i2 Analyze toolkit includes example files that you can use as a starting point. For more information about the example files, see Example schemas . Copy and overwrite the example security-schema.xml , user-registry.xml , and command-access-control.xml files from the pre-reqs/i2analyze/toolkit/examples/security directory to your configs/<config_name>/configuration directory. Deploying your config in the environment Run the ./deploy.sh script from the /scripts directory to deploy and start your environment. Provide the name of the config to deploy with by using the -c parameter and specify the name of the config directory. For example: ./deploy.sh -c config-development You can run only one environment at a time. When you run the deploy.sh script, it stops any containers on the Docker network. If you are deploying with the same config, it update the images and recreates the containers to update the environment with any configuration changes. For more information, see Managing configs . Accessing the system To connect to the deployment, the URL to use is: https://i2analyze.eia:9443/opal Note: If you are using the schema_dev or i2c deployment patterns, you must connect using Analyst's Notebook Premium. Log in as a user that is specified in the user registry file. If you are using the example user registry, the example user has the following credentials: The user name is Jenny The password is Jenny What to do next? After you deploy your environment, you can start to develop the i2 Analyze configuration. Using your environment for configuration development: Developing schemas in your configuration development environment After you develop your schema files, you can develop the rest of the i2 Analyze configuration. Developing the rest of the configuration"
  },
  "versions/2.1.3/content/develop_config_dev.html": {
    "href": "versions/2.1.3/content/develop_config_dev.html",
    "title": "Developing the rest of the configuration",
    "keywords": "Developing the rest of the configuration After you enable i2 Analyze to work with your data, you can define how analysts interact with your data. To do this, use the following information to modify the i2 Analyze configuration to meet your requirements. Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to your intended deployment pattern. For example, to deploy with the i2 Connect gateway and the Information Store set the following value: DEPLOYMENT_PATTERN=\"i2c_istore\" Note: In the analyze-containers repository, the structure of the configuration directory is modified. The configuration files are in the top level configs/<config_name>/configuration directory. You cannot run any toolkit tasks described in the linked documentation in this environment. Instead, after you modify any configuration files, run the deploy.sh script with your config name as you did when you created your deployment. The list of things that you can configure includes: To configure how analysts search for information, including Quick Search, Visual Query, and Highlight Query, refer to Configuring search To configure how analysts can identify matching records, refer to Configuring matching To configure the geospatial mapping options available to analysts, refer to Configuring geospatial mapping To configure the values that analysts can provide in source references, refer to Configuring source references To configure item type access for analysts, refer to Configuring item type security To update the Solr index configuration, refer to Configuring the Solr index For more information about other configuration changes that you can make, see Configuring i2 Analyze ."
  },
  "versions/2.1.3/content/develop_extensions.html": {
    "href": "versions/2.1.3/content/develop_extensions.html",
    "title": "Developing and deploying extensions for i2 Analyze",
    "keywords": "Developing and deploying extensions for i2 Analyze IBM i2 Analyze Developer Essentials contains tools, libraries, and examples that enable development and deployment of custom extensions to i2 Analyze. The following information describes how you can use the config development environment to quickly deploy an extension for a config to test and develop. For more information about the types of extension that are available, see IBM i2 Analyze Developer Essentials . Prerequisites Install the Java Extension Pack in VS Code. Ensure to have the correct JDK configured: Run the Java Configuration Wizard by pressing F1 and typing Java: Configure Java Runtime . Install temurin 11 JDK if it isn't already installed in your machine. Install Maven. For more information about installing Maven, see Installing Apache Maven . Modify the VS Code settings.json file: Open the settings.json file by pressing F1 and typing Preferences: Open Settings (JSON) . Add the classpath information to deploy extensions. In the settings.json file , add the following object: \"java.project.referencedLibraries\": [ \"pre-reqs/i2analyze/toolkit/application/shared/lib/*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/disco-api-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/solr-core-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/solr-solrj-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/Daod.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/httpclient-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/httpcore-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/ApolloLegacy.jar\" ] Creating a project for your extension You must create a Maven artifact to develop your extension in. Copy the template templates/extension-development directory to the i2a-extensions directory and rename it to be the name of your extension. This name is also the artifact id. For example, name the directory opal-default-security-example . The directory structure is: - i2a-extensions - <artifact_id> - src - main - java - test - java - pom.xml Update the pom.xml file inside your new directory with information about your extension. The elements to modify are at the root of the file. Update the values of the following elements: <groupId> is the name of the Java package <artifactId> matches the name of the directory for your extension <version> is the version number for your extension For example: <groupId>com.i2group</groupId> <artifactId>opal-default-security-example</artifactId> <version>1.0.0</version> You can now start developing your extension. For examples on i2 Analyze extensions, check the examples in IBM i2 Analyze Developer Essentials . Copy the contents of src/main folder of the extension into the i2a-extensions/<artifact_id>/src/main directory. Add any settings that are required by the extension to the analyze-settings.properties file. Add any resources that are required by the extension to the /configs/<config_name>/configuration directory. For example, the group-based-default-security-dimension-values.xml file. If your extension depends on another extension. You must add it to your pom.xml file under the comment: <!-- Extension specific dependencies --> . For example, the auditing extensions in Developer Essentials depend on the opal-audit-example-common extension. To add it as a dependency, populate the values of the pom.xml file as follows: <!-- Extension specific dependencies --> <dependency> <groupId>com.i2group</groupId> <artifactId>opal-audit-example-common</artifactId> <version>1.0.0</version> </dependency> Adding extensions to a config In the configuration you want to deploy your extension with, update the /configs/<config_name>/configuration/extension-references.json file and add the directory name for your extension in the extensions array. For example, to add the extension in i2a-extensions/opal-default-security-example the extension-references.json file contains the following extensions array: { \"extensions\": [ { \"name\": \"opal-default-security-example\", \"version\": \"1.0.0\" } ] } Note: The extensions in the extension-references.json file are loaded in order. If an extension depends on another extension, define the dependent extension first. Deploying your extension Use the deploy.sh script with the extensions task to build the extensions for your configuration. For example, to build and deploy the extensions declared in extension-references.json for your configuration: ./deploy.sh -c <config_name> -t extensions If you do not want to build and run all of the declared extensions, you can use the -i and -e options to include or exclude extensions from the process. For example, to build and run only the extensions named opal-default-security-example and opal-default-security-example-2 : ./deploy.sh -c <config_name> -t extensions -i opal-default-security-example -i opal-default-security-example-2 Debugging your extension Deploying in debug mode To deploy with one or more Liberty servers running in debug mode, change the local variable DEBUG_LIBERTY_SERVERS in utils/internalHelperVariables.sh to be a list of the container names of the liberty servers you wish to run in debug mode. For example: DEBUG_LIBERTY_SERVERS=(\"${LIBERTY1_CONTAINER_NAME}\") Then deploy from a clean environment. When the Liberty servers start they all wait for a debugger to be attached before continuing. Note: Each Liberty server will open and expose a unique debug port (starting at port 7777) and wait for a debugger. This behaviour is defined by the WLP_DEBUG_SUSPEND environment variable in the liberty container and could be changed if needed. Attaching a debugger The project is shipped with a default launch configuration which can be changed in .vscode/launch.json . Run the Java debugger by pressing F5 and choose Debug Extensions if asked."
  },
  "versions/2.1.3/content/develop_schemas.html": {
    "href": "versions/2.1.3/content/develop_schemas.html",
    "title": "Developing schemas in your configuration development environment",
    "keywords": "Developing schemas in your configuration development environment To reduce the time that it takes to update the environment with your schema changes, use the schema_dev deployment pattern. Note: In the analyze-containers repository, the structure of the configuration directory is modified. The configuration files are in the top level configs/<config_name>/configuration directory. You cannot run any toolkit tasks described in the linked documentation in this environment. Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to schema_dev . For example: DEPLOYMENT_PATTERN=\"schema_dev\" Developing the data model In i2 Analyze Schema Designer, modify the schema.xml and schema-charting-scheme.xml files in the <config_name>\\configuration directory with your changes. For more information about developing your schema, see Updating the schema and the charting scheme . After you make your changes, run the deploy.sh script again with your config name. Test your changes in Analyst's Notebook Premium. Note: You might need to log out of Analyst's Notebook Premium and log back in to see your changes. Developing security model Update the security-schema.xml file in the <config_name>\\configuration directory with your changes. For more information about developing your security schema, see Configuring the security schema . After you make your changes, run the deploy.sh script again with your config name. Update the user-registry.xml file to align to your security schema changes. For more information about updating the user registry, see Configuring the Liberty user registry . Note: In this release, you must ensure that your user registry file contains the user Jenny in the Administrator group. After you make your changes, run the deploy.sh script again with your config name. Update the command-access-control.xml file to align to your users and groups. For more information about controlling access to features, see The command access control file . After you make your changes, run the deploy.sh script again with your config name. Test your changes in Analyst's Notebook Premium. Note: You might need to log out of Analyst's Notebook Premium and log back in to see your changes. After you develop your schemas, Add data to your config development environment ."
  },
  "versions/2.1.3/content/develop_solr_config_dev.html": {
    "href": "versions/2.1.3/content/develop_solr_config_dev.html",
    "title": "Configuring the Solr index",
    "keywords": "Configuring the Solr index The way that Solr indexes data can be configured using the files in the /configs/<config_name>/configuration/solr directory. The deployed Solr configuration is built from the template schema-template-definition.xml file and other files referenced within it. Any reference files must also be in the configuration/solr directory. For more information about the changes you can make to the schema-template-definition.xml file, see Configuring search . Note: The process you complete to update a deployment with the Solr configuration is different when the deployment contains data. If the deployment does not contain data, you clean the deployment and then update the deployment in the usual way If the deployment contains data, the process requires you to back up and restore the database Updating a deployment without data After you change the Solr configuration, clean the deployment by running the deploy.sh script with the clean task. For example: ./deploy.sh -c <config_name> -t clean For more information about the script, see The deploy.sh script . Then, re-deploy in the usual way. For example: ./deploy.sh -c <config_name> Your deployment is running with the updated Solr configuration. Updating a deployment with data When data is present in the system you must use the backup and restore tasks of the deploy script to update the deployment. For more information about these tasks, refer to refer to Back up and restore a development database . After you change the Solr configuration, backup the deployment. For example: ./deploy.sh -c <config_name> -t backup -b <backup_name> Then, restore from the backup that you created. For example: ./deploy.sh -c <config_name> -t restore -b <backup_name> Solr is updated with your configuration, and the data in the database is reindex when the system starts."
  },
  "versions/2.1.3/content/getting_started.html": {
    "href": "versions/2.1.3/content/getting_started.html",
    "title": "Getting started with the analyze-containers repository",
    "keywords": "Getting started with the analyze-containers repository Prerequisites You can run the code in the analyze-containers repository on Windows with Windows Subsystem for Linux and MacOS. Windows Subsystem for Linux (WSL) If you are on Windows, you must use WSL 2 to run the shell scripts in this repository. For information about installing WSL, see Windows Subsystem for Linux Installation Guide . Code Download the tar.gz or clone the analyze-containers repository from https://github.com/i2group/analyze-containers . If you download the tar.gz file, extract it. On Windows, extract it to a location that is in your WSL file system. For example: \\\\wsl$\\Ubuntu-20.04\\home\\<user-name> . Docker Install Docker CE for your operating system. For more information about installing Docker CE, see https://docs.docker.com/engine/installation/ . Mac OS : Install Docker CE Windows : Install Docker CE Set up Docker on WSL 2 When you configure Docker, give Docker permission to mount files from the analyze-containers directory on your local file system. In the Docker Desktop application, navigate to Settings > Resources > File Sharing . After you install Docker, you must allocate enough memory to Docker to run the containers in the example deployment. For this deployment, allocate at least 5GB of memory. For more information about modifying the resources allocated to Docker, see: Docker Desktop for Windows Docker Desktop for Mac Command line tools If you are using WSL, install and configure the following command line tools: jq XMLStarlet For example: sudo apt-get update sudo apt-get install jq sudo apt-get install xmlstarlet If you are using MacOS, install and configure the following command line tools: GNU sed GNU bash Coreutils - GNU core utilities jq XMLStarlet You can install these command line tools by using Homebrew . Run the following commands to install Homebrew and then install the tools with it: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" brew install gnu-sed bash coreutils jq xmlstarlet After you install the tools, ensure they are added to your shell's path. If you installed the tools by using Homebrew, run the following commands: echo 'export PATH=\"$(brew --prefix)/opt/gnu-sed/libexec/gnubin:$PATH\"' >> ~/.bash_profile echo 'export PATH=\"$(brew --prefix)/opt/bash/bin:$PATH\"' >> ~/.bash_profile echo 'export PATH=\"$(brew --prefix)/opt/coreutils/libexec/gnubin:$PATH\"' >> ~/.bash_profile Visual Studio Code The repository is designed to be used with VS Code to create the development environment. Download and install VS Code To make the environment easier to use, install the following extensions in VS Code: Remote - WSL Docker Shellcheck Bash IDE Red Hat XML Red Hat YAML Modify the VS Code settings.json file: Open the settings.json file by pressing F1 and typing Preferences: Open Settings (JSON) . Add the catalog.xml to enable XSD validation. In the settings.json file , add the following JSON object and reference the catalog.xml file at the root of the analyze-containers directory: \"xml.catalogs\": [ \"catalog.xml\" ] If you are using MacOS, ensure the VS Code uses GNU bash for the integrated terminal. In the settings.json file, add the following code: \"terminal.integrated.profiles.osx\": { \"bash\": { \"path\": \"/usr/local/opt/bash/bin/bash\", \"icon\": \"terminal-bash\" } }, \"terminal.integrated.defaultProfile.osx\": \"bash\", To run the scripts in the analyze-containers repository, use the VS Code integrated terminal. To open the integrated terminal, click Terminal > New Terminal . On Windows, ensure that you are in a WSL terminal. You do not need to run any scripts now, but whenever the documentation instructs you to run a script, do so from the integrated terminal. i2 Analyze minimal toolkit Download the i2 Analyze V4.3.4 Minimal for Linux package using the following part number: G01HNML Rename the IBM_I2A_V4.3.4_MIN_LIN.tar.gz file to i2analyzeMinimal.tar.gz , then copy it to the analyze-containers/pre-reqs directory. Note: If you used the analyze-containers repository with a previous version of i2 Analyze, overwrite the existing minimal toolkit. Analyst's Notebook Premium Download i2 Analyst's Notebook Premium version 9.2.4 using the following part number: G01DHML . Install Analyst's Notebook Premium on a Windows machine. Note: If you are running Docker on MacOS, you can install Analyst's Notebook Premium on a Windows virtual machine. For more information, see Installing IBM i2 Analyst's Notebook Premium . JDBC drivers You must provide the JDBC driver to enable the application to communicate with the database. Download the Microsoft JDBC Driver 7.4 for SQL Server archive from https://www.microsoft.com/en-us/download/details.aspx?id=58505 . Extract the contents of the download, and locate the sqljdbc_7.4\\enu\\mssql-jdbc-7.4.1.jre11.jar file. Copy the mssql-jdbc-7.4.1.jre11.jar file to the analyze-containers/pre-reqs/jdbc-drivers directory. What to do next Create and use a development environment to develop an i2 Analyze configuration. For more information, see Configuration development environment . Create an example pre-production deployment that is used to demonstrate how i2 Analyze can be deployed in a distributed cloud environment. For more information, see Pre-production example environment . To understand how the containerised environment is created, you can review the documentation that explains the images, containers, tools, and functions: Images and containers Tools and functions"
  },
  "versions/2.1.3/content/HA walkthroughs/ha.html": {
    "href": "versions/2.1.3/content/HA walkthroughs/ha.html",
    "title": "High availability walkthroughs",
    "keywords": "High availability walkthroughs The high availability walkthroughs are designed to demonstrate how a deployment of i2 Analyze responds to container failure in a containerized environment."
  },
  "versions/2.1.3/content/HA walkthroughs/liberty_failure.html": {
    "href": "versions/2.1.3/content/HA walkthroughs/liberty_failure.html",
    "title": "Failure of the leader Liberty container",
    "keywords": "Failure of the leader Liberty container This section demonstrates how a deployment of i2 Analyze responds to the failure and recovery of the Liberty server that hosts the leader Liberty instance. This section also describes the messages that you should monitor to detect the failure and ensure that the recovery was successful. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How Liberty is deployed for high availability and the role of the Liberty leader. For more information about Liberty configuration, see Liberty . How a load balancer is used and configured in a deployment of i2 Analyze. For more information about the load balancer configuration, see Deploying a load balancer . That the load balancer is used to monitor the status of the i2 Analyze service. In a containerised deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The libertyHadrFailureWalkthrough.sh script simulates the Liberty leader failure. Identifying the leader Liberty To simulate a failure of the leader Liberty, first identify which server hosts the leader instance. To identify which container is running the leader Liberty, check the logs of the Liberty servers for the message: We are the Liberty leader . In the walkthrough script, this is done with the following grep command: grep -q \"We are the Liberty leader\" See the Identifying the leader Liberty section of the walkthrough script. Simulating leader Liberty failure To simulate leader Liberty failure, stop the leader Liberty. For example, if liberty1 is the leader, run: docker stop liberty1 See the Simulating leader Liberty failure section of the walkthrough script. Detecting failure The load balancer is used to monitor and determine the status of the i2 Analyze service. The load balancer is configured to report the status of the deployment. The status can be either ACTIVE , DEGRADED , or DOWN . When the leader is taken offline, the other Liberty server must restart to become the new leader. During this time, both servers are down and the i2 Analyze service is DOWN . When the new leader Liberty starts and only 1 of the servers is down, the status of the i2 Analyze services is DEGRADED . In the walkthrough, the waitFori2AnalyzeServiceStatus function is used to run a while loop around the geti2AnalyzeServiceStatus function to wait until the i2 Analyze service is in the DEGRADED state. The geti2AnalyzeServiceStatus function is an example of how to return the i2 Analyze service status from a load balancer. See the Detecting failure section of the walkthrough script. Fail over When the Liberty leader fails, one of the remaining Liberty servers is elected as the leader. To identify the new Liberty leader, check the logs of the remaining Liberty servers for the message: We are the Liberty leader . See the Fail over section of the walkthrough script. Reinstating high availability To reinstate high availability to the deployment, restart the failed Liberty server. In this example, that restart the Liberty container by running the following command: docker start liberty1 When both Liberty servers are up, the status of the i2 Analyze services is ACTIVE . In the walkthrough, the waitFori2AnalyzeServiceStatus function is used to run a while loop around the geti2AnalyzeServiceStatus function to wait until the i2 Analyze service is in the ACTIVE state. The geti2AnalyzeServiceStatus function is an example of how to return the i2 Analyze service status from a load balancer. The recovered Liberty server is in the non-leader mode when it starts because the new leader has already been elected while the server was unavailable. To determine it is in the non-leader mode, the following message is displayed in the logs: We are not the Liberty leader . See the Reinstating high availability section of the walkthrough script."
  },
  "versions/2.1.3/content/HA walkthroughs/solr_cluster_failure.html": {
    "href": "versions/2.1.3/content/HA walkthroughs/solr_cluster_failure.html",
    "title": "Failure of the Solr cluster",
    "keywords": "Failure of the Solr cluster This section demonstrates how a deployment of i2 Analyze responds to the failure of all Solr nodes. This section also describes how to monitor and detect failure and ensure the recovery was successful. Before you begin the walkthrough, there a number of concepts that it is useful to understand: How Solr is deployed for high availability. For more information, see Solr . How the Solr status is reported in the component availability log in Liberty. For more information, see Monitor the system availability . In a containerised deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The solrClusterFailureWalkthrough.sh scripts simulates a Solr cluster failure and recovery. Simulating Solr Cluster failure To simulate the cluster failure, remove all the Solr containers. For example, run: docker stop solr2 solr1 See the Simulating the cluster failure section of the walkthrough script. Detecting failure The component availability log in Liberty is used to monitor and determine the status of the Solr cluster. When the Solr cluster is unavailable, the status is reported as DOWN . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'DOWN'\" Reinstating high availability To reinstate high availability, restart the failed Solr containers. In this example, restart both Solr containers by running the following command: docker start solr2 solr1 After the failed nodes recover, Liberty reports the changes to the cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. The reinstating high availability section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is active: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'ALL_REPLICAS_ACTIVE'\""
  },
  "versions/2.1.3/content/HA walkthroughs/solr_node_failure.html": {
    "href": "versions/2.1.3/content/HA walkthroughs/solr_node_failure.html",
    "title": "Failure of a Solr node",
    "keywords": "Failure of a Solr node This walkthrough demonstrates losing a Solr node from a collection, and describes how to identify failure, continue operations, and reinstate high availability with your Solr nodes. Before you begin the walkthrough, there a number of concepts that it is useful to understand: How Solr is deployed for high availability. For more information, see Solr . How the Solr status is reported in the component availability log in Liberty. For more information, see Monitor the system availability . In a containerised deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The solrNodeFailureWalkthrough.sh script demonstrates how to monitor the Liberty logs to identify Solr node failure and recovery. In the example, each shard has 2 replicas and 1 replica is located on each Solr node. This means that the Solr cluster can continue to process requests when one Solr node is taken offline. Simulating Solr node failure To simulate a node failure, one of the Solr containers is stopped in the Stop the Solr container section. For example: docker stop solr2 See the Simulating the cluster failure section of the walkthrough script. Detecting failure The component availability log in Liberty is used to monitor and determine the status of the Solr cluster. When the Solr Node is unavailable, the status is reported as DEGRADED . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'DEGRADED'\" Reinstating high availability To reinstate high availability, restart the failed Solr containers. In this example, restart the Solr by running the following command: docker start solr1 After the failed node recovers, Liberty reports the changes to the cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. The reinstating high availability section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is active: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'ALL_REPLICAS_ACTIVE'\""
  },
  "versions/2.1.3/content/HA walkthroughs/zookeeper_quorum_failure.html": {
    "href": "versions/2.1.3/content/HA walkthroughs/zookeeper_quorum_failure.html",
    "title": "Loss of the ZooKeeper quorum",
    "keywords": "Loss of the ZooKeeper quorum This walkthrough demonstrates losing more than 50% of the ZooKeeper servers from an ensemble, which causes the loss of the quorum. It also describes how to identify failure, and reinstate high availability with your ZooKeeper ensemble. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How ZooKeeper is deployed for high availability and how the ensemble functions when one or more ZooKeeper servers fail. For more information, see Configuring ZooKeeper for HADR . When the ZooKeeper quorum is lost, the Solr cluster also fails. To monitor that whether the ZooKeeper quorum is met or not the component availability log in Liberty is used. The status of Solr is used to determine the status of ZooKeeper. In a containerised deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. Simulating loss of quorum To simulate a loss of quorum, more than 50% of the ZooKeeper servers must be stopped. For example, to stop zk1 and zk2 , run: docker stop zk1 zk2 See the Simulating Zookeeper Quorum failure section of the walkthrough script. Detecting failure When the ZooKeeper quorum is lost, the Solr cluster also fails. The Solr status is reported as DOWN . For more information about losing Solr, see Failure of the Solr cluster . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep -q \"^.*[com.i2group.apollo.common.toolkit.internal.ConsoleLogger] - (opal-services) - '.*', .*'DOWN'\" For more information see the Detecting failure section of the walkthrough script. Reinstating high availability To reinstate high availability to the deployment, restart the failed ZooKeeper servers. In this example, restart the ZooKeeper containers by running the following command: docker start zk1 zk2 When enough ZooKeeper servers are up to achieve at least a DEGRADED quorum, the status of the i2 Analyze services is ACTIVE and Liberty reports the changes to the Solr cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. See the Reinstating high availability section of the walkthrough script."
  },
  "versions/2.1.3/content/HA walkthroughs/zookeeper_server_failure.html": {
    "href": "versions/2.1.3/content/HA walkthroughs/zookeeper_server_failure.html",
    "title": "Failure of a ZooKeeper server",
    "keywords": "Failure of a ZooKeeper server This walkthrough demonstrates losing a single ZooKeeper server from an ensemble, and describes how to identify failure, continue operations, and reinstate high availability with your ZooKeeper ensemble. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How ZooKeeper is deployed for high availability. For more information, see Multi-server setup . The ZooKeeper AdminServer is used to monitor the status of the ZooKeeper ensemble The AdminServer . In the zookeeperServerFailureWalkthrough.sh script demonstrates stopping one of the ZooKeeper containers, monitoring the status, and reinstating high availability. Simulating a server failure To simulate a server failure in the ensemble, one of the ZooKeeper servers is stopped. For example, to stop zk1 , run: docker stop zk1 See the Simulating ZooKeeper server failure section of the walkthrough script. Detecting failure When one ZooKeeper server goes offline, the other servers can still make a quorum and remain active. Because the ensemble can sustain only one more server failure, the state is defined as DEGRADED . In the walkthrough, the getZkQuorumEnsembleStatus function is used to monitor and determine the ensemble status by calling the commands/srvr resource on each ZooKeeper servers's admin endpoint and reports the status as DEGRADED when one of the servers is unavailable. See the Detecting failure section of the walkthrough script. Restoring high availability To restore high availability to the ensemble, restart the failed ZooKeeper server. In this example, restart the ZooKeeper container by running the following command: docker start zk1 When the ZooKeeper server is up again, the status of the ensemble is ACTIVE . In the walkthrough, the getZkQuorumEnsembleStatus function is used again to determine the ensemble status. See the Reinstating high availability section of the walkthrough script."
  },
  "versions/2.1.3/content/images and containers/etl_client.html": {
    "href": "versions/2.1.3/content/images and containers/etl_client.html",
    "title": "ETL Client",
    "keywords": "ETL Client An ETL Client container is an ephemeral container that is used to run ETL tasks. Building an ETL Client image The ETL Client image is built from the Dockerfile in images/etl_client . It uses the i2 Analyze Tools image as the base image. Docker build command The following docker build command builds the ETL Client image: docker build -t \"etlclient_redhat:4.3.4\" \"/images/etl_client\" \\ --build-arg USER_UID=\"$(id -u \"${USER}\")\" \\ --build-arg BASE_IMAGE=\"i2a_tools_redhat:4.3.4\" The --build-arg flag is used to provide your local user ID to the Docker image when it is built. The value of $USER comes from your shell. For examples of the build commands, see the buildImages.sh script. Running an ETL Client container A ETL Client container uses the ETL Client image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs the ETL Client container: docker run --rm \\ --name \"etl_client\" \\ --network \"eia\" \\ --user \"$(id -u \"${USER}\"):$(id -u \"${USER}\")\" \\ -v \"/examples/pre-prod/configuration/logs:/opt/configuration/logs\" \\ -v \"/prereqs/i2analyze/toolkit/examples/data:/var/i2a-data\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_NAME=\"ISTORE\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_OS_TYPE=\"UNIX\" \\ -e DB_INSTALL_DIR=\"/opt/mssql-tools\" \\ -e DB_LOCATION_DIR=\"/var/opt/mssql/data\" \\ -e ETL_TOOLKIT_JAVA_HOME=\"/opt/java/openjdk\" \\ -e DB_USERNAME=\"i2etl\" \\ -e DB_PASSWORD=\"DB_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"etlclient_image:4.3.4\" \"$@\" For an example of the docker run command, see runEtlToolkitToolAsi2ETL function in the clientFunctions.sh script. For an example of how to use runEtlToolkitToolAsi2ETL function, see runEtlToolkitToolAsi2ETL . Environment variables The ETl Client is built on top of the SQL Client. Any environment variables referenced in the SQL Client can be used in the ETL Client. Additional Environment variables Environment Variable Description DB_DIALECT The database dialect. Currently only sqlserver is supported DB_OS_TYPE The Operating System that the database is on. Can be UNIX , WIN , or AIX . DB_INSTALL_DIR Specifies the database CMD location. DB_LOCATION_DIR Specifies the location of the database. ETL_TOOLKIT_JAVA_HOME Specifies the location on Java. Useful links Defining an ingestion source"
  },
  "versions/2.1.3/content/images and containers/i2analyze_tool.html": {
    "href": "versions/2.1.3/content/images and containers/i2analyze_tool.html",
    "title": "i2 Analyze Tool",
    "keywords": "i2 Analyze Tool An i2 Analyze Tool container is an ephemeral container that is used to run the i2 Analyze tools. For more information about the tools, see i2 Analyze tools . Building the i2 Analyze Tool image The i2 Analyze Tool image is built from the Dockerfile in images/i2a_tools . The image contains the i2-tools & scripts folder from the toolkit. Docker build command The following docker build command builds the i2 Analyze Tool image: docker image build -t i2a_tools_redhat:4.3.4 images/i2a_tools \\ --build-arg USER_UID=\"$(id -u \"${USER}\")\" The --build-arg flag is used to provide your local user ID to the Docker image when it is built. The value of $USER comes from your shell. The local user ID is required so that a user is created in the Docker container with the same user ID as the local user. The user is required to ensure that the local user can access any files that are generated on the container and mounted to the host via a bind mount. For examples of the build commands, see buildImages.sh script. Running an i2 Analyze Tool container An i2 Analyze Tool container uses the i2 Analyze Tool image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs an i2 Analyze Tool container: docker run \\ --rm \\ --name \"i2atool\" \\ --network \"eia\" \\ --user \"$(id -u \"${USER}\"):$(id -u \"${USER}\")\" \\ -v \"/examples/pre-prod/configuration:/opt/configuration\" \\ -v \"/examples/pre-prod/database-scripts/generated:/opt/databaseScripts/generated\" \\ -e LIC_AGREEMENT=\"ACCEPT\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_NAME=\"ISTORE\" \\ -e CONFIG_DIR=\"/opt/configuration\" \\ -e GENERATED_DIR=\"/opt/databaseScripts/generated\" \\ -e DB_USERNAME=\"dba\" \\ -e DB_PASSWORD=\"DBA_PASSWORD\" \\ -e DB_OS_TYPE=\"UNIX\" \\ -e DB_INSTALL_DIR=\"/opt/mssql-tools\" \\ -e DB_LOCATION_DIR=\"/var/opt/mssql/data\" \\ -e SOLR_ADMIN_DIGEST_USERNAME=\"solr\" \\ -e SOLR_ADMIN_DIGEST_PASSWORD=\"SOLR_ADMIN_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD=\"ZOO_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD=\"ZOO_DIGEST_READONLY_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SSL_PRIVATE_KEY=\"SSL_PRIVATE_KEY\" \\ -e SSL_CERTIFICATE=\"SSL_CERTIFICATE\" \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"i2a_tools_redhat:4.3.4\" \"$@\" Bind mounts Configuration : The i2 Analyze Tool container requires the i2 Analyze configuration. To access the configuration, the configuration directory must be mounted into the container. The CONFIG_DIR environment variable must specify the location where the configuration is mounted. Generated scripts directory : Some of the i2 Analyze tools generate scripts to be run against the Information Store database, or run scripts that were generated by other i2 Analyze tools. For the i2 Analyze Tool container to interact with these scripts, the directory where they are generated must be mounted into the container. In the example scripts, this is defaulted to /database-scripts/generated . The GENERATED_DIR environment variable must specify the location where the generated scripts are mounted. This directory must be created with your local user before mounting it to the container, otherwise docker will create this directory as a root. Environment variables To configure the i2 Analyze Tool container, you provide environment variables to the Docker container in the docker run command. The following table describes the supported environment variables that you can use: Environment variable Description LIC_AGREEMENT The license agreement for use the i2Analyze tools. ZK_HOST The connection string the ZooKeeper client to connect to. DB_DIALECT See i2a Tools Environment Variables . DB_SERVER See i2a Tools Environment Variables . DB_PORT See i2a Tools Environment Variables . DB_NAME See i2a Tools Environment Variables . CONFIG_DIR The root location of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. CLASSES_PATH The location to the files required by the i2 Analyze tools. The files are built into the image. DB_USERNAME See i2a Tools Environment Variables . DB_PASSWORD See i2a Tools Environment Variables . SOLR_ADMIN_DIGEST_USERNAME The name of the administrator user for performing administration tasks. SOLR_ADMIN_DIGEST_PASSWORD The password for the administrator user. ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. The following environment variables enable you to use SSL: Environment variable Description SERVER_SSL See Secure Environment variables . DB_SSL_CONNECTION See Secure Environment variables . SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SSL_PRIVATE_KEY See Secure Environment variables . SSL_CERTIFICATE See Secure Environment variables . SSL_CA_CERTIFICATE See Secure Environment variables . GATEWAY_SSL_CONNECTION See Secure Environment variables . SSL_OUTBOUND_PRIVATE_KEY See Secure Environment variables . SSL_OUTBOUND_CERTIFICATE See Secure Environment variables . NOTE: when you set SERVER_SSL and SSL_CA_CERTIFICATE environment variables, the CA.cer certificate will be located at /tmp/i2acerts/CA.cer . when you set GATEWAY_SSL_CONNECTION , SSL_OUTBOUND_PRIVATE_KEY and SSL_OUTBOUND_CERTIFICATE environment variables, the certificated will be located at /tmp/i2acerts/i2Analyze.pem ."
  },
  "versions/2.1.3/content/images and containers/images.html": {
    "href": "versions/2.1.3/content/images and containers/images.html",
    "title": "Images and containers",
    "keywords": "Images and containers The documentation in this section describes the images and containers that are required for a deployment of i2 Analyze in a containerised environment."
  },
  "versions/2.1.3/content/images and containers/liberty.html": {
    "href": "versions/2.1.3/content/images and containers/liberty.html",
    "title": "Liberty",
    "keywords": "Liberty In a containerized deployment, you configure the i2 Analyze application and Liberty in an image that is layered on top of the liberty_ubi_base image. The liberty_ubi_base contains static configuration and application jars that are required by i2 Analyze and should not be changed. Configuring the Liberty server Liberty is configured by exception. The runtime environment operates from a set of built-in configuration default settings, and you only need to specify configuration that overrides those default settings. You do this by editing either the server.xml file or another XML file that is included in server.xml at run time. In a containerized deployment of i2 Analyze, a server.xml file is provided for you. To provide or modify any values, you specify a number of environment variables when you run a Liberty container. Additionally, you can extend the server.xml by using the provided server.extensions.xml in the i2 Analyze configuration. Any elements that you add to the extensions file are included in the server.xml when you run a Liberty container. Configuring the i2 Analyze application The contents of the configuration directory must be copied into the images/liberty_ubi_combined/classes directory. The contents of the classes directory is added to the configured Liberty image when the image is built. If you make changes to the configuration, you must copy the changes to the classes directory and rebuild the configured image. Note: The system match rules are configured differently. The application is updated to use the system-match-rules.xml from the Solr client command line. For more information about updating the system match rules, see Updating the system match rules . Building a configured Liberty image The configured image is built from the base image. The configured image contains the i2 Analyze application and Liberty configuration that is required to start the i2 Analyze application. When you change the configuration, the configured image must be rebuilt to reflect the changes. Docker build command The configured image is built from the Dockerfile in images/liberty_ubi_combined . The following docker build command builds the configured image: docker build -t \"liberty_configured_redhat:4.3.4\" images/liberty_ubi_combined --build-arg BASE_IMAGE=\"liberty_redhat:4.3.4\" An example of providing the configuration to the classes directory and building the image is included in the buildLibertyConfiguredImage function in the serverFunctions.sh script. Running a Liberty container A Liberty container uses the configured image. In the docker run command, you can use -e to pass environment variables to Liberty on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Liberty container: docker run -m 1g -d \\ --name \"liberty1\" \\ --network \"eia\" \\ --net-alias \"liberty1.eia\" \\ -p \"9045:9443\" \\ -v \"liberty1_secrets:/run/secrets\" \\ -v \"liberty1_data:/data\" \\ -e LICENSE=\"accept\" \\ -e FRONT_END_URI=\"https://liberty.eia:9045/opal\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_USERNAME=\"i2analyze\" \\ -e DB_PASSWORD_FILE=\"/run/secrets/DB_PASSWORD\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_PASSWORD\" \\ -e SOLR_HTTP_BASIC_AUTH_USER=\"liberty\" \\sp -e SOLR_HTTP_BASIC_AUTH_PASSWORD_FILE=\"/run/secrets/SOLR_APPLICATION_DIGEST_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ -e SSL_CA_CERTIFICATE_FILE=\"/run/secrets/CA.cer\" \\ -e GATEWAY_SSL_CONNECTION=true \\ -e SSL_OUTBOUND_PRIVATE_KEY_FILE=\"/run/secrets/gateway_user.key\" \\ -e SSL_OUTBOUND_CERTIFICATE_FILE=\"/run/secrets/gateway_user.cer\" \\ -e SSL_OUTBOUND_CA_CERTIFICATE_FILE=\"/run/secrets/outbound_CA.cer\" \\ -e LIBERTY_HADR_MODE=1 \\ -e LIBERTY_HADR_POLL_INTERVAL=1 \\ liberty_configured_redhat For an example of the docker run command, see serverFunctions.sh . The runLiberty function takes the following arguments to support running multiple Liberty containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container and the Solr host. VOLUME - the name for the named volume of the Liberty container. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. KEY_FOLDER - The folder with keys and certificates for the container. For more information, see Security . An example of running Liberty container by using runLiberty function: runLiberty liberty1 liberty1.eia liberty1_data 9045 liberty1 Volumes A named volume is used to persist data, which is generated and used in the Liberty container, outside of the container. To configure the Liberty container to use the volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For Liberty, the directory that is mounted must be /data , this directory folder stores: jobs, record groups and charts. For example: -v liberty_data:/data \\ -v liberty1_secrets:/run/secrets Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access ZooKeeper, the database, and the certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure the Liberty server, you provide environment variables to the Docker container in the docker run command. The following table describes the supported environment variables that you can use: Environment variable Description FRONT_END_URI The URI that clients use to connect to i2 Analyze. For more information, see Specifying the connection URI . DB_DIALECT Specifies which database management system to configure i2 Analyze for. In this release, it can be set to sqlserver . For more information, see properties.microsoft.sqlserver . DB_SERVER Specifies the fully qualified domain name of the database server to connect to. The value populates the serverName attribute in the Liberty server configuration. For more information, see properties.microsoft.sqlserver . DB_PORT Specifies the port number of the SQL Server database to connect to. The value populates the portNumber attribute in the Liberty server configuration. You can specify DB_PORT or DB_INSTANCE . For more information, see properties.microsoft.sqlserver . DB_USERNAME The database user that is used by Liberty to connect to the database. DB_PASSWORD The database user password. ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated. The connection string must be in the following format: <hostname>:<port>,<hostname>:<port> . SOLR_HTTP_BASIC_AUTH_USER The Solr user that Liberty uses to connect to Solr. This is not an administrator user. SOLR_HTTP_BASIC_AUTH_PASSWORD The Solr user password. ZOO_DIGEST_USERNAME The ZooKeeper user that is used by Liberty to connect to ZooKeeper. ZOO_DIGEST_PASSWORD The ZooKeeper user password. The following environment variables enable you to use SSL: Environment variable Description DB_SSL_CONNECTION See Secure Environment variables . SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . GATEWAY_SSL_CONNECTION See Secure Environment variables . SSL_OUTBOUND_PRIVATE_KEY_FILE See Secure Environment variables . SSL_OUTBOUND_CERTIFICATE_FILE See Secure Environment variables . SSL_OUTBOUND_CA_CERTIFICATE_FILE See Secure Environment variables . Liberty HADR You can run Liberty in an active/active configuration with multiple Liberty containers. In an active/active configuration, multiple instance of the i2 Analyze application run concurrently on multiple Liberty containers. One instance of the i2 Analyze application is determined to be the leader at any given time. The following tables describes the environment variables that you can use to configure HADR: Environment variable Description LIBERTY_HADR_MODE Can be set to 1 or 0. If set to 1, Liberty starts in HADR mode. The default is 0. LIBERTY_HADR_POLL_INTERVAL The interval in minutes to poll for liberty leadership status. The default is 5. LIBERTY_HADR_MAX_ERRORS The maximum number of errors allowed before Liberty initiates a leadership poll. The time span in which the errors can occur is determined by LIBERTY_HADR_ERROR_TIME_SPAN . The default is 5. LIBERTY_HADR_ERROR_TIME_SPAN The time span in seconds for the LIBERTY_HADR_MAX_ERRORS to occur within. The default is 30."
  },
  "versions/2.1.3/content/images and containers/solr.html": {
    "href": "versions/2.1.3/content/images and containers/solr.html",
    "title": "Solr",
    "keywords": "Solr In a containerized deployment, Solr is configured and run from a Solr image. Configuring Solr Solr is configured by the solr.xml file. A default solr.xml configuration is generated by the Solr container. To modify the solr.xml , you can modify the Dockerfile to build an image with your solr.xml file. The Dockerfile in images/solr_redhat contains a commented out example COPY command that copies a solr.xml into the image. By copying a solr.xml file into the image, the container does not generate a default solr.xml . For more information about the file, see Format of solr.xml . Building a Solr image The Solr image for i2 Analyze is built from a Dockerfile that is based on the Dockerfile from Apache Solr. The Dockerfile is modified to configure Solr for use with i2 Analyze. For more information about the Dockerfile provided by Apache Solr, see docker-solr . Docker build command The Solr image is built from the Dockerfile in images/solr_redhat . The following docker build command builds the Solr image: docker build -t \"solr_redhat:4.3.4\" images/solr_redhat For examples of the build commands, see buildImages.sh script. Running a Solr container A Solr container uses the Solr image. In the docker run command, you can use -e to pass environment variables to Solr on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Solr server container: docker run -d \\ --name \"sol1\" \\ --net \"eia\" \\ --net-alias \"solr1.eia\" \\ --init \\ -p 8983:8983 \\ -v \"solr1_data:/var/solr\" \\ -v \"solr1_secrets:/run/secrets\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e SOLR_HOST=\"solr1.eia\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_READONLY_PASSWORD\" \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ -e SSL_CA_CERTIFICATE_FILE=\"/run/secrets/CA.cer\" \\ \"solr_redhat:4.3.4\" For an example of the docker run command, see serverFunctions.sh . The runSolr function takes the following arguments to support running multiple Solr containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container and the Solr host. VOLUME - The name for the named volume of the Solr container. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. An example of running Solr container by using runSolr function: runSolr solr1 solr1.eia solr1_data 8983 Volumes A named volume is used to persist data and logs that are generated and used in the Solr container, as well as a separate volume for backups, outside of the container. To configure the Solr container to use the volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For Solr, the directory that must be mounted is /var/solr . For example: -v solr1_data:/var/solr \\ -v solr_backup:/backup \\ -v solr1_secrets:/run/secrets A unique volume name must be used for each Solr container. For more information, see How the image works . Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure Solr, you can provide environment variables to the Docker container in the docker run command. Solr The following table describes the supported environment variables that you can use for Solr: Environment variable Description SOLR_HOST Specifies the fully qualified domain name of the Solr container. ZooKeeper authentication The following environment variables are used to configure Solr to connect to ZooKeeper as a client: Environment variable Description ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated list. The connection string must be in the following format: <hostname>:<port>,<hostname>:<port> . ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. For more information about configuring Solr to connect to ZooKeeper, see: Client Configuration Parameters . ZooKeeper Access Control Solr SSL The following environment variables enable you use SSl with Solr Environment variable Description SOLR_ZOO_SSL_CONNECTION See below. SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . For more information about the Solr SSL configuration, see Set Common SSL-Related System Properties . For more information about the ZK SSL configuration, see Client Configuration Parameters . Solr Environment Variable Mapping The above environment variables are either passed through to the standard Solr launch script or used to construct the following Solr environment variables. For exact details see the Docker image. SOLR_ZK_CREDS_AND_ACLS SOLR_OPTS"
  },
  "versions/2.1.3/content/images and containers/solr_client.html": {
    "href": "versions/2.1.3/content/images and containers/solr_client.html",
    "title": "Solr Client",
    "keywords": "Solr Client A Solr Client container is an ephemeral container that is used to run Solr commands. Building a Solr Client container The Solr Client uses the same image as the Solr Server container. For more information about building the Solr image, see Solr . Running a Solr Client container A Solr Client container uses the Solr image. In the docker run command, you can use -e to pass environment variables to Solr on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Solr Client container: docker run --rm \\ --net \"eia\" \\ -v \"/examples/pre-prod/configuration:/opt/configuration\" \\ -e SOLR_ADMIN_DIGEST_USERNAME=\"solr\" \\ -e SOLR_ADMIN_DIGEST_PASSWORD=\"SOLR_ADMIN_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD=\"ZOO_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD=\"ZOO_DIGEST_READONLY_PASSWORD\" \\ -e SECURITY_JSON=\"SECURITY_JSON\" \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SSL_PRIVATE_KEY=\"SSL_PRIVATE_KEY\" \\ -e SSL_CERTIFICATE=\"SSL_CERTIFICATE\" \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"solr_redhat:4.3.4\" \"$@\" For an example of the docker run command, see runSolrClientCommand function in clientFunctions.sh script. For an example of how to use runSolrClientCommand function, see runSolrClientCommand . Bind mounts Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Configuration : The Solr client requires the i2 Analyze configuration to perform some Solr operations. To access the configuration, the configuration directory must be mounted into the container. Environment variables To configure the Solr client, you can provide environment variables to the Docker container in the docker run command. Environment variable Description SOLR_ADMIN_DIGEST_USERNAME For usage see Command Parsing SOLR_ADMIN_DIGEST_PASSWORD For usage see Command Parsing ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. SECURITY_JSON The Solr security.json. Solr Basic Authentication SOLR_ZOO_SSL_CONNECTION See Secure Environment Variables . SERVER_SSL See Secure Environment Variables . SSL_PRIVATE_KEY See Secure Environment Variables . SSL_CERTIFICATE See Secure Environment Variables . SSL_CA_CERTIFICATE See Secure Environment Variables . Command parsing When commands are passed to the Solr client by using the \"$@\" notation, the command that is passed to the container must be escaped correctly. On the container, the command is run using docker exec \"$@\" . Because the command is passed to the docker run command using bash -c , the command must be maintained as a double quoted string. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\\\" -H Content-Type:text/xml --data-binary \\\"<delete><query>*:*</query></delete>\\\"\" Different parts of the command must be escaped in different ways: \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" Because the curl command uses the container's local environment variables to obtain the values of SOLR_ADMIN_DIGEST_USERNAME and SOLR_ADMIN_DIGEST_PASSWORD , the $ is escaped by a \\ . The \" around both of the variables are escaped with a \\ to prevent the splitting of the command, which means that the variables are evaluated in the container's environment. \\\"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\\\" The URL is surrounded in \" because the string contains a variable. The \" are escaped with a \\ . Because the SOLR1_FQDN variable is evaluated before it is passed to the container, the $ is not escaped. \\\"<delete><query>*:*</query></delete>\\\" The data portion of the curl command is escaped with \" because it contains special characters. The \" are escaped with a \\ ."
  },
  "versions/2.1.3/content/images and containers/sql_client.html": {
    "href": "versions/2.1.3/content/images and containers/sql_client.html",
    "title": "SQL Server Client",
    "keywords": "SQL Server Client An SQL Server Client container is an ephemeral container that is used to run the sqlcmd commands to create and configure the database. Building an SQL Server Client image The SQL Server Client is built from a Dockerfile that is based on Microsoft SQL Server . The SQL Server Client image is built from the Dockerfile in images/sql_client . Docker build command The following docker build command builds the SQL Server Client image: docker build -t \"sqlserver_client_redhat:4.3.4\" images/sql_client Running a SQL Server Client container An SQL Server Client container uses the SQL Server Client image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command runs a SQL Server Client container: docker run \\ --rm \\ --name \"sqlclient\" \\ --network \"eia\" \\ -v \"pre-reqs/i2analyze/toolkit:/opt/toolkit\" \\ -v \"/examples/pre-prod/database-scripts/generated:/opt/databaseScripts/generated\" \\ -e SQLCMD=\"/opt/mssql-tools/bin/sqlcmd\" \\ -e SQLCMD_FLAGS=\"-N -b\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_NAME=\"ISTORE\" \\ -e GENERATED_DIR=\"/opt/databaseScripts/generated\" \\ -e DB_USERNAME=\"dba\" \\ -e DB_PASSWORD=\"DBA_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"sqlserver_client_redhat:4.3.4\" \"$@\" For an example of the docker run command, see runSQLServerCommandAsETL function in clientFunctions.sh script. For an example of how to use runSQLServerCommandAsETL function, see runSQLServerCommandAsETL . Note: you can run SQL Server Client container as different users, see runSQLServerCommandAsDBA , runSQLServerCommandAsSA Bind mounts Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access ZooKeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_CA_CERTIFICATE_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Toolkit : For the SQL Server Client to use the tools in /opt/toolkit/i2-tools/scripts , the toolkit must be mounted into the container. In the example scripts, this is defaulted to /opt/toolkit . Generated scripts directory : Some of the i2 Analyze tools generate scripts to be run against the Information Store database. For the SQL Server Client to run these scripts, the directory where they are generated must be mounted into the container. In the example scripts, this is defaulted to /database-scripts/generated . The GENERATED_DIR environment variable must specify the location where the generated scripts are mounted. Environment variables Environment Variable Description DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_USERNAME The user. DB_PASSWORD The password. GENERATED_DIR The root location where any generated scripts are created. The following environment variables enable you use SSL Environment variable Description DB_SSL_CONNECTION See Secure Environment variables . SSL_CA_CERTIFICATE See Secure Environment variables . Command parsing When commands are passed to the Solr client by using the \"$@\" notation, the command that is passed to the container must be escaped correctly. On the container, the command is run using docker exec \"$@\" . Because the command is passed to the docker run command using bash -c , the command must be maintained as a double quoted string. For example: runSQLServerCommandAsETL bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -S \\${DB_SERVER},${DB_PORT} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} -Q \\\"BULK INSERT ${STAGING_SCHEMA}.${table_name} FROM '/var/i2a-data/${BASE_DATA}/${csv_and_format_file_name}.csv' WITH (FORMATFILE = '/var/i2a-data/${BASE_DATA}/sqlserver/format-files/${csv_and_format_file_name}.fmt', FIRSTROW = 2)\\\"\" Different parts of the command must be escaped in different ways: \\${DB_SERVER} , \\${DB_USERNAME} , \\${DB_PASSWORD} , and \\${DB_NAME} Because the command uses the container's local environment variables to obtain the values of these variables, the $ is escaped by a \\ . ${DB_PORT} is not escaped because this is an environment variable available to the script calling the client function. \\\"BULK INSERT ${STAGING_SCHEMA}.${table_name} ... FIRSTROW = 2)\\\" The string value for the -Q argument must be surrounded by \" when it is run on the container. The surrounding \" are escaped with \\ . The variables that are not escaped in the string are evaluated outside of the container when the function is called."
  },
  "versions/2.1.3/content/images and containers/sql_server.html": {
    "href": "versions/2.1.3/content/images and containers/sql_server.html",
    "title": "SQL Server",
    "keywords": "SQL Server In a containerized deployment, the database is located on a SQL Server container. Building a SQL Server image SQL Server is built from a Dockerfile that is based on the Dockerfile from Microsoft SQL Server . The SQL Server image is built from the Dockerfile in images/sql_server . Docker build command The following docker build command builds the SQL Server image: docker build -t \"sqlserver_redhat:4.3.4\" images/sqlserver For examples of the build commands, see buildImages.sh script. Running a SQL Server container A SQL Server container uses the SQL Server image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command runs a SQL Server container: docker run -d \\ --name \"sqlserver\" \\ --network \"eia\" \\ --net-alias \"sqlserver.eia\" \\ -p \"1433:1433\" \\ -v \"sqlserver_data:/var/opt/mssql\" \\ -v \"sqlserver_sqlbackup:/backup\" \\ -v \"sqlserver_secrets:/run/secrets/\" \\ -v \"i2a_data:/var/i2a-data\" \\ -e ACCEPT_EULA=\"Y\" \\ -e MSSQL_AGENT_ENABLED=true \\ -e MSSQL_PID=\"Developer\" \\ -e SA_PASSWORD_FILE=\"/run/secrets/SA_PASSWORD_FILE\" \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ \"sqlserver_redhat:4.3.4\" For an example of the docker run command, see serverFunctions.sh . The runSQLServer does not take any arguments. Volumes Named volumes are used to persist data and logs that are generated and used in the SQL Server container, as well as a separate volume for backups, outside of the container. Note: It is good practice to have a separate volume for the backup from the database storage. For more information, see SQL Server Backup best practices . To configure the SQL Server container to use these volumes, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For SQL Server, the path to the directory that must be mounted is /var/opt/mssql . For example: -v sqlvolume:/var/opt/mssql -v sqlserver_sqlbackup:/backup For more information, see Use Data Volume Containers . Bind mounts Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_PRIVATE_KEY_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Example Data : To demonstrate ingesting data into the Information Store, the i2 Analyze toolkit is mounted to /var/i2a-data in the container. Environment variables Environment Variable Description ACCEPT_EULA Set to Y to confirm your acceptance of the End-User Licensing Agreement . MSSQL_AGENT_ENABLED For more information see Configure SQL Server settings with environment variables on Linux MSSQL_PID For more information see Configure SQL Server settings with environment variables on Linux SA_PASSWORD The administrator user's password. The following environment variables enable you to use SSL: Environment variable Description SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . For more information about the SSL in SQLServer, see Specify TLS settings ."
  },
  "versions/2.1.3/content/images and containers/zookeeper.html": {
    "href": "versions/2.1.3/content/images and containers/zookeeper.html",
    "title": "ZooKeeper",
    "keywords": "ZooKeeper The ZooKeeper image for i2 Analyze is built from a Dockerfile that is based on the Dockerfile from Apache ZooKeeper. The Dockerfile is modified to configure ZooKeeper for use with i2 Analyze. Building a ZooKeeper image The ZooKeeper image is built from the Dockerfile located in images/zookeeper_redhat . Docker build command The The following docker build command builds the ZooKeeper image: docker build -t \"zookeeper_redhat:4.3.4\" images/zookeeper_redhat Running a ZooKeeper container A ZooKeeper container uses the ZooKeeper image. In the docker run command, you can use -e to pass environment variables to ZooKeeper on the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command starts a ZooKeeper container: docker run --restart always -d \\ --name \"zk1\" \\ --net \"eia\" \\ --net-alias \"zk1.eia\" \\ -p \"8080:8080\" \\ -p \"2181:2181\" \\ -p \"2281:2281\" \\ -p \"3888:3888\" \\ -p \"2888:2888\" \\ -v \"zk1_data:/data\" \\ -v \"zk1_datalog:/datalog\" \\ -v \"zk1_logs:/logs\" \\ -v \"zk1_secrets:/run/secrets\" \\ -e \"ZOO_SERVERS=server.1=zk1.eia:2888:3888 server.2=zk2.eia:2888:3888 server.3=zk3.eia:2888:3888\" \\ -e \"ZOO_MY_ID=1\" \\ -e \"ZOO_SECURE_CLIENT_PORT=2281\" \\ -e \"ZOO_CLIENT_PORT=2181\" \\ -e \"ZOO_4LW_COMMANDS_WHITELIST=ruok, mntr, conf\" \\ -e \"SERVER_SSL=true\" \\ -e \"SSL_PRIVATE_KEY_FILE=/run/secrets/server.key\" \\ -e \"SSL_CERTIFICATE_FILE=/run/secrets/server.cer\" \\ -e \"SSL_CA_CERTIFICATE_FILE=/run/secrets/CA.cer\" \\ \"zookeeper_redhat:4.3.4\" Note: SERVER_SSL variable is set based on the SOLR_ZOO_SSL_CONNECTION switch, see Environment variables . â€‹ZooKeeper Service Ports Default ports used by ZooKeeper are: 8080 - By default, the server is started on port 8080, and commands are issued by going to the URL \"/commands/[command name]\", e.g., http://localhost:8080/commands/stat . 2181 - The port at which the clients will connect (non-secure). This is defined by setting ZOO_CLIENT_PORT . 2281 - The port at which the clients will connect (secure). This is defined by setting ZOO_SECURE_CLIENT_PORT . 3888 - Port used by ZooKeeper peers to talk to each other. 2888 - Port used by ZooKeeper peers to talk to each other. For more information, see ZooKeeper Service Ports . For an example of the docker run command, see serverFunctions.sh . The runZK function takes the following arguments to support running multiple ZooKeeper containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container. DATA_VOLUME - The name for the data named volume. For more information, see Volumes . DATALOG_VOLUME - The name for the datalog named volume. For more information, see Volumes . LOG_VOLUME - The name for the log named volume. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. ZOO_ID - An identifier for the ZooKeeper server. For more information, see Environment variables . An example of running Zookeeper container using runZK function: runZK zk1 zk1.eia zk1_data zk1_datalog zk1_logs 8080 1 Volumes A named volume or a bind mount can be used to persist data and logs that are generated and used in the ZooKeeper container, outside of the container. For more information, see Where to store data . Named Volumes To configure the ZooKeeper container to use the named volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For ZooKeeper, the directories that must be mounted are /data , /datalog , /logs . For example: -v zk1_data:/data \\ -v zk1_datalog:/datalog \\ -v zk1_log:/logs \\ -v zk1_secrets:/run/secrets A unique volume name must be used for each ZooKeeper container. A bind mount can be used instead of the named volume: For example: -v /var/zk/data:/data \\ -v /var/zk/datalog:/datalog \\ -v /var/zk/logs:/logs \\ A unique bind mount must be used for each ZooKeeper container. Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_PRIVATE_KEY_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure ZooKeeper, you can provide environment variables to the Docker container in the docker run command. The zoo.cfg configuration file for ZooKeeper is generated from the environment variables passed to the container. The following table describes the mandatory environment variables for running ZooKeeper in replicated mode: Environment variable Description ZOO_SERVERS Specified the list of ZooKeeper servers in the ZooKeeper ensemble. Servers are specified in the following format: server.id=<address1>:<port1>:<port2>;<client port> . ZOO_MY_ID An identifier for the ZooKeeper server. The identifier must be unique within the ensemble. ZOO_CLIENT_PORT Specifies the port number for client connections. Maps to the clientPort configuration parameter. ZOO_4LW_COMMANDS_WHITELIST A list of comma separated Four Letter Words commands that user wants to use. A valid Four Letter Words command must be put in this list else ZooKeeper server will not enable the command. By default the whitelist only contains \"srvr\" command which zkServer.sh uses. The rest of four letter word commands are disabled by default. For more information, see ZooKeeper Docker hub . The following table described the security environment variables: Environment variable Description ZOO_SECURE_CLIENT_PORT Specifies the port number for client connections that use SSL. Maps to the secureClientPort configuration parameter. SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . For more information about securing ZooKeeper, see Encryption, Authentication, Authorization Options . The following table describes the environment variables that are supported: Environment variable Description ZOO_TICK_TIME The length of a single tick, which is the basic time unit used by ZooKeeper, as measured in milliseconds. Maps to the tickTime configuration parameter. The default value is 2000 . ZOO_INIT_LIMIT Amount of time, in ticks, to allow followers to connect and sync to a leader. Increase this value as needed, if the amount of data managed by ZooKeeper is large. Maps to the initLimit configuration parameter. The default value is 10 . ZOO_SYNC_LIMIT Amount of time, in ticks, to allow followers to sync with ZooKeeper. If followers fall too far behind a leader, they will be dropped. Maps to the syncLimit configuration parameter. The default value is 5 . ZOO_AUTOPURGE_PURGEINTERVAL The time interval in hours for which the purge task has to be triggered. Set to a positive integer (1 and above) to enable the auto purging. Maps to the autopurge.purgeInterval configuration parameter. The default value is 24 . ZOO_AUTOPURGE_SNAPRETAINCOUNT When auto purge is enabled, ZooKeeper retains the specified number of most recent snapshots and the corresponding transaction logs in the dataDir and dataLogDir respectively and deletes the rest. Maps to the autopurge.snapRetainCount setting. The default value is 3 . ZOO_MAX_CLIENT_CNXNS Limits the number of concurrent connections (at the socket level) that a single client, identified by IP address, may make to a single member of the ZooKeeper ensemble. Maps to the maxClientCnxns configuration parameter. The default value is 60 . ZOO_STANDALONE_ENABLED When set to true , if ZooKeeper is started with a single server the ensemble will not be allowed to grow, and if started with more than one server it will not be allowed to shrink to contain fewer than two participants. Maps to the standaloneEnabled configuration parameter. The default value is true . ZOO_ADMINSERVER_ENABLED Enables the AdminServer. The AdminServer is an embedded Jetty server that provides an HTTP interface to the four letter word commands. Maps to the admin.enableServer configuration parameter. The default value is true . ZOO_DATA_DIR The location where ZooKeeper stores in-memory database snapshots. Maps to the dataDir configuration parameter. The default value is /data . ZOO_DATA_LOG_DIR The location where ZooKeeper writes the transaction log. Maps to the dataLogDir configuration parameter. The default value is /datalog . ZOO_CFG_EXTRA You can add arbitrary configuration parameters, that are not exposed as environment variables in ZooKeeper, to the Zookeeper configuration file using this variable. ZOO_CONF_DIR Specifies the location for the ZooKeeper configuration directory. The default value is /conf . ZOO_LOG_DIR Specifies the location for the ZooKeeper logs directory. The default value is /logs . For more information about configuring ZooKeeper, see: Configuration Parameters ZooKeeper Docker hub . Note: Values that are specified in the environment variables override any configuration that is included in the ZOO_CFG_EXTRA block."
  },
  "versions/2.1.3/content/ingest_config_dev.html": {
    "href": "versions/2.1.3/content/ingest_config_dev.html",
    "title": "Ingesting development data into the Information Store",
    "keywords": "Ingesting development data into the Information Store When you are developing a configuration, ingest a small amount of representative test data into the system to ensure the schema is suitable for your data and you can configure i2 Analyze to meet your requirements. For more information about ingesting data, see Ingesting data into the Information Store . Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to a pattern that includes the Information Store. For example: DEPLOYMENT_PATTERN=\"i2c_istore\" Process overview: Provide a data set Create the ingestion sources Provide and run scripts to complete the ingestion process If you have deployed with the law enforcement schema, complete the steps in Example ingestion process to ingest example data into your Information Store. Data sets The /i2a-data directory is used to contain the data sets that you ingest into the Information Store of a deployment. The data that you ingest into the Information Store must conform to the Information Store schema. However, one data set can be ingested with different configs. There is not a 1-to-1 mapping between data sets and configs. Each data set must contain at least one ingestion script. This script contains the functions that populate the staging tables with your data and calls the ETL toolkit tools that ingest the data. The expected directory structure is as follows: - i2a-data - <data_set> - scripts - <script1> - <script2> Ingesting data into the config dev environment The manageData.sh script is used to manage the ingestion process in the dev environment. To ingest data into the Information Store, you must create scripts that call the ETL tools that complete the actions required by i2 Analyze for you ingest data. For more information, see: ETL tools Ingesting data into the Information Store You can find example scripts in the examples/ingestion/scripts directory. These scripts demonstrate how to create staging tables, populate them, and ingest the data into the Information Store. To run scripts, the manageData.sh script is called as follows: ./manageData.sh -c <config_name> -t ingest -d <data_set> -s <script_name> Where: <config_name> is the name of the config that is currently deployed and running in the config dev environment <data_set> is the name of a directory in i2a-data <script_name> is the name of a script in the directory specified for <data_set> Creating ingestion sources The ingestion sources for a config are contained in the configuration. Ingestion sources are defined in <config_name>/configuration/ingestion/scripts/createIngestionSources.sh . Copy the examples/ingestion/scripts/createIngestionSources.sh file to the <config_name>/configuration/ingestion/scripts/ directory. In the script, the INGESTION_SOURCES array contains the name and description of 2 example sources. INGESTION_SOURCES=( [Example Ingestion Source 1]=EXAMPLE_1 [Example Ingestion Source 2]=EXAMPLE_2 ) You can modify or add to the array of ingestion sources. To create the ingestion sources in the array, the manageData.sh script is called as follows: ./manageData.sh -c <config_name> -t sources Example ingestion process The i2 Analyze minimal toolkit contains the example law-enforcement-data-set-1 data that can be ingested when the example law enforcement schema ( law-enforcement-schema.xml ) is deployed. This contains a number of CSV files that contain the data, and a mapping.xml file. For more information about the mapping file, see Ingestion mapping files . Before you can ingest the law enforcement example data, complete the following steps to provide the data set and scripts: Copy the pre-reqs/i2analyze/toolkit/examples/data/law-enforcement-data-set-1 directory to the i2a-data directory. Copy the examples/ingestion/scripts directory to the i2a-data/law-enforcement-data-set-1 directory. The directory structure is as follows: - i2a-data - law-enforcement-data-set-1 - scripts - ingestLawEnforcementDataSet1.sh - createStagingTables.sh Copy the examples/ingestion/scripts/createIngestionSources.sh file to the <config_name>/configuration/ingestion/scripts/ directory. Use the manageData.sh to create the ingestion sources defined in the example createIngestionSources.sh . For example: ./manageData.sh -c config-development -t sources The example scripts separate the creation of the staging tables from the ingestion of data. To ingest the example data into the config-development config, run the following commands: ./manageData.sh -c config-development -t ingest -d law-enforcement-data-set-1 -s createStagingTables.sh ./manageData.sh -c config-development -t ingest -d law-enforcement-data-set-1 -s ingestLawEnforcementDataSet1.sh The managaData.sh script The scripts/managaData.sh script is used to manage data in an environment. It can be used to run scripts that use the ETL toolkit tools, or to remove all data from the Information Store. The following usage and help is provided for the manageData.sh script: Usage: manageData.sh -c <config_name> -t {ingest} -d <data_set> -s <script_name> [-v] manageData.sh -c <config_name> -t {sources} [-s <script_name>] [-v] manageData.sh -c <config_name> -t {delete} [-v] manageData.sh -h Options: -c <config_name> Name of the config to use. -t {delete|ingest|sources} The task to run. Either delete or ingest data, or add ingestion sources. Delete permanently removes all data from the database. -d <data_set> Name of the data set to ingest. -s <script_name> Name of the ingestion script file. -v Verbose output. -h Display the help. After you add data to your environment, you can configure the rest of the configuration ."
  },
  "versions/2.1.3/content/manage_backup_restore.html": {
    "href": "versions/2.1.3/content/manage_backup_restore.html",
    "title": "Back up and restore a development database",
    "keywords": "Back up and restore a development database When you are developing a configuration that uses an Information Store, you might populate the Information Store with demonstration or test data. You can use the tooling provided to back up the Information Store that is associated with a config. The Information Store is contained in a Docker volume. Because Docker is not designed for permanent data storage, you can back up an Information Store to your local file system. Backup location The backups are stored in the /backups directory. A sub-directory is created for each config name, with another sub-directory for each backup name. The backup file is called ISTORE.bak . For example: - backups - <config_name> - <backup_name> - ISTORE.bak - <backup_name> - ISTORE.bak Creating a backup Use the deploy script to create your backup. The following usage pattern shows how you create a backup: ./deploy.sh -c <config_name> -t backup -b <backup_name> If you do not provide a backup name, the backup is created in a directory called default . For example, to create a backup called test-1 for the config-development config: ./deploy.sh -c config-development -t backup -b test-1 Restoring from backup Use the deploy script to restore from a backup. The following usage pattern shows how you restore from a backup: ./deploy.sh -c <config_name> -t restore -b <backup_name> For example, to restore a backup called test-data-1 for the config-development config: ./deploy.sh -c config-development -t restore -b test-1"
  },
  "versions/2.1.3/content/managing_config_dev.html": {
    "href": "versions/2.1.3/content/managing_config_dev.html",
    "title": "Managing configurations",
    "keywords": "Managing configurations Configs The /configs directory contains all of your configs. The name of the directory for each config is used to identify it when you run the deploy script. You can have as many different configs in the configs directory, however you can only have one deployed and running in the environment at any time. When you run the deploy.sh script and specify the name of a config that is already deployed, the running deployment is updated with any changes you have made to the configuration. When you run the deploy.sh script and specify the name of a config that is not deployed, the containers in the current environment are stopped and a new environment is deployed with the specified config. The deploy.sh script The scripts/deploy.sh script is used to deploy configs and manage your environment. The following usage and help is provided for the deploy.sh script: Usage: deploy.sh -c <config_name> [-t {clean}] [-v] [-y] deploy.sh -c <config_name> [-t {connectors}] [-v] [-y] deploy.sh -c <config_name> [-t {backup|restore} [-b <backup_name>]] [-v] [-y] deploy.sh -c <config_name> -a [-t {connectors}] -d <deployment_name> -l <dependency_label>] [-v] [-y] deploy.sh -h Options: -c <config_name> Name of the config to use. -t {clean} Clean the deployment. Will permanently remove all containers and data. -t {connectors} Generate the secrets and build the images for connectors. By default, it acts on all connectors in the connector-images directory. -t {backup} Backup the database. -t {restore} Restore the database. -b <backup_name> Name of the backup to create or restore. If not specified, the default backup is used. -a Produce or use artefacts on AWS. -d <deployment_name> Name of deployment to use on AWS. -l <dependency_label> Name of dependency image label to use on AWS. -n <connector_name> Name of a connector. This option is used with the connectors task to specify which connectors to act on. -v Verbose output. -y Answer 'yes' to all prompts. -h Display the help."
  },
  "versions/2.1.3/content/managing_connectors.html": {
    "href": "versions/2.1.3/content/managing_connectors.html",
    "title": "Managing connectors",
    "keywords": "Managing connectors The /connector-images directory contains the files that are required to build each connector image in your config dev environment. The complete contents of each connector directory depends on the template that you used to create it. All the templates include a folder that you use to place the connector source code, the files used to build the Docker images and run the Docker containers, and JSON files that you use to provide information about the connector. For instructions about how to add connectors to your environment, see Adding connectors to your development environment . Connector versioning You specify the version and tag of a connector in the connector-version.json file of each connector. version The version value is used as a version for your connector. If you are deploying an i2 Connect server connector, the version value has more requirements. For more information, see i2 Connect server connector versioning . tag The tag value is used in the image name, host name, and secrets for your connector container. For example: { \"version\": \"0.0.1\", \"tag\": \"1-0-0\" } Building connectors Use the deploy.sh script with the connectors task to generate secrets and build the images for all connectors. You can specify connector names to build a subset of the connectors in your environment. For example, to build and run all of the connectors in the connector-images directory: ./deploy.sh -c <config_name> -t connectors If you do not want to build and run all of the connectors, you can use the -i and -e options to include or exclude connectors from the process. For example, to build and run only the connectors named example-connector and example-connector-2 : ./deploy.sh -c <config_name> -t connectors -i example-connector -i example-connector-2 Connector secrets Before you can use a connector, you must generate secrets for it. For more information about the secrets used in the environment, see Managing container security . The secrets that are generated for a particular connector are tied to the tag, if you change the tag you must to regenerate the secrets for that connector. To regenerate the secrets for a connector after you change the tag, complete the following steps: Remove the dev-environment-secrets/<connector_name> directory for the connector that you changed the tag of. Run the deploy.sh script and specify the connector name to regenerate the secrets, rebuild the image, and deploy the environment. For example: ./deploy.sh -c <config_name> -t connectors -i <connector_name> Note: To secure the connection to an external connector, you must install the following certificates where the connector is running. At this release, your connector must be configured to use the certificates signed by the config development environment CA. dev-environment-secrets/generated-secrets/certificates/<connector-name>/server.key dev-environment-secrets/generated-secrets/certificates/<connector-name>/server.cer dev-environment-secrets/generated-secrets/certificates/CA/CA.cer i2 Connect server connectors If you are deploying one or more connectors that were developed using the i2 Connect SDK. There are two versions that you must consider; the version of a connector and the version of the i2 Connect server that the connectors depends on. Connector version In the connector-version.json file, the version specifies the range of supported versions for the connector. This enables you to have more control of the version of a connector that is used in your deployment. You can specify the range of versions at the major, minor, or patch level. For example, if you specify a major version 1 in the connector-version.json file, any connector that has a major version of 1 is compatible. When the connector is developed, the version of the connector is provided in the package.json file by the developer. If the version of the connector specified in package.json does not match the version, or range of versions, that you specify in connector-version.json a warning is displayed when you deploy your environment. The following table demonstrates how you can specify the version ranges and their compatibility with connector versions: package.json version connector-version.json version Compatible? (Y/N) 1.0.0 1 Y 1.3.0 1 Y 1.4.0 1.3 N 1.4.6 1.4 Y 1.4.8 1.4.7 N 2.0.0 1 N i2 Connect server version i2 Connect server connectors depend on the i2 Connect server. When the connector is developed, the version of the i2 Connect server that the connector depends on is provided in the package.json file by the developer. In the connector-images/i2connect-server-version.json file, you specify the range of versions of the i2 Connect server that you want all i2 Connect server connectors to depend on. The versions in these files are specified in the npm semantic versioning syntax. For more information about the syntax, see semver - the semantic versioning for npm . If the version of the i2 Connect server that a connector depends on specified in package.json does not match the version, or range of versions, that you specify in i2connect-server-version.json a warning is displayed when you deploy your environment. The following table demonstrates how you can specify the version ranges and their compatibility with i2 Connect server versions: package.json version connector-version.json version Compatible? (Y/N) ^1.0.0 ^1.0.0 Y 1.3.2 ^1.0.0 Y 1.4.0 1.3.0 N 2.0.0 ^1.0.0 N i2 Connect server connector configuration (Optional) i2 Connect server connectors can have specific connector configuration that is defined in the connector.conf.json and connector.secrets.json files. When the connector is developed, default files can be provided by the developer. connector.conf.json After the first deployment of the connector, if a connector.conf.json file is provided, the system extracts it to the connector-images/<connector-name>/app directory. The file is in the same directory structure as in the supplied connector. For example, <connector-name>/app/dist/connectors/connector.conf.json . To change the configuration, modify the connector.conf.json and redeploy the environment. connector.secrets.json After the first deployment of the connector, if a connector.secrets.json file is provided, the system extracts it to the connector-images/<connector-name>/app directory. The file is in the same directory structure as in the supplied connector. For example, <connector-name>/app/dist/connectors/connector.secrets.json . The file contains configuration of secrets that are required for a connector. For example an API Key or user name and password that is required to access the data source that the connector queries."
  },
  "versions/2.1.3/content/managing_toolkit_configuration.html": {
    "href": "versions/2.1.3/content/managing_toolkit_configuration.html",
    "title": "Synchronize configurations",
    "keywords": "Synchronize configurations You can synchronize configurations between the i2 Analyze deployment toolkit and the configuration development environment. This enables you to develop the i2 Analyze configuration by using the config development environment, and deploy with it as part of the i2 Analyze deployment toolkit. This can be useful in the following situations: You want to create a new deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to develop the configuration for it. You have an existing deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to further develop the configuration. When you synchronize a configuration, you must do so with a complete i2 Analyze deployment toolkit. You can synchronize from an existing deployment toolkit configuration (import) or from an existing configuration development config (export). The configuration from the i2 Analyze deployment toolkit is the source of truth. When you have an existing deployment toolkit, the instructions direct you to back up the configuration before you synchronize it with the config development environment. You complete the synchronization by using the manageToolkitConfiguration.sh script. Examples The following use cases demonstrate the process of synchronizing configurations between the config development environment and an i2 Analyze deployment toolkit: Use case 1 : You want to create a new deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to develop the configuration for it. Use case 2 : You have an existing deployment that uses the i2 Analyze deployment toolkit and you want to convert to use the config development environment to further develop the configuration. After you develop the configuration, you will still deploy i2 Analyze using the deployment toolkit. Use case 1 Use the config development environment to create and develop a config. For more information, see Configuration development environment . When you are happy with the developed configuration, you can complete the process that enables you to use it with the i2 Analyze deployment toolkit. Install the i2 Analyze deployment toolkit. For more information, see Installing i2 Analyze . Export the configuration from the config development environment to the i2 Analyze deployment toolkit. Run the manageToolkitConfiguration.sh script with the create task to create a base configuration in the i2 Analyze deployment toolkit. ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t create Where: -c is the name of the config. -p is the absolute path to the root of the i2 Analyze deployment toolkit that was installed in step 2. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. Note: The base configuration that is created is suitable for the deployment pattern that is specified in the variables.sh file of the specified config. Run the manageToolkitConfiguration.sh script with the export task to export the config dev configuration into the deployment toolkit. ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t export Where: -c is the name of the config to export. -p is the full path to the i2 Analyze deployment toolkit that was installed in step 2. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. You can then deploy your i2 Analyze deployment toolkit configuration. For more information about deploying i2 Analyze, see Deploying i2 Analyze . Start at step 3 from the linked documentation. There are a number of environment specific configuration files that are not populated in the config development environment. Use case 2 This section assumes that you are starting with a configuration from an i2 Analyze deployment toolkit. For more information about deploying i2 Analyze, see Deploying i2 Analyze . The config development environment uses fixed names for schemas and configuration files. Before you can import you deployment toolkit configuration into the config development environment, your configuration must be updated to use the expected file names. The manageToolkitConfiguration.sh script can rename the files for you and update any references to them. You can also decide whether or not to use the config development environment configuration set in your deployment toolkit configuration. If you decide to use the new config set, all settings for the deployment must be specified in the analyze-settings.properties file. If you choose not to use the new configuration set, any additions that you make to the analyze-settings file in the config development environment are not exported. Back up your i2 Analyze deployment toolkit configuration. Fore more information about backing up your configuration, see Back up and restore the configuration . Use the config development environment to create an empty config to import your configuration in to. For more information, see Configuration development environment . You do not need to specify any schema files or deploy the config. Run the manageToolkitConfiguration.sh script with the prepare task to rename the schema and configuration files and update the references: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t prepare Where: -c is the config name you are working with. -p is the absolute path to the root of the i2 Analyze deployment toolkit. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. To use the config development environment configuration set, complete the following 2 steps. Otherwise, move to step 5. Copy the contents of templates/toolkit-config-mod directory to the /toolkit/configuration/fragments/common/WEB-INF/classes directory of the i2 Analyze deployment toolkit. Review the settings that are used in the DiscoClientSettings and DiscoServerSettingsCommon properties files of i2 Analyze deployment toolkit and move any settings that you want to continue using into the /toolkit/configuration/fragments/common/WEB-INF/classes/analyze-settings.properties file. You can remove the DiscoClientSettings and DiscoServerSettingsCommon properties files after you move any settings. Run the manageToolkitConfiguration.sh script with the import task to import the configuration into the config development environment: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t import Where: -c is the config name to import the configuration into. -p is the absolute path to the root of the i2 Analyze deployment toolkit. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. Use the config development environment to develop the configuration. For more information, see Developing the configuration . In the config development environment, a user.registry.xml is required. You can start from the example provided in pre-reqs/i2analyze/toolkit/examples/security and align it to your security schema or copy in an existing one. If you want to use your developed configuration in your i2 Analyze deployment toolkit, you can use the export task to export the configuration into your toolkit. Otherwise, continue to use the config development environment. Run the manageToolkitConfiguration.sh script with the export task. For example: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t export Where: -c is the config name you have created to synchronize the configuration to. -p is the absolute path to the root of the i2 Analyze deployment toolkit. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. You must update the i2 Analyze deployment toolkit deployment with the changes to the configuration. Connectors In the config development environment, i2 Connect connectors are run on Docker images and can only be access from within the Docker network. This means that the connectors and URL references to the connectors cannot be synchronized between configurations. The connector IDs are displayed when you run the import and export tasks to enable you to provide the connectors in each environment. For more information about deploying connectors, see: Adding connectors to your config development environment Adding connectors to your i2 Analyze deployment toolkit configuration Extensions In the config development environment, i2Analyze extensions are built and deployed using Maven. To add an extension to your i2 Analyze deployment toolkit, copy the JAR from i2a-extensions/<extension-name>/target/<extension-name>-<version>.jar and follow the DevEssentials deployment steps. For more information, see Configuring the 'group-based default security dimension values' example project for an example of the steps to take. manageToolkitConfiguration.sh script Usage: manageToolkitConfiguration.sh -c <config_name> -p <toolkit_path> -t { create | prepare | import | export } [-v] Options: -c <config_name> Name of the config to use. -p <toolkit_path> The absolute path to the root of an i2 Analyze deployment toolkit. -t {create} Creates a configuration in the i2 Analyze deployment toolkit that can be imported into the config development environment. -t {prepare} Prepares an existing i2 Analyze deployment toolkit configuration to be imported into the config development environment. -t {export} Export a config development environment configuration to an i2 Analyze deployment toolkit configuration. -t {import} Import an i2 Analyze deployment toolkit configuration to a config development environment configuration. -v Verbose output. -h Display the help."
  },
  "versions/2.1.3/content/managing_update_env.html": {
    "href": "versions/2.1.3/content/managing_update_env.html",
    "title": "Updating to the latest version of the analyze-containers repository",
    "keywords": "Updating to the latest version of the analyze-containers repository To update your config development environment to use the latest version of the analyze-containers repository, update the images, secrets, and deploy your configs. Before you update your environment: Back up any databases that are associated with your configs. For more information, see Back up and restore a development database . Back up the following directories from your existing copy of the repository: backups configs connector-images dev-environment-secrets gateway-schemas i2a-data Download the tar.gz or pull the latest version of the analyze-containers repository from https://github.com/i2group/analyze-containers . If you download the tar.gz , extract the contents and overwrite your existing copy of the analyze-containers repository. Then, copy your backed up directories from step 1.1 over the new version of the repository. Update the command line tools in your environment. The latest version of the analyze-containers repository requires XMLStarlet. For more information about installing XMLStarlet, see Command line tools . To update the images and secrets, run the createDevEnvironment.sh script from the scripts directory. ./createDevEnvironment.sh Before you can use i2 Analyze and the tools, you must read the licence agreement and copyright notices. The licence file is in the pre-reqs/i2analyze/license directory. To accept the licence agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the utils/simulatedExternalVariables.sh script. Before you can use Microsoft SQL Server, you must accept the licence agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the licence in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variables are in the utils/simulatedExternalVariables.sh script. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y To update any deployed configs, run the deploy.sh script from the scripts directory with your config name. You must run the script twice, once with the clean task. For example: ./deploy.sh -c <config_name> -t clean ./deploy.sh -c <config_name>"
  },
  "versions/2.1.3/content/managing_upgrade_env.html": {
    "href": "versions/2.1.3/content/managing_upgrade_env.html",
    "title": "Upgrading",
    "keywords": "Upgrading Upgrade your config development environment to use the latest version of the analyze-containers repository, or i2 Analyze update the images, secrets, and deploy your configs. The process of upgrading the config development environment is completed in a different directory from your current environment. Download the tar.gz for the latest version of the analyze-containers repository from https://github.com/i2group/analyze-containers . Extract the contents into a new directory. For example, analyze-containers-213 . Copy the analyze-containers/pre-reqs to the analyze-containers-213/pre-reqs directory. To set the ANALYZE_CONTAINERS_ROOT_DIR variable, run the following command: . initShell.sh To create the configuration development environment and template config, in the analyze-containers-213/scripts directory run: ./createDevEnvironment.sh The script performs the following actions: Extracts the required files from the i2 Analyze deployment toolkit Builds the required Docker images for the development environment Generates the secrets that are used in the environment Creates the configuration template The configuration template is located in /templates/config-development. Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the analyze-containers-213/pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the analyze-containers-213/utils/simulatedExternalVariables.sh script. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux . To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variables are in the utils/simulatedExternalVariables.sh script. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y After you upgrade, you will have 2 directories for each version of the analyze-containers project. The manageEnvironment script can delete the analyze-containers directory and rename your analyze-containers-213 directory for you. To upgrade all of the configs in your environment to use the latest version, run the following command from the analyze-containers-213/scripts directory: ./manageEnvironment.sh -t upgrade -p <path-to-previous-project> Where <path-to-previous-project> is the absolute path to the root of your previous analyze-containers repository. This script completes the following actions: Creates a backup of the ISTORE database for each config that contains one Copies the following directories from the previous project to the current project: backups configs connector-images dev-environment-secrets gateway-schemas i2a-data Upgrades the configs to the latest version of analyze-containers If you no longer need to previous version, answer yes to the prompt from the manageEnvironment.sh script."
  },
  "versions/2.1.3/content/reference architecture/deploy_aws.html": {
    "href": "versions/2.1.3/content/reference architecture/deploy_aws.html",
    "title": "Deploying the AWS reference architecture",
    "keywords": "Deploying the AWS reference architecture When you follow the instructions to deploy the AWS reference architecture, the scripts launch, configure, and run the AWS compute, network, storage, and other services for the i2 Analyze on AWS. The reference architecture includes AWS CloudFormation templates that automate the deployment and a deployment guide that describes the architecture. The deployment is running on AWS and you are charged by AWS for the infrastructure that is used. Consult the AWS pricing information before you run the scripts: ... By default, the AWS reference architecture is deployed on the following instances. For information about modifying these values before you deploy the reference architecture, see XXX. (TODO: populate resources and identify steps to modify the reference architecture). Prerequisites Before you can deploy the AWS reference architecture, you need to have installed and configured the following pre-requisites: Have an AWS account Install the AWS CLI v2 For more, see Installing, updating, and uninstalling the AWS CLI version 2 Configure the analyze-containers repository For more information, see Getting started with the analyze-containers repository . In your terminal, navigate to the examples/aws directory. Accepting the licences Before you can use i2 Analyze and the tools, you must read the licence agreement and copyright notices. The licence file is in the pre-reqs/i2analyze/license directory. To accept the licence agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the examples/aws/utils/simulated-external-variables.sh script. For example: LIC_AGREEMENT=ACCEPT Before you can use Microsoft SQL Server, you must accept the licence agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the licence in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variables are in the utils/simulatedExternalVariables.sh script. For example: MSSQL_PID=Developer ACCEPT_EULA=Y Setting up AWS Variables Before you can deploy the AWS reference architecture, you must provide your AWS account information in the examples/aws/utils/simulated-external-variables.sh script. To set up the system to communicate with your AWS account, set the value of the ECR_BASE_NAME environment variable to your default private registry URL. For more information, see Amazon ECR private registries . Set the value of the AWS_REGION environment variable to the region name you want to deploy the architecture in. For example: ECR_BASE_NAME=\"#####.dkr.ecr.eu-west-2.amazonaws.com\" AWS_REGION=\"eu-west-2\" Creating and uploading resources to AWS The create-and-upload-aws-resources.sh script creates the configuration, database scripts, secrets, images, and runbooks then uploads them to AWS. For more information about what is created, see create-and-upload-aws-resources.sh . Run create-and-upload-aws-resources.sh : ./create-and-upload-aws-resources.sh Deploying the i2 Analyze on AWS The deploy-aws.sh script deploys i2 Analyze on AWS. For more information about what is deployed, see deploy-aws.sh . Run deploy-aws.sh : ./deploy-aws.sh"
  },
  "versions/2.1.3/content/reference architecture/deploy_pre_prod.html": {
    "href": "versions/2.1.3/content/reference architecture/deploy_pre_prod.html",
    "title": "Pre-production example environment",
    "keywords": "Pre-production example environment Prerequisites Before you create an example pre-production environment, you must configure the analyze-containers repository. For more information, see Getting started with the analyze-containers repository . Creating a containerised deployment After you have all of the prerequisites in place, use the example scripts and artifacts in the examples/pre-prod directory to create the reference pre-production containerised deployment. Creating the environment and configuration The createEnvironment.sh script performs a number of actions that ensure all of the artifacts for a deployment are created and in the correct locations. These actions include: Extracting the i2 Analyze minimal toolkit to the pre-reqs/i2analyze directory. Creating the i2 Analyze Liberty application Creating and populating the configuration directory structure For more information about the what the script does, see: Create environment To create the environment and configuration, run the following commands: ./createPreProdEnvironment.sh The configuration directory is created in the examples/pre-prod directory. By default, the environment is created for an Information Store and i2 Connect gateway deployment. Accepting the licences Before you can use i2 Analyze and the tools, you must read and accept the licence agreement and copyright notices in the pre-reqs/i2analyze/license directory. To accept the licence agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the examples/pre-prod/utils/simulatedExternalVariables.sh script. Before you can use Microsoft SQL Server, you must accept the licence agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the licence in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variable is in the utils/simulatedExternalVariables.sh script. Running the containers and start i2 Analyze To deploy and start i2 Analyze, run the following command: ./deploy.sh For more information about the actions that are completed, see Deploying i2 Analyze . Modifying the hosts file To enable you to connect to the deployment and the Solr Web UI, update your hosts file to include the following lines: 127.0.0.1 solr1.eia 127.0.0.1 i2analyze.eia NOTE: On Windows, you must edit your hosts file in C:\\Windows\\System32\\drivers\\etc and the hosts file in /etc/hosts for WSL. Installing Certificate To access the system, the server that you are connecting from must trust the certificate that it receives from the deployment. To enable trust, install the /dev-environment-secrets/generated-secrets/certificates/externalCA/CA.cer certificate as a trusted root certificate authority in your browser and operating system's certificate store. For information about installing the certificate, see: Install Certificates with the Microsoft Management Console Setting up certificate authorities in Firefox Set up an HTTPS certificate authority in Chrome Accessing the system To connect to the deployment, the URL to use is: https://i2analyze.eia:9046/opal/ What to do next To understand how the environment is created, you can review the documentation that explains the images, containers, tools, and functions: Images and containers Tools and functions To learn how to configure and administer i2 Analyze in a containerised environment, you can complete the walkthroughs that are included in the repository: Walkthroughs"
  },
  "versions/2.1.3/content/reference architecture/reference_architectures.html": {
    "href": "versions/2.1.3/content/reference architecture/reference_architectures.html",
    "title": "Containerised deployment reference architectures",
    "keywords": "Containerised deployment reference architectures Containerised deployment reference"
  },
  "versions/2.1.3/content/reference architecture/understanding.html": {
    "href": "versions/2.1.3/content/reference architecture/understanding.html",
    "title": "Understanding the reference architecture",
    "keywords": "Understanding the reference architecture The Analyze-Containers repository includes Dockerfiles and example scripts that provide a reference architecture for creating a containerised deployment of i2 Analyze. The scripts demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. How to use the reference architecture? The repository is designed to be used with the i2 Analyze minimal toolkit. The minimal toolkit is similar to the standard i2 Analyze deployment toolkit, except that it only includes the minimum amount of application and configuration files. The i2 Analyze minimal toolkit is used to provide the artefacts that are required to build the images and provide the configuration for the deployment. Bash scripts are then used to build the images and run the containers. To demonstrate creating an example containerized deployment, complete the actions described in Pre-production example environment . The minimal toolkit also contains the tools that are used by the bash scripts to deploy, configure, and administer your deployment of i2 Analyze. The tools are in the form of JAR files that are called from shell scripts. For more information about the tools that are available and their usage, see i2 Analyze tools . Dockerfiles and images A deployment of i2 Analyze consists of the following components: Liberty Solr ZooKeeper Optionally a database management system The Analyze-Containers repository contains the Dockerfiles that are used to build the images for each component. For Liberty and SQL Server, the image provided by Liberty and SQL Server is used. For Solr and ZooKeeper, the repository contains custom Dockerfiles that were created from the ones provided by Solr and ZooKeeper. For more information about the images and containers, see images and containers . Scripts The Analyze-Containers repository provides example scripts that you can use and leverage for your own deployment use cases. The Analyze-Containers repository contains a number of scripts that are designed to be used at various stages when working towards creating a containerised deployment. The repository also includes example artifacts that are used with the scripts. These artifacts include an example certificate authority and certificates, secrets and keys to be used with i2 Analyze, and utilities that are used by the example scripts. Walkthroughs A number of walkthroughs are provided that demonstrate how to complete configuration and administration tasks in a containerized deployment. The walkthroughs consist of a reference script that demonstrates how to complete the action, and a document that explain the process in more detail. What is deployed? When you run the provided scripts to create the example deployment, i2 Analyze is deployed in the following topology: The deployment includes: A load balancer container, using HAProxy Two Liberty containers configured for high availability A Solr cluster with two Solr containers A ZooKeeper ensemble with three ZooKeeper containers A SQL Server container A number of \"client\" ephemeral containers are used to complete a single actions. The following client containers are used: SQL Server client Solr client For more information about the images and containers, see images and containers ."
  },
  "versions/2.1.3/content/reference architecture/understanding_aws.html": {
    "href": "versions/2.1.3/content/reference architecture/understanding_aws.html",
    "title": "Understanding the AWS reference architecture",
    "keywords": "Understanding the AWS reference architecture The analyze-containers repository includes yaml files and example scripts that provide a reference architecture for creating an AWS deployment of i2 Analyze. The scripts demonstrate how to upload resources, create CloudFormation Stacks and enable you to deploy, configure, and run i2 Analyze on AWS. How to use the reference architecture? You should already have an understanding of the Containerised deployment reference before starting with the AWS reference architecture. This architecture uses the same Dockerfiles and images used in the containerised deployment reference which the exception of the database management system (server) and the load balancer. To create the example AWS deployment, complete the actions described in AWS example environment . Scripts The analyze-containers repository provides example scripts that you can use and leverage for your own deployment use cases. The analyze-containers repository contains a number of scripts that are designed to be used at various stages when working towards creating an AWS deployment. The repository also includes example artifacts that are used with the scripts. These artifacts include an example certificate authority and certificates, secrets and keys to be used with i2 Analyze, CloudFormation templates, runbooks, and utilities. Walkthroughs A number of walkthroughs are provided that demonstrate how to complete configuration and administration tasks in an AWS deployment. The walkthroughs consist of a reference script that demonstrates how to complete the action, and a document that explain the process in more detail. What is deployed? When you run the provided scripts to create the example deployment, i2 Analyze is deployed in the following topology: TODO! The deployment includes: A load balancer (AWS ELB) Liberty containers running in ECS as Fargate A Solr cluster with two Solr containers deployed in EC2 instances A ZooKeeper ensemble with three ZooKeeper containers deployed in EC2 instances SQL Server running in EC2 with EBS A number of \"client\" ephemeral containers are used to complete a single actions. The following client containers are used: SQL Server client Solr client #TODO: What about i2a-tools? we don't mention it in the understanding.md either For more information about the artifacts that are used, see: Stacks Runbooks AWS Tools"
  },
  "versions/2.1.3/content/runbooks/helper_runbooks.html": {
    "href": "versions/2.1.3/content/runbooks/helper_runbooks.html",
    "title": "Helper runbooks",
    "keywords": "Helper runbooks i2a-UpdateScripts Description This runbook is an automation pulling down i2a-scripts on all required EC2 instances Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group updateScripts Pull down and untar i2a-scripts.tar.gz N/A update-tools/update"
  },
  "versions/2.1.3/content/runbooks/runbooks.html": {
    "href": "versions/2.1.3/content/runbooks/runbooks.html",
    "title": "Runbooks",
    "keywords": "Runbooks The documentation in this section describes the runbooks that are used to deploy i2 Analyze in an AWS environment."
  },
  "versions/2.1.3/content/runbooks/solr_runbooks.html": {
    "href": "versions/2.1.3/content/runbooks/solr_runbooks.html",
    "title": "Solr runbooks",
    "keywords": "Solr runbooks The following runbooks are used to deploy, start, and stop Solr and ZooKeeper. i2a-SolrFirstRun Description Initialize, configure, and start Solr and ZooKeeper Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group runZookeeper1 Start ZooKeeper container on the zk1 instance run-zk.sh solr-first-run/run-zk runZookeeper2 Start ZooKeeper container on the zk2 instance run-zk.sh solr-first-run/run-zk runZookeeper3 Start ZooKeeper container on the zk3 instance run-zk.sh solr-first-run/run-zk configureZkForSolr Create is_cluster and upload solr/security.json configure-zk-for-solr.sh solr-first-run/configure-zk-for-solr runSolr Start Solr containers on all available solr instances run-solr.sh solr-first-run/run-solr configureSolr Create and configure Solr collections configure-solr.sh solr-first-run/configure-solr i2a-SolrStart Description Start previously created ZooKeeper and Solr containers Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group runZookeeper1 Start ZooKeeper container on the zk1 instance run-zk.sh solr-start/run-zk runZookeeper2 Start ZooKeeper container on the zk2 instance run-zk.sh solr-start/run-zk runZookeeper3 Start ZooKeeper container on the zk3 instance run-zk.sh solr-start/run-zk RUN_SOLR Start Solr containers on all available solr instances run-solr.sh solr-start/run-solr"
  },
  "versions/2.1.3/content/runbooks/sqlserver_runbooks.html": {
    "href": "versions/2.1.3/content/runbooks/sqlserver_runbooks.html",
    "title": "SQL Server runbooks",
    "keywords": "SQL Server runbooks i2a-SqlServerFirstRun Description This runbook is an automation for initializing and configuring the Information Store in SQL Server Parameters Name Description DeploymentName The name of the deployment"
  },
  "versions/2.1.3/content/security and users/aws_security.html": {
    "href": "versions/2.1.3/content/security and users/aws_security.html",
    "title": "",
    "keywords": ""
  },
  "versions/2.1.3/content/security and users/db_users.html": {
    "href": "versions/2.1.3/content/security and users/db_users.html",
    "title": "Database roles, users, and logins",
    "keywords": "Database roles, users, and logins During the deployment, administration, and use of i2 Analyze, a number of different actions are completed against the Information Store database. These actions can be separated into different categories that are usually completed by users with differing permissions. In SQL Server, you can create a number of database roles and assign users to roles. In the example deployment, a number of different roles and users are used to demonstrate the types of roles that might complete each action. Database roles In the example, the following roles are used: Role Description DBA_Role The DBA_Role is used to perform database administrative tasks. External_ETL_Role The External_ETL_Role is used to move data from an external system into the staging tables in the Information Store database. i2_ETL_Role The i2_ETL_Role is used to read data from the staging tables and ingest it into - and delete it from - the Information Store database. i2analyze_Role The i2analyze_Role is used to complete actions required by the Liberty application. For example, returning results for Visual Queries. Database role permissions Each database role requires a specific set of permissions to complete the actions attributed to them. DBA_Role The DBA_Role requires permissions to: Set up and maintain the database management system and Information Store database. Create and modify the database management system objects. For example, bufferpools, tablespaces, and filegroups. Create and modify database objects. For example, tables, views, indexes, sequences. Troubleshoot performance or other issues. For example, has all privileges on all tables. This can be restricted in some environments. Configure high availability. Manage backup and recovery activities. Additionally, the role requires access to two roles in the msdb database: SQLAgentUserRole - for more information, see SQLAgentUserRole Permissions db_datareader - for more information, see Fixed-Database Roles These roles are required for database creation, to initialize the deletion-by-rule objects, and to create the SQL Server Agent jobs for the deletion rules. Note: The configureDbaRolesAndPermissions.sh script is run during deploy to grant the correct permissions. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes All CREATE TABLE, CREATE VIEW, CREATE SYNONYM Required to create the database objects. All ALTER, SELECT, UPDATE, INSERT, DELETE, REFERENCES Required to make changes for maintaining the database. IS_Core EXECUTE Required for deletion-by-rule and database configuration. IS_Public EXECUTE Required to run the stored procedures for deletion-by-rule. The following table provides an overview of the permissions required on the schemas in the msdb database: Schema Permissions Notes dbo SQLAgentUserRole Required to create the deletion jobs during deployment, and to manage the deletion job schedule. dbo db_datareader Required to create the deletion job schedule. The following table provides an overview of the permissions required on the schemas in the master database: Schema Permissions Notes All VIEW SERVER STATE Required for deletion-by-rule automated jobs via the SQL Server Agent. sys EXECUTE ON fn_hadr_is_primary_replica Required for deletion-by-rule automated jobs. The configureDbaRolesAndPermissions.sh script is used to configure the DBA user with all the required role memberships and permissions. External_ETL_Role The External_ETL_Role requires permissions to move data from external systems into the Information Store staging tables. For example, it can be used by an ETL tool - such as DataStage or Informatica - to move and transform data that results in populated staging tables in the Information Store staging schema. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging SELECT, UPDATE, INSERT, DELETE Required to populate the staging tables with date to be ingested or deleted. In addition to these permissions, in an environment running SQL server in a Linux container, users with this role must also be a member of the sysadmin group in order to perform BULK INSERT into the external staging tables. The addEtlUserToSysAdminRole.sh script is used to make the etl user a member of the sysadmin fixed-server role. i2_ETL_Role The i2_ETL_Role requires permissions to use the i2 Analyze ingestion tools to ingest data from the staging tables into the Information Store. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging ALTER, SELECT, UPDATE, INSERT, DELETE Required by the ingestion tools to create and modify objects during the ingestion process. IS_Stg ALTER, SELECT, UPDATE, INSERT, DELETE Required by the ingestion tools to create and modify objects during the ingestion process. IS_Meta SELECT, UPDATE, INSERT UPDATE and INSERT are required to update the ingestion history table. SELECT is required to read the schema meta data. IS_Data ALTER, SELECT, UPDATE, INSERT, DELETE ALTER is required to drop and create indexes and update statistics as part of the ingestion process. IS_Public ALTER, SELECT ALTER is required to delete and create synonyms when enabling merged property views. IS_Core SELECT, EXECUTE Required to check configuration of the database. i2analyze_Role The i2analyze_Role requires permissions to complete actions required by the Liberty application. These actions include: Visual Query, Find Path, Expand, Add to chart, Upload, and Online upgrade. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging ALTER, SELECT, UPDATE, INSERT, DELETE Required to run deletion-by-rule jobs. IS_Stg ALTER, SELECT, UPDATE, INSERT, DELETE Required to upload and delete records via Analyst's Notebook Premium and to run deletion-by-rule jobs. IS_Meta SELECT, UPDATE, INSERT, DELETE DELETE is required to process the ingestion history queue. IS_Data SELECT, UPDATE, INSERT, DELETE UPDATE, INSERT, and DELETE are required by deletion-by-rule and to upload and delete records via Analyst's Notebook Premium. IS_Core SELECT, UPDATE, INSERT, DELETE Required for online upgrade. IS_VQ SELECT, UPDATE, INSERT, DELETE Required to complete Visual Queries. IS_FP SELECT, INSERT, EXEC Required to complete Find Path operations. IS_WC SELECT, UPDATE, INSERT, DELETE Required to work with Web Charts. The database backup operator role The example also demonstrates how to perform a database backup. The dbb user will perform this action and is a member of the SQL Server built-in role, db_backupoperator . This gives this user the correct permissions for performing a backup and nothing else. For more information, see Fixed-Database Roles for more details. Database users and logins In the example, a user is created for each role described previously. These users are then used throughout the deployment and administration steps to provide a reference for when each role is required. The following users and logins are used in the example: User and login Description Secrets sa The system administrator user. The sa user has full permissions on the database instance. This user creates the Information Store database, roles, users, and logins. The password is in the SA_PASSWORD file, and the username is in the SA_USERNAME file in the secrets/sqlserver directory. i2analyze The i2analyze user is a member of the i2analyze_Role . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/liberty directory. etl The etl user is a member of External_ETL_Role . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/etl directory. i2etl The i2etl user is a member of i2_ETL_Role . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/i2etl directory. dba The dba user is a member of DBA_Role . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/dba directory. dbb The dbb user is the database backup user, it is a member of the SQL Server built in role: db_backupoperator . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/dbb directory. The sa user and login exists on the base SQL Server image. The sa user is used to create the following artefacts: Database: ISTORE Roles: i2analyze_Role , External_ETL_Role, , i2_ETL_Role and DBA_Role Logins: i2analyze , etl , i2etl , dba , and dbb Users: i2analyze , etl , i2etl , dba , and dbb The roles and users must be created after the Information Store database is created. Creating the roles The sa user is used to run the createDbRoles.sh client function that creates the i2Analyze_Role , External_ETL_Role , i2_ETL_Role , and DBA_Role roles. To create the roles, the createDbRoles.sh script is run using the runSQLServerCommandAsSA client function. This function uses an ephemeral SQL Server client container to create the database roles. For more information about the client function, see: runSQLServerCommandAsSA createDbRoles.sh All the secrets required at runtime by the client container are made available by providing a file path to the secret which is converted to an environment variable by the docker container. For example, to provide the SA_USERNAME environment variable to the client container, a file containing the secret is declared in the docker run command: -e \"SA_USERNAME_FILE=${CONTAINER_SECRETS_DIR}/SA_USERNAME_FILE\" The file name can be anything, but the environment variable is fixed. For more information see, managing container security In the example, the createDbRoles.sh script is called in deploy.sh . Create the login and user Use the sa user to create the login and the user on the ISTORE , and make the user a member of the role. You can use an ephemeral SQL Client container to create the login and the user. The createDbLoginAndUser.sh script in /images/sql_client/db-scripts is used to create the login and user. The scripts are called from the deploy.sh scripts. The createDbLoginAndUser function The createDbLoginAndUser function uses an ephemeral SQL Client container to create the database administrator login and user. The login and user are created by the sa user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The createDbLoginAndUser.sh script is used to create the login and user. The function requires the following environment variables to run: Environment variable Description SA_USERNAME The sa username. SA_PASSWORD The sa user password. DB_USERNAME The database user name. DB_PASSWORD The database user password. DB_SSL_CONNECTION Whether to use SSL for connection. SSL_CA_CERTIFICATE The path to the CA certificate. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_ROLE The name of the role that user will be added to. It has to be one of the roles from this list . Changing SA password In a Docker environment, you must start the SQL Server as the existing sa user before you can modify the password. The changeSAPassword function The changeSAPassword function uses an ephemeral SQL Client to change the sa user password. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The changeSAPassword.sh script is used to change the password. The function requires the following environment variables to run: Environment variable Description SA_USERNAME The sa username SA_OLD_PASSWORD The current sa password. SA_NEW_PASSWORD The new sa password. DB_SSL_CONNECTION Whether to use SSL for connection. SSL_CA_CERTIFICATE The path to the CA certificate. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store."
  },
  "versions/2.1.3/content/security and users/security.html": {
    "href": "versions/2.1.3/content/security and users/security.html",
    "title": "Managing container security",
    "keywords": "Managing container security SSL certificates in the deployment The example deployment is configured to use SSL connections for communication between clients and i2 Analyze, and between the components of i2 Analyze. To achieve this, the appropriate certificate authorities and certificates are used. The generateSecrets.sh script is used to simulate the process of creating the required keys and acquiring certificate authority-signed certificates. Note: The keys and certificates used are set to expire after 90 days. To use the certificates for longer than this, you must run the generateSecrets.sh script again. Certificate Authorities (CA) In the example, two CAs are used to provide trust: Internal CA The internal CA is to provide trust for the containers that are used for the components of i2 Analyze. Each container's certificates are signed by the internal CA. External CA The external CA is to provide trust for external requests to the i2 Analyze service via the load balancer. In our example, the external certificate authority is generated for you. However, in production a real certificate should be used. Container certificates To communicate securely using TLS each container requires the following certificates: Private key Certificate key Internal certificate authority The containers will generate truststores and keystores based on the keys provided to the container. For more information about how the keys are passed to the containers securely please see Secure Environment variables . Secure communication between containers When the components communicate, the CA certificate is used to establish trust of the container certificate that is received. Each container has its own private key ZooKeeper requires client authentication to initiate communication. The i2 Analyze, i2 Analyze Tool, and Solr client containers require container certificates to authenticate with ZooKeeper. Creating keys and certificates The following diagram shows a simplified sequence of creating a container certificate from the certificate authority and using it to establish trust: The certificate authority's certificate is distributed to the client. The private key is generated on the server. In the generateSecrets.sh script, the key is created by: openssl genrsa -out server.key 4096 The public part of the private key is used in a Certificate Signing Request (CSR). In the generateSecrets.sh script, this is completed by: openssl req -new -key server.key -subj \"/CN=solr1.eia\" -out key.csr The common name that is used for the certificate is the server's fully qualified domain name. The CSR is sent to the certificate authority (CA). The CA signs and returns a signed certificate for the server. In the generateSecrets.sh script, the CA signing the certificate is completed by: openssl x509 -req -sha256 -CA CA.cer -CAkey CA.key -days 90 -CAcreateserial -CAserial CA.srl -extfile x509.ext -extensions \"solr\" -in key.csr -out server.cer When communication is established, the container certificate is sent to the client. The client uses it's copy of the CA certificate to verify that the container certificate was signed by the same CA. Password generation The example simulates secrets management performed by various secrets managers provided by cloud vendors. The generateSecrets.sh generates these secrets and populates the dev-environment-secrets/simulated-secret-store with secrets required for each container. The docker desktop does not support secrets, but the example environment example simulates this by mounting the secrets folder. For more information see Manage sensitive data with Docker secrets . After these passwords have been generated, they can be uploaded to a secrets manager. Alternatively you can use a secrets manager to generate your passwords. Solr Basic Authentication Solr authorisation can be enabled using the BasicAuthPlugin. The basic auth plugin defines users, user roles and passwords for users. For the BasicAuthPlugin to be enabled, solr requires a security.json file to be uploaded. In our example the security.json file is created by the generateSecrets.sh and located in dev-environment-secrets/generated-secrets/secrets/solr/security.json . For more information about solr authentication see Basic Authentication Plugin Secure Environment variables In general secrets used by a particular container can be supplied via an environment variable containing the path to a file containing the secret, or an environment variable specifying the literal secret value, for example: Note: Secrets can be passwords, keys or certificates. docker run --name solr1 -d --net eia --secret source=SOLR_SSL_KEY_STORE_PASSWORD,target=SOLR_SSL_KEY_STORE_PASSWORD \\ -e SOLR_SSL_KEY_STORE_PASSWORD_FILE=\"/run/secrets/SOLR_SSL_KEY_STORE_PASSWORD_FILE\" or docker run --name solr1 -d --net eia -e SOLR_SSL_KEY_STORE_PASSWORD=\"jhga98u43jndfj\" The docker files in the analyze-containers repository have been modified to accept either. The convention is that the environment variable must match the property being set with \"_file\" appended if the secret is in a file, and without if a literal value is being used instead. In the example scripts, each container gets the relevant key stores mounted along with the correct secrets files in a secrets directory. NOTE: By default this is set as CONTAINER_SECRETS_DIR=\"/run/secrets\" in the common variables file. All the containers use the same environment variables to define the location of certificates. These are then used to generate appropriate artifacts for the particular container. There is also a standard way of turning on and off server SSL. Security switching variables Environment variable Description SERVER_SSL Can be set to true or false . If set to true , the container is configured to use encrypted connections. LIBERTY_SSL_CONNECTION Can be set to true or false . If set to true , connections to the Liberty container use an encrypted connection. DB_SSL_CONNECTION Can be set to true or false . If set to true , connections to the database use an encrypted connection SOLR_ZOO_SSL_CONNECTION Can be set to true or false . If set to true , connections to ZooKeeper and Solr use an encrypted connection. GATEWAY_SSL_CONNECTION Can be set to true of false . If set to true , connections to i2 Connect connectors use an encrypted connection. SSL_ENABLED Can be set to true or false . If set to true , the connector container communicates with the i2 Connect gateway using an encrypted connection. Security environment variables Environment variable Description SSL_PRIVATE_KEY The private key for the container certificate. SSL_CERTIFICATE The container certificate. SSL_CA_CERTIFICATE The Certificate Authority certificate. SSL_OUTBOUND_PRIVATE_KEY The private key for the Liberty container, which is used for outbound connections. SSL_OUTBOUND_CERTIFICATE The private certificate for the Liberty container, which is used for outbound connections. SSL_OUTBOUND_CA_CERTIFICATE_FILE The certificate authority used for verifying outbound connections."
  },
  "versions/2.1.3/content/security and users/security_users.html": {
    "href": "versions/2.1.3/content/security and users/security_users.html",
    "title": "Security and users",
    "keywords": "Security and users The documentation in this section describes how security and users are configured for a deployment of i2 Analyze in a containerised environment."
  },
  "versions/2.1.3/content/stacks/solr_stack.html": {
    "href": "versions/2.1.3/content/stacks/solr_stack.html",
    "title": "The Solr stack",
    "keywords": "The Solr stack Solr CloudFormation template in the i2a-stack-solr.yaml file creates: 3 ZooKeeper instances Solr Launch template 2 Solr instances SSM parameters for ZooKeeper and Solr ZooKeeper stack The ZooKeeper stack is based on the i2a-stack-docker-ec2.yaml CloudFormation template and uses a launch template to complete the following actions: Downloads i2a-scripts from the S3 resources bucket Installs Docker Downloads the zk image from AWS ECR Sets up CloudWatch ZooKeeper stack tags Tags: - Key: \"instance-name\" Value: \"zk1\" - Key: \"stack-name\" Value: !Ref DeploymentName ZooKeeper stack security groups Each ZooKeeper instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) ZkSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId Solr stack Solr Launch Template This Launch templates specifies the Solr instance configuration information, such as security groups and the commands required to initialize each Solr instance. EBS Volume Each Solr instance has a dedicated EBS Volume. The EBS Volume is in the /dev/sdh directory. (TODO: Add more info what is this for) EFS Volume Each Solr instance has dedicated EFS Storage to store data. EFS Storage is in the /var/solr directory. (TODO: Add more info what is this for) Solr stack security groups Each Solr instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) SolrSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId EFSSecurityGroupId Solr stack tags Tags: - Key: \"Name\" Value: !Sub \"${DeploymentName}-Solr-Instance\" - Key: \"stack-name\" Value: !Ref DeploymentName - Key: \"instance-name\" Value: \"solr\" Parameters DeploymentName: Type: String Description: Deployment Name PrivateSubnetAId: Type: AWS::EC2::Subnet::Id Description: Private subnet id to use for the instance IntraSubnetEncryption: Type: String Description: When set to true, uses an encrypted connection AllowedValues: - \"false\" - \"true\" Default: \"false\" ResourcesS3Bucket: Type: String Description: The AWS S3 bucket for CloudFormation templates ZkInstanceType: Type: String Description: EC2 instance type to use for ZooKeeper AllowedValues: - t3a.medium - t3a.large - t3.medium - t3.large Default: \"t3a.medium\" ConstraintDescription: Must be a valid EC2 instance type SolrInstanceType: Type: String Description: EC2 instance type to use for Solr AllowedValues: - r5.large - r5.xlarge - r5.2xlarge - r5b.large - r5b.xlarge - r5b.2xlarge - r5d.large - r5d.xlarge - r5d.2xlarge Default: \"r5.large\" ConstraintDescription: Must be a valid EC2 instance type SolrEBSVolumeSize: Type: Number Description: Volume size to use for Solr, in GiBs Default: 8 SolrEBSVolumeType: Type: String Description: Volume type to use for Solr AllowedValues: - gp3 - io1 - io2 Default: 'gp3' SolrEBSVolumeIops: Type: Number Description: > The number of I/O operations per second provisioned for the volume to use for Solr The following are the supported values for each volume type: gp3: 3,000-16,000 IOPS io1: 100-64,000 IOPS io2: 100-64,000 IOPS' Default: 3000 LatestAmiId: Type: \"AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>\" Description: This gets the latest AMI image ID. Do NOT change Default: \"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\" IamInstanceProfile: Type: String Description: The name of the shared instance template stack Default: \"SSMInstanceProfile\" (TODO: do we need to add info about all ssm params being created as a part of this stack?)"
  },
  "versions/2.1.3/content/stacks/sqlserver_stack.html": {
    "href": "versions/2.1.3/content/stacks/sqlserver_stack.html",
    "title": "The SQL Server stack",
    "keywords": "The SQL Server stack SQL Server CloudFormation template in the i2a-sqlserver.yaml file creates: SQL Server instance SSM parameters for SQL Server SQL Server stack The SQL Server stack uses the latest ami image and uses a launch template to complete the following actions: Installs jQuery Installs Amazon EFS client Creates a mount directory Mounts EFS file system Sets up SQL Server SQL Server stack tags Tags: - Key: Name Value: !Join [\"\", [!Ref \"AWS::StackName\", -SQLServer2019-Instance]] - Key: \"instance-name\" Value: \"sqlserver\" - Key: \"stack-name\" Value: !Ref DeploymentName SQL Server stack security groups The SQL Server instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) SQLServerSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId EFSSecurityGroupId EBS Volume The SQL Server instance has a dedicated EBS Volume. The EBS Volume is in the /dev/sdh directory. (TODO: Add more info what is this for) Parameters Parameters: DeploymentName: Description: Deployment Name Type: String PrivateSubnetAId: Description: Private subnet id to use for the instance Type: AWS::EC2::Subnet::Id InstanceType: Description: EC2 instance type to use for SQL Server Type: String AllowedValues: - m5.xlarge - m5.2xlarge - m5.4xlarge - m5.8xlarge - m5.12xlarge - m5.16xlarge - m5.24xlarge - r5.xlarge - r5.2xlarge - r5.4xlarge - r5.8xlarge - r5.12xlarge - r5.16xlarge - r5.24xlarge - i3.xlarge - i3.2xlarge - i3.4xlarge - i3.8xlarge - i3.16xlarge Default: m5.xlarge # Vendor recommended ConstraintDescription: must be a valid EC2 instance type supported by SQL Server. LatestAmiId: Description: This gets the latest SQL Server AMI image ID. Do NOT change Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id> Default: /aws/service/ami-windows-latest/amzn2-x86_64-SQL_2019_Standard DBEBSVolumeSize: Description: Volume size to use for database, in GiBs Type: Number Default: 50 DBEBSVolumeType: Description: Volume type to use for database Type: String AllowedValues: - gp3 - io1 - io2 Default: gp3 DBEBSVolumeIops: Description: 'The number of I/O operations per second provisioned for the volume to use for the database. The following are the supported values for each volume type: gp3: 3,000-16,000 IOPS io1: 100-64,000 IOPS io2: 100-64,000 IOPS' Type: Number Default: 3000 IamInstanceProfile: Description: The name of the shared instance template stack Type: String Default: SSMInstanceProfile (TODO: do we need to add info about all ssm params being created as a part of this stack?)"
  },
  "versions/2.1.3/content/stacks/stacks.html": {
    "href": "versions/2.1.3/content/stacks/stacks.html",
    "title": "Stacks",
    "keywords": "Stacks The documentation in this section describes the stacks that are used to deploy i2 Analyze in an AWS environment."
  },
  "versions/2.1.3/content/tools and functions/aws_tools.html": {
    "href": "versions/2.1.3/content/tools and functions/aws_tools.html",
    "title": "AWS tools",
    "keywords": "AWS tools create-and-upload-aws-resources.sh deploy-aws.sh create-and-upload-aws-resources.sh script The create-and-upload-aws-resources.sh script creates the configuration, secrets, images, and runbooks then uploads them to AWS. The following functions are used when you run the script: Set the dependency tag The setDependenciesTag function sets the value of the dependency tag to the I2ANALYZE_VERSION defined in the version file. This ensures all the core images are labeled in AWS ECR for that version of the product and any other deployment can reuse them. (TODO: populate with more info for CIR-781) Creates the configuration The AWS reference architecture uses an example configuration. The configuration is created by the createBaseConfiguration function. The generateSolrConfiguration function generates the configuration for Solr from the base configuration. Creates the S3 buckets The createS3Bucket function creates two AWS S3 buckets using the AWS CLI: analyze-containers-aws-ref-arch-config stores the configuration. analyze-containers-aws-ref-arch-resources stores other resources. You can use the S3 management console in AWS to view the contents of the buckets. Creates the secrets The createSecrets function generates all the secrets and passwords that are required for the deployment and pushes them to the secret store (TODO - any doc/reference for this) and shreds the local copy of these secrets. Creates the images The createImages function builds the Docker images that are required for the deployment and tags them with the I2ANALYZE_VERSION defined in the version file. The built images are uploaded to your AWS Elastic Container Registry. To see your AWS ECR, go to ECR console The function uses the utils/buildImages.sh script to build the images, and the examples/aws/utils/push-images.sh script to push the images to AWS. Upload artifacts The uploadArtifacts function is used to upload the configuration to the analyze-containers-aws-ref-arch-config S3 bucket. The function also uploads the CloudFormation templates and i2a-scripts to the analyze-containers-aws-ref-arch-resources bucket. Create runbooks The createRunbooks function creates the runbooks that are required to start and stop the system and uploads them to AWS System Manager . For more information about the runbooks, see Runbooks . deploy-aws.sh script The script creates all the stacks that are required for the i2 Analyze deployment. The stacks are created in the following order: vpc security storage launch-templates sqlserver solr admin-client For more information about the stacks, see Stacks . After the stacks are created, the following runbooks are used to start the system: i2a-UpdateScripts i2a-SolrFirstRun i2a-SqlServerFirstRun For more information about the runbooks, see Runbooks ."
  },
  "versions/2.1.3/content/tools and functions/client_functions.html": {
    "href": "versions/2.1.3/content/tools and functions/client_functions.html",
    "title": "Client utilities",
    "keywords": "Client utilities The clientFunctions.sh file contains functions that you can use to perform actions against the server components of i2 Analyze. Secrets utilities The getSecret function gets a secret such as a password for a user. Status utilities The status utilities report whether a component of i2 Analyze is live. waitForIndexesToBeBuilt The waitForIndexesToBeBuilt function sends a request to the admin/indexes/status endpoint. If the response indicates no indexes are still BUILDING the function returns and prints a message to indicate the indexes have been built. If the indexes are not all built after 5 retries the function will print an error and exit. waitForConnectorToBeLive This function takes (1) the fully qualified domain name of a connector and (2) the port of the connector as its arguments. The waitForConnectorToBeLive function sends a request to the connector's /config endpoint. If the response is 200 , the connector is live. If the connector is not live after 50 tires the function will print an error and exit. getAsyncRequestStatus This function takes (1) the request id of the asynchronous request to get the status of. The getAsyncRequestStatus function makes a request to the Asynchronous Collection API and check the state is marked as completed in the JSON response returned. If the state is not marked as completed , the function returns the response message which contains any error messages that are reported with the asynchronous request. For more information about the Asynchronous Collection API, see REQUESTSTATUS: Request Status of an Async Call . getSolrStatus This function takes (1) a timestamp that is used to specify the point in the logs after which the logs are monitored. The getSolrStatus function used the logs from the Liberty container, and uses a grep command to find specific component availability messages. The function returns the entries in the logs that match message in the grep command. If no matching message is found, the function prints the following error and exits: \"No response was found from the component availability log (attempt: ${i}). Waiting...\" Database Security Utilities changeSAPassword The changeSAPassword function uses the generated secrets to call the changeSAPassword.sh with the initial (generated) sa password and the new (generated) password. createDbLoginAndUser The createDbLoginAndUser function takes a user and a role as its arguments. The function creates a database login and user for the provided user , and assigns the user to the provided role . Execution utilities The execution utilities enable you to run commands and tools from client containers against the server components of i2 Analyze. runSolrClientCommand The runSolrClientCommand function uses an ephemeral Solr client container to run commands against Solr. For more information about the environment variables and volume mounts that are required for the Solr client, see Running a Solr client container . The runSolrClientCommand function takes the command you want to run as an argument. For example: runSolrClientCommand \"/opt/solr-8.8.2/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd clusterprop -name urlScheme -val https For more information about commands you can execute using the Solr zkcli , see Solr ZK Command Line Utilities runi2AnalyzeTool The runi2AnalyzeTool function uses an ephemeral i2 Analyze Tool container to run the i2 Analyze tools. For more information about the environment variables and volume mounts that are requires for the i2Analyze tool, see Running an i2 Analyze Tool container The runi2AnalyzeTool function takes the i2 tool you want to run as an argument. For example: runi2AnalyzeTool \"/opt/i2-tools/scripts/updateSecuritySchema.sh\" runi2AnalyzeToolAsExternalUser The runi2AnalyzeToolAsExternalUser function uses an ephemeral i2 Analyze Tool container to run commands against the i2 Analyze service via the load balancer as an external user. The container contains the required secrets to communicate with the i2 Analyze service from an external container. For example, if you would like to send a Curl request to the load balancer stats endpoint, run: runi2AnalyzeToolAsExternalUser bash -c \"curl \\ --silent \\ --cookie-jar /tmp/cookie.txt \\ --cacert /tmp/i2acerts/CA.cer \\ --request POST \\\"${FRONT_END_URI}/j_security_check\\\" \\ --header 'Origin: ${FRONT_END_URI}' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'j_username=Jenny' \\ --data-urlencode 'j_password=Jenny' \\ && curl \\ --silent \\ --cookie /tmp/cookie.txt \\ --cacert /tmp/i2acerts/CA.cer\\ \\\"${FRONT_END_URI}/api/v1/admin/indexes/status\\\"\" runSQLServerCommandAsETL The runSQLServerCommandAsETL function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the etl user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsETL function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsETL bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -S \\${DB_SERVER} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} -Q \\\"BULK INSERT IS_Staging.E_Person FROM '/var/i2a-data/law-enforcement-data-set-2-merge/person.csv' WITH (FORMATFILE = '/var/i2a-data/law-enforcement-data-set-2-merge/sqlserver/format-files/person.fmt', FIRSTROW = 2)\\\"\" runSQLServerCommandAsi2ETL The runSQLServerCommandAsi2ETL function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the i2etl user, such as executing generated drop/create index scripts, created by the ETL toolkit. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsi2ETL function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsi2ETL bash -c \"${SQLCMD} ${SQLCMD_FLAGS} \\ -S \\${DB_SERVER},${DB_PORT} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} \\ -i /opt/database-scripts/ET5-drop-entity-indexes.sql\" runSQLServerCommandAsFirstStartSA The runSQLServerCommandAsFirstStartSA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the sa user with the initial SA password. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . runSQLServerCommandAsSA The runSQLServerCommandAsSA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the sa user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsSA function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsSA \"/opt/i2-tools/scripts/database-creation/runStaticScripts.sh\" runSQLServerCommandAsDBA The runSQLServerCommandAsDBA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the dba user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsDBA function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsDBA \"/opt/i2-tools/scripts/clearInfoStoreData.sh\" runSQLServerCommandAsDBB The runSQLServerCommandAsDBB function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the dbb (the backup operator) user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsDBB function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsDBA \"runSQLServerCommandAsDBB bash -c \" \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"USE ISTORE; BACKUP DATABASE ISTORE TO DISK = '/backup/istore.bak' WITH FORMAT;\\\"\"\" runEtlToolkitToolAsi2ETL The runEtlToolkitToolAsi2ETL function uses an ephemeral ETL toolkit container to run ETL toolkit tasks against the Information Store using the i2 ETL user credentials. For more information about running the ETL Client container and the environment variables required for the container, see ETL Client . For more information about running the ETL toolkit container and the tasks that you can run, see ETL The runEtlToolkitToolAsi2ETL function takes command that you want to run as an argument. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/addInformationStoreIngestionSource --ingestionSourceName EXAMPLE_1 --ingestionSourceDescription EXAMPLE_1\" runEtlToolkitToolAsDBA Some ETL tasks must be performed by the DBA. The runEtlToolkitToolAsDBA function used the same ephemeral ETL toolkit container but uses the DBA user instead of the ETL user. For more information about running the ETL Client container and the environment variables required for the container, see ETL Client . For more information about running the ETL toolkit container and the tasks that you can run, see ETL The runEtlToolkitToolAsDBA function takes command that you want to run as an argument. For example: runEtlToolkitToolAsDBA bash -c \"/opt/ibm/etltoolkit/enableMergedPropertyValues --schemaTypeId ET5\""
  },
  "versions/2.1.3/content/tools and functions/common_variables.html": {
    "href": "versions/2.1.3/content/tools and functions/common_variables.html",
    "title": "Common Variables",
    "keywords": "Common Variables All scripts in the containerised environment use variables to configure the system such as location of file paths, host names, port etc These variables are all stored in a central script in utils/commonVariables.sh script. The following descriptions explains the variables in the createVariables.sh script in more detail. License Variables This section contains the license variables that are used by the containers. These variables must be accepted before running the system. Network Security Variables This section contains the variables to turn on or off SSL. For more information about switching SSL on or off see the image and containers section or secrets section Ports This section contains the variables for all the ports that are exposed on the containers. Connection Information This section contains connection variables such as the Zookeeper hosts that are used by various containers Localisation variables This section contains the localisation variables. These variables are used by the createConfiguration.sh script to set up the configuration in the correct locale. Image names This section contains the image names for all the images built by the buildImages.sh script. These variables are also used by the clientFunctions.sh and severFunctions.sh scripts. Container names This section contains the container names for all the containers run in the environment. Volume names This section contains the volume names for all the volumes used by the containers run in the environment. User names This section contains all the usernames used by the servers in the containerised environment. Container secrets paths This section contains variables for the container secrets and certificate directories. For more information about these variables see the secrets section. Security configuration This section contains security variables used by the generateSecrets.sh scripts. For example the duration or the certificate or the certificate key size. Database variables This section contains database connection variables, database paths, OS type and other database related variables used by the scripts. Path variables The section contains the various paths in the environment that are used by the scripts. Front end URI This section contains the variable for the front end URI."
  },
  "versions/2.1.3/content/tools and functions/create_configuration.html": {
    "href": "versions/2.1.3/content/tools and functions/create_configuration.html",
    "title": "Creating the configuration",
    "keywords": "Creating the configuration Before you can deploy i2 Analyze, the configuration for the deployment must exist. You can create the configuration by running the utils/createConfiguration.sh script. The following descriptions explain what the createConfiguration.sh script does in more detail. Creates the base configuration The configuration is created in examples/pre-prod/ . The configuration is created from the all-patterns base configuration in the minimal toolkit. The base configuration can be used for any of the deployment patterns, however some properties are for specific patterns. By default, a schema and security schema are specified. The schemas that are used are based on the law-enforcement-schema.xml . Copies the JDBC drivers The JDBC drivers are copied from the pre-reqs/jdbc-drivers directory to the configuration/environment/common/jdbc-drivers directory in your configuration. Configures Form Based Authentication The createConfiguration.sh scripts configures form based authentication in the examples/pre-prod/configuration/fragments/common/WEB-INF/classes/server.extensions.xml ."
  },
  "versions/2.1.3/content/tools and functions/create_environment.html": {
    "href": "versions/2.1.3/content/tools and functions/create_environment.html",
    "title": "Creating the environment",
    "keywords": "Creating the environment Before you can deploy i2 Analyze in a containerised environment, the local environment must be created. The local environment requires the prerequisites to be in the correct locations, and copies some artifacts and tools from the i2 Analyze toolkit. You can create the local environment by running the utils/createEnvironment.sh script. The following descriptions explain what the createEnvironment.sh script does in more detail. Extracts the i2 Analyze minimal toolkit The script extracts the toolkit tar.gz located in the pre-reqs directory into the the pre-reqs/i2Analyze directory. Populate image clients The createEnvironment.sh script creates populate client images with the contents of the i2-tools & scripts folder of the toolkit. ETL toolkit The createEnvironment.sh script creates the ETL toolkit that is built into the etl toolkit image. The etl toolkit is created in images/etl_client/etltoolkit . Example connector application The createEnvironment.sh script creates the example connector that is built into the connector image. The application consists of the contents of i2analyze/toolkit/examples/connectors/example-connector . The connector application is created in images/example_connector/app . Liberty application The createEnvironment.sh script creates the i2 Analyze liberty application that is built into the Liberty base image. The application consists of files that do not change based on the configuration. After the application is created, you do not need to modify it. The application is created in images/liberty_ubi_base/application ."
  },
  "versions/2.1.3/content/tools and functions/deploy.html": {
    "href": "versions/2.1.3/content/tools and functions/deploy.html",
    "title": "Deploy and start i2 Analyze",
    "keywords": "Deploy and start i2 Analyze This topic describes how to deploy and start i2 Analyze in a containerized environment. For an example of the activities described, see the deploy.sh Running Solr and ZooKeeper The running Solr and ZooKeeper section runs the required containers and creates the Solr cluster and ZooKeeper ensemble. Create Solr cluster The createSecureCluster function creates the secure Solr cluster for the deployment. The function includes a number of calls that complete the following actions: The runZK server function runs the ZooKeeper containers that make up the ZooKeeper ensemble. For more information about running a ZooKeeper container, see ZooKeeper . In deploy.sh , 3 ZooKeeper containers are used. The runSolrClientCommand client function is used a number of times to complete the following actions: Create the znode for the cluster. i2 Analyze uses a ZooKeeper connection string with a chroot. To use a chroot connection string, a znode with that name must exist. For more information, see SolrCloud Mode . Set the urlScheme to be https . Configure the Solr authentication by uploading the security.json file to ZooKeeper. For more information about the function, see runSolrClientCommand . The runSolr server function runs the Solr containers for the Solr cluster. For more information about running a Solr container, see Solr . In deploy.sh , 2 Solr containers are used. At this point, your ZooKeepers are running in an ensemble, and your Solr containers are running in SolrCloud Mode managed by ZooKeeper. Initializing the Information Store database The initializing the Information Store database section creates a persistent database backup volume, runs the database container and configures the database management system. The database backup volume is created first with the Docker command: docker volume create \"${SQL_SERVER_BACKUP_VOLUME_NAME}\" so that the volume will not be automatically deleted when the SQL Server container is remove. This helps maintain any backups created while running a SQL server container. For more information about docker storage, see Docker Storage . Running the database server container The runSQLServer server function creates the secure SQL Server container for the deployment. For more information about building the SQL Server image and running a container, see Microsoft SQL Server . Before continuing, deploy.sh uses the waitForSolrToBeLive client function and waitForSQLServerToBeLive common function to ensure that Solr and SQL Server are running. For more information, see Status utilities Configuring SQL Server The configureSecureSqlServer function uses a number of client and server functions to complete the following actions: The changeSAPassword client function is used to change the sa user's password. For more information, see changeSAPassword Generate the static Information Store database scripts. The runi2AnalyzeTool client function is used to run the generateStaticInfoStoreCreationScripts.sh tool. runi2AnalyzeTool Generate static database scripts tool Create the Information Store database and schemas. The runSQLServerCommandAsSA client function is used to run the runDatabaseCreationScripts.sh tool. runSQLServerCommandAsSA runDatabaseCreationScripts.sh Create the database roles, logins, and users. For more information about the database users and their permissions, see Database users . The runSQLServerCommandAsSA client function runs the createDbRoles.sh script. The createDbLoginAndUser client function creates the logins and users. The runSQLServerCommandAsSA client function runs the applyBuiltInSQLServerRoles.sh script. Grant the dba user the required permissions in the msdb and master databases, this grants the correct permissions for the Deletion by Rule feature of i2Analyze. The runSQLServerCommandAsSA client function runs the configureDbaRolesAndPermissions.sh script. Make the etl user a member of the SQL server sysadmin group to allow this user to perform bulk inserts into the external staging tables. The runSQLServerCommandAsSA client function runs the addEtlUserToSysAdminRole.sh script. Run the static scripts that create the Information Store database objects. The runSQLServerCommandAsSA client function is used to run the runStaticScripts.sh tool. runSQLServerCommandAsSA Run static database scripts tool Configuring Solr and ZooKeeper The configuring Solr and ZooKeeper sections creates Solr configuration and then configures the Solr cluster and creates the Solr collections. The generateSolrSchemas.sh i2-tool creates the solr directory in examples/pre-prod/configuration/solr/generated_config . This directory contains the managed-schema, Solr synonyms file and Solr config files for each index. The configureSecureSolr function uses the runSolrClientCommand client function to upload the managed-schema , solr.xml , and synonyms file for each collection to ZooKeeper. For example: runSolrClientCommand solr zk upconfig -v -z \"${ZK_HOST}\" -n daod_index -d /conf/solr_config/daod_index The setClusterPolicyForSecureSolr function uses the runSolrClientCommand client function to set a cluster policy such that each host has 1 replica of each shard. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer -X POST -H Content-Type:text/xml -d '{ \\\"set-cluster-policy\\\": [ {\\\"replica\\\": \\\"<2\\\", \\\"shard\\\": \\\"#EACH\\\", \\\"host\\\": \\\"#EACH\\\"}]}' \\\"${SOLR1_BASE_URL}/api/cluster/autoscaling\\\"\" For more information about Solr policies, see Autoscaling Policy and Preferences . The createCollectionForSecureSolr function uses the runSolrClientCommand client function to create each Solr collection. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/admin/collections?action=CREATE&name=main_index&collection.configName=main_index&numShards=1&maxShardsPerNode=4&rule=replica:<2,host:*\\\"\" For more information about the Solr collection API call, see CREATE: Create a Collection . Configuring the Information Store database The configuring the Information Store database section creates objects within in the database. Generate the dynamic database scripts that create the schema specific database objects. The runi2AnalyzeTool client function is used to run the generateDynamicInfoStoreCreationScripts.sh tool. runi2AnalyzeTool Generate dynamic Information Store creation scripts tool Run the generated dynamic database scripts. The runSQLServerCommandAsSA client function is used to run the runDynamicScripts.sh tool. runSQLServerCommandAsSA Run dynamic Information Store creation scripts tool Configuring the Example Connector The configuring example connector section runs the example connector used by the i2 Analyze application. The runExampleConnector server function runs the example connector application. The waitForConnectorToBeLive client function checks the connector is live before allowing the script to proceed. Configuring i2 Analyze The configuring i2 Analyze section runs the Liberty containers that run the i2 Analyze application. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . The runLiberty server function runs a Liberty container from the configured image. For more information, see Running a Liberty container In deploy.sh , 2 liberty containers are used. Starting the load balancer. The runLoadBalancer functions in serverFunctions.sh runs HAProxy as a load balancer in a Docker container. The load balancer configuration we use is generated from the haproxy-template.cfg template file. The load balancer routes requests to the application to both Liberty servers that are running. The configuration that used is a simplified configuration for example purposes and is not to be used in production. For more information about configuring a load balancer with i2 Analyze, see Load balancer . Before continuing, deploy.sh uses the waitFori2AnalyzeServiceToBeLive common function to ensure that Liberty is running. Deploy the system match rules. The runSolrClientCommand client function is used to run the runIndexCommand.sh tool. The tool is run twice, once to update the match rules file and once to switch the match indexes. For more information, see Manage Solr indexes tool ."
  },
  "versions/2.1.3/content/tools and functions/etl_tools.html": {
    "href": "versions/2.1.3/content/tools and functions/etl_tools.html",
    "title": "ETL Tools",
    "keywords": "ETL Tools This topic describes how to perform ETL tasks by using the ETL toolkit in a containerized deployment of i2 Analyze. All of the tools described here are located in the images/etl_client directory. The etl_client directory is populated when running createEnvironment.sh . The runEtlToolkitToolAsi2ETL client function is used to run the ETL tools described in this topic as the i2ETL user. For more information about this client function, see runEtlToolkitToolAsi2ETL Building an ETL Client image The ETL client image is built from the Dockerfile in images/etl_client . The following docker run command builds the configured image: docker build -t \"etl_client:4.3.4\" \"images/etl_client\" Add Information Store ingestion source The addInformationStoreIngestionSource tool defines an ingestion source in the Information Store. For more information about ingestion sources in the Information Store, see Defining an ingestion source . You must provide the following arguments to the tool: Argument Description Maximum characters n A unique name for the ingestion source 30 d A description of the ingestion source that might appear in the user interface 100 Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/addInformationStoreIngestionSource -n <> -d <> \" Create Information Store staging table The createInformationStoreStagingTable tool creates the staging tables that you can use to ingest data into the Information Store. For more information about creating the tables, see Creating the staging tables . You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the staging table for sn The name of the database schema to create the staging table in tn The name of the staging table to create Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/createInformationStoreStagingTable -stid <> -sn <> -tn <> \" Ingest Information Store records The ingestInformationStoreRecords is used to ingest data into the Information Store. For more information about ingesting data into the Information Store, see The ingestInformationStoreRecords toolkit task You can use the following arguments with the tool: Argument Description imf The full path to the ingestion mapping file. imid The ingestion mapping identifier in the ingestion mapping file of the mapping to use im Optional: The import mode to use. Possible values are STANDARD, VALIDATE, BULK, DELETE, BULK_DELETE or DELETE_PREVIEW. The default is STANDARD. icf Optional: The full path to an ingestion settings file il Optional: A label for the ingestion that you can use to refer to it later lcl Optional: Whether (true/false) to log the links that were deleted/affected as a result of deleting entities Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/ingestInformationStoreRecords -imf <> -imid <> -im <>\" Sync Information Store records The syncInformationStoreCorrelation tool is used after an error during correlation, to synchronize the data in the Information Store with the data in the Solr index so that the data returns to a usable state Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/syncInformationStoreCorrelation\" Duplicate provenance check The duplicateProvenanceCheck tool can be used for identifying records in the Information Store with duplicate origin identifiers. Any provenance that has a duplicated origin identifier is added to a staging table in the Information Store. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTool bash -c \"/opt/ibm/etltoolkit/syncInformationStoreCorrelation\" Duplicate provenance delete The duplicateProvenanceDelete tool deletes (entity/link) provenance from the Information Store that has duplicated origin identifiers. The provenance to delete is identified in the staging tables created by the duplicateProvenanceCheck tool. You can provide the following argument to the tool: Argument Description stn The name of the staging table that contains the origin identifiers to delete. If no arguments are provided, duplicate origin identifiers are deleted from all staging tables. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/syncInformationStoreCorrelation\" Generate Information Store index creation scripts The generateInformationStoreIndexCreationScript tool generates the scripts that create the indexes for each item type in the Information Store. For more information, see database index management You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the index creation scripts for. op The location to create the scripts. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTask bash -c \"/opt/ibm/etltoolkit/generateInformationStoreIndexCreationScript -op <> -stid <> \" Generate Information Store index drop scripts The generateInformationStoreIndexDropScript tool generates the scripts that drop the indexes for each item type in the Information Store. For more information, see database index management You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the index drop scripts for. op The location to create the scripts. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTask bash -c \"/opt/ibm/etltoolkit/generateInformationStoreIndexDropScript --op <> -stid <> \" Delete orphaned database objects The deleteOrphanedDatabaseObjects tool deletes (entity/link) database objects that are not associated with an i2 Analyze record from the Information Store. You can provide the following arguments to the tool: Argument Description iti Optional: The schema type identifier of the item type to delete orphaned database objects for. If no item type id is provided, orphaned objects for all item types are removed Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/deleteOrphanedDatabaseObjects -iti <> \" Disable merged property values The disableMergedPropertyValues tool removes the database views used to define the property values of merged i2 Analyze records. You can provide the following arguments to the tool: Argument Description etd The location of the root of the etl toolkit. stid The schema type identifier to disable the views for. If no schema type identifier is provided, the views for all of the item types are be removed Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/disableMergedPropertyValues -etd <> -stid <>\" For more information about correlation, see Information Store data correlation Enable merge property values The enableMergedPropertyValues tool creates the database views used to define the property values of merged i2 Analyze records. You can provide the following arguments to the tool: Argument Description etd The location of the root of the etl toolkit. stid The schema type identifier to create the views for. If no schema type identifier is provided, the views for all of the item types are generated. If the views already exist, they are overwritten. Use the runEtlToolkitToolAsDBA client function to run the tool as the database administrator. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/enableMergedPropertyValues -etd <> -stid <> \" For more information about correlation, see Information Store data correlation"
  },
  "versions/2.1.3/content/tools and functions/i2analyze_tools.html": {
    "href": "versions/2.1.3/content/tools and functions/i2analyze_tools.html",
    "title": "i2 Analyze tools",
    "keywords": "i2 Analyze tools The i2 Analyze deployment toolkit includes a number of tools that enable you to deploy and administer i2 Analyze. Note the TOOLKIT_DIR environment variable is set by the i2Tools image and does not need to be passed to the container at runtime. Generate Information Store scripts The generateInfoStoreToolScripts.sh script is used to generate Information Store scripts from templates in i2-tools/scripts/database-template directory located inside the i2Tools image. The generated scripts end up in in the GENERATED_DIR . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. GENERATED_DIR The root location where any generated scripts are created. DB_TRUSTSTORE_LOCATION The location of the truststore.jks . Schema update tool The generateUpdateSchemaScripts.sh script is used to update the i2 Analyze schema in the Information Store. To update the schema, the tool generates a number of SQL scripts that must be run against the Information Store database. When you run the tool, it compares the schema file in the configuration directory against the schema that is stored in the IS_META.I2_SCHEMAS table in Information Store database. If they are different, the schema in the database is updated with the one from the configuration. Then, the IS_DATA table structure is compared with the schema IS_META.I2_SCHEMAS . If the IS_DATA objects are different, the scripts to update the objects are generated. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. /update is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration ./database/sqlserver/InfoStore/generated/update You can run the generated scripts against the database manually, or you can use the Run database scripts tool . Security schema update tool The updateSecuritySchema.sh script is used to update the security schema in a deployment of i2 Analyze. When you run the script, the security schema file in the configuration directory is compared against the security schema that is stored in the IS_META.I2_SCHEMAS table in Information Store database. If they are different, the security schema in the database is updated with the one from the configuration. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration Run database scripts tool You can run any generated scripts against the database manually, or you can use the runDatabaseScripts.sh script. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires the following environment variables to be set: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. Manage Solr indexes tool By using the indexControls.sh script, you can pause and resume indexing on any index, upload system match rules, and switch a standby index to be the live index. Note: Switching from a standby index to live is possible for the following index types: chart, match, and main. The script requires the following environment variables to be present: Environment Variable Description ZOO_USERNAME The name of the administrator user for performing administration tasks. ZOO_PASSWORD The password for the administrator user. ZK_HOST The connection string for each ZooKeeper server to connect to. ZOO_SSL_TRUST_STORE_LOCATION The location of the truststore.jks file to be used for SSL connections to ZooKeeper. ZOO_SSL_TRUST_STORE_PASSWORD The password for the truststore. ZOO_SSL_KEY_STORE_LOCATION The location of the keystore.jks file to be used for SSL connections to ZooKeeper. ZOO_SSL_KEY_STORE_PASSWORD The password for the keystore. CONFIG_DIR The location of your configuration. This is used to locate the system-match-rules.xml . TASK The index control task to perform. You can specify the following values for TASK : UPDATE_MATCH_RULES SWITCH_STANDBY_MATCH_INDEX_TO_LIVE CLEAR_STANDBY_MATCH_INDEX REBUILD_STANDBY_MAIN_INDEX SWITCH_STANDBY_MAIN_INDEX_TO_LIVE CLEAR_STANDBY_MAIN_INDEX REBUILD_STANDBY_CHART_STORE_INDEX SWITCH_STANDBY_CHART_STORE_INDEX_TO_LIVE CLEAR_STANDBY_CHART_STORE_INDEX PAUSE_INDEX_POPULATION RESUME_INDEX_POPULATION For more information about the index control tasks, see Index control toolkit tasks . Information Store database consistency tool The databaseConsistencyCheck.sh script checks that the database objects in the IS_DATA database schema are consistent with the information in IS_META . The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration If the database is a consistent state with the schema, the following message will be returned in the console The Database is in a consistent state. If the database is an inconsistent state with the schema, errors will be reported in the console where the tool was run. For example: The following changes to the Information Store database tables and/or views have been detected: MODIFIED ITEM TYPES Item type: Arrest ADDED PROPERTY TYPES Property type: Voided Item type: Arrest The following changes to the Information Store database tables and/or views have been detected: ADDED ITEM TYPES Item type: Vehicle > ERROR [DatabaseConsistencyCheckCLI] - Information Store database is inconsistent with the schema stored in the database Schema validation tool The validateSchemaAndSecuritySchema.sh script checks that the database objects in the IS_META database schema are consistent with the i2 Analyze schema and the security schema in the IS_META.I2_SCHEMAS . The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration If the schemas are valid no errors will be reported. However, if the schemas are the same as the version of the database this information will be returned to the console. If the schemas contain validation errors will be reported to the console where the tool was run. For example: VALIDATION ERROR: Schema contains duplicate type EntityType with ID 'ET5'. Generate static database scripts tool The generateStaticInfoStoreCreationScripts.sh script injects values from your configuration into the static database scripts. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. /static is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration ../scripts/database/{dialect}/InfoStore/generated/static The script creates the .sql files in the location of the GENERATED_DIR that was supplied to the container. Run static database scripts tool The runStaticScripts.sh script runs the static scripts that create the Information Store database. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the static scripts to be run. /static is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Generate dynamic Information Store creation scripts tool The generateDynamicInfoStoreCreationScripts.sh script creates the .sql scripts that are required to create the database objects that align to your schema in the configuration. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The location where any generated scripts are created. /dynamic is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the i2-tools/scripts directory: ../../toolkit ../configuration ../scripts/database/{database dialect}/InfoStore/generated/dynamic The script creates the .sql files in the location of the GENERATED_DIR that was supplied to the container. Run dynamic Information Store creation scripts tool The runDynamicScripts.sh script runs the dynamic scripts that create the Information Store database. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The runSQLServerCommandAsSA client function is used to run the runDynamicScripts.sh script that create the Information store and schemas. runSQLServerCommandAsSA The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the scripts to be run. /dynamic is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Run database creation scripts tool The runDatabaseCreationScripts.sh script runs the create-database-storage.sql & 0001-create-schemas.sql database creation scripts. These scripts create the Information Store database and schemas. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the scripts to be run. /dynamic is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Remove data from the Information Store tool The runClearInfoStoreData.sh script removes all the data from the Information Store database. The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The path to the root of the directory that contains the generated static scripts. /static is appended to the value of GENERATED_DIR . The GENERATED_DIR environment variable must be present and is not defaulted. For more information about clearing data from the Information Store, see Clearing Data Create static Liberty application tool The createLibertyStaticApp.sh creates the static application to be hosted in Liberty. The tool combines the contents of various folders to create the application, in a similar way to the deployLiberty toolkit task. To run the tool, you must provide a location to create the application files as an argument. Alternatively if you are running the tool from outside of the toolkit/i2-tools/scripts folder the following environment variable must be set: Environment Variable Description TOOLKIT_DIR The location of the toolkit. When you call the createLibertyStaticApp.sh script, you must pass the location where the Liberty application directory is created. Connection properties Some of the tools require database connection properties such as database username, database truststore location & password for example in order to communicate with the database so that the tool can run. These properties can be loaded from a file called Connection.properties that is located in your configuration in the following directory configuration/fragments/common/WEB-INF/classes/ . For example: DBName=ISTORE DBDialect=sqlserver DBServer=sqlserver.eia DBPort=1433 DBOsType=UNIX DBSecureConnection=true DBCreateDatabase=false db.installation.dir=/opt/mssql-tools db.database.location.dir=/opt/mssql In the docker environment, these properties can be alternatively loaded via environment variables passed into the container. The following environment variables are supported for supplying connection properties: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_DIALECT The dialect for the database. Can be sqlserver or db2 . DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_OS_TYPE The Operating System that the database is on. Can be UNIX , WIN , or AIX . DB_INSTALL_DIR Specifies the database CMD location. DB_LOCATION_DIR Specifies the location of the database. DB_TRUSTSTORE_LOCATION The location of the truststore.jks . * DB_TRUSTSTORE_PASSWORD The password for the truststore. * The security settings are only required if your database is configured to use a secure connection. Generate Solr Config The generateSolrSchemas.sh generates Solr Config by taking the template schema-template-definition.xml file from the configuration/solr and generating solr config per index. For each index, the tool will generate solrconfig.xml , managed_schema and synonyms-<LOCALE>.txt file. NOTE: to change SOLR locale uncomment the relevant section from the schema-template-definition.xml file before deploying. The script requires the following environment variables to be present: Environment Variable Description CONFIG_DIR The location of the root of the configuration directory."
  },
  "versions/2.1.3/content/tools and functions/reset_repository.html": {
    "href": "versions/2.1.3/content/tools and functions/reset_repository.html",
    "title": "Resetting the repository",
    "keywords": "Resetting the repository The local repository can be returned to its initial state by running the resetRepository.sh script. The following artifacts are removed by the resetRepository.sh : All Docker resources such as images, containers, volumes and networks. The simulated-secret-store directory. Resources that are copied to various image directories when the environment is created. Database scripts that are generated as part of the deployment process. The ETL toolkit that is located in images/etl_client/etltoolkit The example connector that is located in images/example_connector/app After you run the resetRepository.sh script, you must follow the getting started process from the Creating a containerised deployment section."
  },
  "versions/2.1.3/content/tools and functions/tools.html": {
    "href": "versions/2.1.3/content/tools and functions/tools.html",
    "title": "Tools and functions",
    "keywords": "Tools and functions The documentation in this section describes the tools and functions that are used to deploy i2 Analyze in a containerised environment."
  },
  "versions/2.1.3/content/walkthroughs/add_connector.html": {
    "href": "versions/2.1.3/content/walkthroughs/add_connector.html",
    "title": "Adding a new Connector",
    "keywords": "Adding a new Connector This section describes an example of the process of adding a new Connector. You can use addExampleConnector.sh script to add a new second example connector to the example deployment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Updating Connectors.json To make liberty aware of the new connector, you need to add a new connector definition to the connectors.json. The addConnectorToConnectorsJson can be used to do this. See Add example Connector to connectors.json section of the walkthrough script. Running a new Connector container To add a connector in the Docker environment, you need to run a new Connector container. See the Add example Connector section of the walkthrough script. The runExampleConnector server function in the serverFunctions.sh is used to run a new Example connector container. Updating your configuration After the connectors.json has been updated and the example connector has been run, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "versions/2.1.3/content/walkthroughs/add_solr_node.html": {
    "href": "versions/2.1.3/content/walkthroughs/add_solr_node.html",
    "title": "Adding a new Solr node",
    "keywords": "Adding a new Solr node This section describes an example of the process of adding a new Solr Node to an existing Solr cluster and using the Solr Collections API to add a replica on the newly created Solr node. You can use addSolrNodeWalkthrough.sh script to add a Solr node to the example deployment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Running a new Solr container To add a Solr node to an existing Solr collection in the Docker environment, you run a new Solr container. See the Running a new Solr container section of the walkthrough script. The runSolr server function in the addSolrNodeWalkthrough.sh is used to run a new Solr container with a node that is added to the existing cluster. For more information about running a Solr container, see Solr . The following environment variables are used to specify the hostname of the container and to add the node to the existing cluster: SOLR_HOST specifies the fully qualified domain name of the Solr container. ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated list. When the connection string connects to the existing ZooKeeper quorum, the new Solr node is automatically added to the Solr cluster. Adding a Solr replica To use the new Solr node, you must add a replica for a shard and create it on the new Solr node. See the Adding a Solr replica section of the walkthrough script. To add a replica, use the Solr Collections API. For more information about the API command, see ADDREPLICA: Add Replica The following curl command is an example that creates a replica for shard1 in the main_index collection on the new Solr node: curl -u \"${SOLR_ADMIN_DIGEST_USERNAME}\":\"${SOLR_ADMIN_DIGEST_PASSWORD}\" --cacert /CA/CA.cer \"${SOLR1_BASE_URL}/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard1&node=${SOLR3_FQDN}:8983_solr\" In the addSolrNodeWalkthrough.sh script, the runSolrClientCommand function contains an example of how to run the curl command in a containerized environment: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard1&node=${SOLR3_FQDN}:8983_solr\\\"\" For more information about command parsing, see Command parsing The example above uses ${SOLR3_FQDN} , but you can use the fully qualified domain name of any Solr node in the cluster."
  },
  "versions/2.1.3/content/walkthroughs/clear_data.html": {
    "href": "versions/2.1.3/content/walkthroughs/clear_data.html",
    "title": "Clearing data from a deployment",
    "keywords": "Clearing data from a deployment This section describes an example of the process to clear the data from your deployment in a Docker environment. Clearing the data from a deployment includes the following high-level steps: Removing the Liberty containers Clearing the search index Creating and running a delete query Removing the collection properties Clearing data from the Information Store Remove all of the data in the Information Store Running the Liberty containers The clearDataWalkthrough.sh script is a worked example that demonstrates how to clear the data from the Information Store in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you clear the data from a deployment, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Clearing the search index Creating and running a delete query To clear the search index, run a Solr delete query against your indexes via the Solr API. You can run the delete query by using a curl command. In Solr, data is stored as documents. You must remove every document from each collection in your deployment. The runSolrClientCommand client function is used to run the curl commands that remove the documents from each collection. For more information about the function, see runSolrClientCommand . The following curl command removes every document from a the main_index : curl -u \"${SOLR_ADMIN_DIGEST_USERNAME}:${SOLR_ADMIN_DIGEST_PASSWORD}\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\" -H Content-Type:\"text/xml\" --data-binary \"<delete><query>*:*</query></delete>\" See the Clearing the search index section of the walkthrough script. For more information about command parsing, see Command parsing Removing the collection properties The runSolrClientCommand client function is used to remove the file from ZooKeeper. For more information about the function, see runSolrClientCommand . The following zkcli call removes the collection properties for the main_index . zkcli.sh -zkhost \"${ZK_HOST}\" -cmd clear \"/collections/main_index/collectionprops.json\" See the Clearing the search index section of the walkthrough script. The collection properties must be removed for any main, match, or chart collections. The collectionprops.json file is recreated when the i2 Analyze application is started. Clearing data from the Information Store See the Clearing the Information Store database section of the walkthrough script. The runSQLServerCommandAsDBA client function is used to run the clearInfoStoreData.sh tool to remove the data from the Information Store. runSQLServerCommandAsDBA clearInfoStoreData Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "versions/2.1.3/content/walkthroughs/correlated_data.html": {
    "href": "versions/2.1.3/content/walkthroughs/correlated_data.html",
    "title": "Defining the property values of merged records",
    "keywords": "Defining the property values of merged records This section describes an example of the process to define how property values of merged records are calculated in a Docker environment. The process includes the following high-level steps: Enabling merged property values To define the view, enable and then modify it to meet your correlation requirements Updating property value definitions Ingest data that correlates and review its properties The mergedPropertyValuesWalkthrough.sh script is a worked example that demonstrates how to enables and modify the views in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Default merge property behavior To demonstrate the default behavior, use the ingestDataWalkthrough.sh script to ingest some data the correlates. For more information about ingesting the data, see Ingesting data into the Information Store . During the ingestion walk-through, the default behavior is used to determine the property values of the correlated record. In the default behavior, the property values from the merge contributor with the most recent value for source_last_updated are used. For more information about the how the property values for merged records are calculated, see Define how property values of merged records are calculated After you ingest the data, in Analyst's Notebook Premium search for Julia Yochum and add the returned entity to the chart. Keep the chart open for the remainder of the walkthrough script. Enabling merged property values To inform i2 Analyze that you intend to define the property values of merged records, run the enableMergedPropertyValues.sh tool. You can take control of the property values for records of specific item types, or all item types in the i2 Analyze schema. Note that this operation must be performed by the database administrator. See the Enabling merged property values section of the walkthrough script. The runEtlToolkitToolAsDBA client function is used to run the enableMergedPropertyValues.sh tool. runEtlToolkitToolAsDBA enableMergedPropertyValues Defining the property values of merged i2 Analyze records In the mergedPropertyValuesWalkthrough.sh , the views are created for the Person item type. The enableMergedPropertyValues.sh tool is used in the Create the merged property views for the CORRELATED_SCHEMA_TYPE_IDS section. Updating property value definitions The walkthrough provides an example .sql script that drops the existing IS_Public.E_Person_MPVDV view and replaces it with another. The new view prioritizes property values from merge contributors that come from the ingestion source names EXAMPLE_1 over values from EXAMPLE_2 and any other sources. The createAlternativeMergedPropertyValuesView.sql script is in examples/pre-prod/walkthroughs/configurationChanges . After the views are enabled, the merged property values definition view ( Person_MPVDV ) is modified to change how the property values of correlated records are calculated. Note, this step is also performed by the database administrator. See the Updating property value definitions section of the walkthrough script. The runSQLServerCommandAsDBA client functions in used to run the createAlternativeMergedPropertyValuesView.sql script. runSQLServerCommandAsDBA The merged property values definition view Reingesting the data The property values of merged records do not update when the MPVDV views are modified. To update the values of existing records, you must reingest at least one of the merge contributors to the record. To do this, use the ingestDataWalkthrough.sh script to ingest some data the correlates. For more information about ingesting the data, see Ingesting data into the Information Store . See the Reingesting the data section of the walkthrough script. After the data is ingested, in the Analyst's Notebook Premium chart that you have open, select the Julia Yochum item and click Get changes . The name of the item changes to Julie Yocham , because the property values that make up the name are now from the merge contributor where the ingestion source name is EXAMPLE_1 ."
  },
  "versions/2.1.3/content/walkthroughs/ingestion.html": {
    "href": "versions/2.1.3/content/walkthroughs/ingestion.html",
    "title": "Ingesting data into the Information Store",
    "keywords": "Ingesting data into the Information Store The ingestDataWalkthrough.sh script is a worked example that demonstrates how to script the process of ingesting data into the Information Store. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . The example provides a scripted mechanism to ingest data by using ETL toolkit. The script ingests data with and without correlation identifiers using both STANDARD and BULK import modes. For more information about ingestion, see: Information Store data ingestion Information Store data correlation To script the ingestion process, the following information is stored within the script: The mapping of item type identifiers to staging tables The mapping of staging tables to CSV files and format files. A list of import mapping identifiers from the mapping file. The script uses the runSqlServerCommandAsETL client function to import the data into the staging tables, and runEtlToolkitToolAsi2ETL to run the ingestion commands. Note the external ETL user is performing the import into the staging tables, but the i2 Internal ETL user is executing ETL tasks such as creating the ingestion source or performing the import from the staging tables. The data and ingestion artefacts that are used in the example are in the examples/data directory of the minimal toolkit. Creating the ingestion sources The runEtlToolkitToolAsi2ETL client function is used to run addInformationStoreIngestionSource ETL toolkit tool to create the ingestion sources. runEtlToolkitToolAsi2ETL addInformationStoreIngestionSource For more information about ingestion sources in the Information Store, see Defining an ingestion source For an example of how to use the tool, see the Create the ingestion sources section in the ingestDataWalkthrough.sh Creating the staging tables The runEtlToolkitToolAsi2ETL client function is used to run createInformationStoreStagingTable ETL toolkit tool to create the staging tables. runEtlToolkitToolAsi2ETL createInformationStoreStagingTable For more information about creating staging tables in the Information Store, see Creating the staging tables To create all of the staging tables in the example, schema type identifiers are mapped to staging table names. For an example of how the mappings are used, see the Create the staging tables section in the ingestDataWalkthrough.sh Note: Because there are multiple staging tables for the LAC1 link type, a second loop is used to iterate through the staging table names of each schema type. Ingesting data The walkthrough demonstrates how to ingest both non-correlated and correlated data. For each type of data the staging tables and populated, the data ingestion task is executed, then the staging tables are cleaned. Ingesting non correlated data In this case, the SQL Server BULK INSERT command is used to insert the CSV data into the staging tables. Ingesting correlated data See the Ingesting correlated data section of the walkthrough script. The ingestion procedure The runSqlServerCommandAsETL client function is used to run the sql statement that inserts the data into the staging tables. runSqlServerCommandAsETL Populating the staging tables The example uses CSV and format files to insert the data. The files have the same name ( person.csv and person.fmt ). The staging table names are mapped to the CSV and format files. There is one mapping for base data ( BASE_DATA_TABLE_TO_CSV_AND_FORMAT_FILE_NAME ) and one for correlation data ( CORRELATED_DATA_TABLE_AND_FORMAT_FILE_NAME ). For an example of how the mappings are used, see the Insert the base data into the staging tables section in the ingestDataWalkthrough.sh The runEtlToolkitToolAsi2ETL client function is used to run ingestInformationStoreRecords ETL toolkit tool to ingest the data into the Information Store from the staging tables. runEtlToolkitToolAsi2ETL ingestInformationStoreRecords The ingestInformationStoreRecords toolkit task The ingestInformationStoreRecords tool is used with BULK and STANDARD import modes. Standard import mode is used to ingest the correlation data sets. Bulk import mode is used to ingest the base data set. The import mapping identifiers to use with the ingestInformationStoreRecords tool are defined in the IMPORT_MAPPING_IDS and BULK_IMPORT_MAPPING_IDS lists. A loop is used to ingest data for each mapping identifier in the lists."
  },
  "versions/2.1.3/content/walkthroughs/reset_walkthroughs.html": {
    "href": "versions/2.1.3/content/walkthroughs/reset_walkthroughs.html",
    "title": "Resetting your environment",
    "keywords": "Resetting your environment Resetting the deployment after you complete one of the walkthroughs is done in 2 steps: Resets the configuration directory to the base example configuration Runs the deploy script to redeploy the system with the base configuration Resetting the configuration The configuration directory is reset by calling the pre-prod/createPreProdEnvironment.sh script. This creates a clean configuration for your deployment. Redeploying the system The pre-prod/deploy.sh script is run to redeploy the system."
  },
  "versions/2.1.3/content/walkthroughs/update_configuration.html": {
    "href": "versions/2.1.3/content/walkthroughs/update_configuration.html",
    "title": "Updating the i2 Analyze application configuration",
    "keywords": "Updating the i2 Analyze application configuration This section describes an example of the process to update the configuration of a deployment in a Docker environment. After you update the configuration, rebuild the configured Liberty image. You must rebuild the image, because the configuration is part of the image. Updating the configuration includes the following high-level steps: Removing Liberty containers Updating configuration Rebuilding the configured Liberty image with the modified configuration Running the Liberty containers When you start Liberty, the schema that it caches from the database is updated Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the application configuration, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Updating your configuration In the updateConfigurationWalkthrough.sh script, a modified geospatial-configuration.xml is being copied from the /walkthrouhgs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. For information about modifying the configuration, see Configuring the i2 Analyze application . See the Updating the configuration section of the walkthrough script. After you modify the configuration, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "versions/2.1.3/content/walkthroughs/update_match_rules.html": {
    "href": "versions/2.1.3/content/walkthroughs/update_match_rules.html",
    "title": "Updating the system match rules",
    "keywords": "Updating the system match rules This section describes an example of the process to update the system match rules of a deployment with the Information Store. Updating your match rules in your deployment includes the following high-level steps after you modify your system match rules file: Update the match rules file Upload the updated system match rules file to ZooKeeper Create a standby match index with the new system match rules, and wait for a response from the server to say that the standby match index is ready Switch your standby match index to live The updateMatchRulesWalkthrough.sh script is a worked example that demonstrates how to update the system match rules in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Updating your system match rules file In the updateMatchRulesWalkthrough.sh script, the modified system-match-rules.xml is copied from the examples/pre-prod/walkthrouhgs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. See the Updating system-match-rules.xml section of the walkthrough script. For more information about modifying the system match rules file, see Deploying system match rules . Uploading system match rules to ZooKeeper After you modify the system match rules file, use the update_match_rules function of the runIndexCommand.sh script to upload the match rules to ZooKeeper. For more information, see Manage Solr indexes tool . The runIndexCommand function with the update_match_rules argument in the updateMatchRulesWalkthrough.sh script contains the docker run command that uses an ephemeral Solr client container to upload the match rules. To use the tool in a Docker environment, the following prerequisites must be present: The Docker toolkit and your configuration must be available in the container. In the example, the toolkit and configuration are volume mounted in the docker run command. For example: -v \"pre-reqs/i2analyze/toolkit:/opt/toolkit\" \\ -v \"examples/pre-prod/configuration:/opt/\" Environment variables The tool requires a number of environment variables to be set. For the list of environment variables that you can set, see Manage Solr indexes tool . Java The container must be able to run Java executables. In the example, the container uses the adoptopenjdk image from DockerHub. adoptopenjdk/openjdk8:ubi-jre Checking the standby match index status When uploading match rules, the runIndexCommand.sh will wait for 5 minutes for a response from the server. If however, it takes longer to create the new match index a curl command can be run against liberty. The waitForIndexesToBeBuilt client function makes a request to the api/v1/admin/indexes/status endpoint and inspects the JSON response to see if the match index is built. Switching the standby match index to live After the standby index is ready, you can switch the standby index to live for resolving matches. Use the switch_standby_match_index_to_live function of the runIndexCommand.sh script to switch the indexes. The runIndexCommand function with the switch_standby_match_index_to_live argument in the updateMatchRulesWalkthrough.sh script contains the docker run command that uses an ephemeral Solr client container to switch the match indexes. The tool outputs a message when the action is completed successfully and exit code 0 is returned. For example: > INFO [IndexControlHelper] - The server processed the command successfully: > Switched the live Match index from 'match_index1' to 'match_index2'. If there are any errors, the error is displayed in the console."
  },
  "versions/2.1.3/content/walkthroughs/update_schema.html": {
    "href": "versions/2.1.3/content/walkthroughs/update_schema.html",
    "title": "Updating the schema",
    "keywords": "Updating the schema This section describes an example of the process to update the schema of a deployment in a Docker environment. To update the schema in a deployment, you need to update the Information Store database and restart the application server. Updating the schema includes the following high-level steps: Removing Liberty containers Modifying the schema file Generating the database scripts to update the Information Store database Running the generated scripts against your Information Store database Running Liberty containers When you start Liberty, the schema that it caches from the database is updated Validating database consistency The updateSchemaWalkthrough.sh script is a worked example that demonstrates how to update the schema in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the schema in the Information Store, you remove the Liberty containers. To remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Modifying your schema In the updateSchemaWalkthrough.sh script, a modified schema.xml from the walkthrouhgs/configuration-changes directory is copied to the configuration/fragments/common/WEB-INF/classes/ directory. After you modify the configuration, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . See the Modifying the schema section of the walkthrough script. To modify your schema, use IBM i2 Analyze Schema Designer. For more information on installing Schema Designer please see: IBM i2 Analyze Schema Designer . Validating your schema After you modify the schema, you can use the validateSchemaAndSecuritySchema.sh tool to validate it. If the schema is invalid, errors are reported. See the Validating the schema section of the walkthrough script. The runi2AnalyzeTool client function is used to run the validateSchemaAndSecuritySchema.sh tool. runi2AnalyzeTool validateSchemaAndSecuritySchema Generating the database scripts After you modify and validate the schema, generate the database scripts that are used to update the Information Store database to reflect the change. See the Generating update schema scripts section of the walkthrough script. The runi2AnalyzeTool client function is used to run the generateUpdateSchemaScripts.sh tool. runi2AnalyzeTool generateUpdateSchemaScripts Running the generated scripts After you generate the scripts, run them against the Information Store database to update the database objects to represent the changes to the schema. See the Running the generated scripts section of the walkthrough script. The runSQLServerCommandAsDBA client function is used to run the runDatabaseScripts.sh tool. runSQLServerCommandAsDBA runDatabaseScripts After the database scripts are run, the Information Store database is updated with any changes to the schema. Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running. Validating database consistency After the system has started, the dbConsistencyCheckScript.sh tool is used to check the state of the database after the Information Store tables are modified. See the Validating database consistency section of the walkthrough script. The runi2AnalyzeTool client function is used to run the dbConsistencyCheckScript.sh tool. runi2AnalyzeTool dbConsistencyCheckScript"
  },
  "versions/2.1.3/content/walkthroughs/update_security_schema.html": {
    "href": "versions/2.1.3/content/walkthroughs/update_security_schema.html",
    "title": "Updating the security schema",
    "keywords": "Updating the security schema This section describes an example of the process to update the security schema of a deployment in a Docker environment. To update the security schema in a deployment, you need to update the Information Store database and restart the application server. Updating the security schema includes the following high-level steps: Removing Liberty containers Modifying the security schema file Starting Liberty containers When you start Liberty, the security schema that it caches from the database is updated Updating the Information Store Running the Liberty containers Validating database consistency The updateSecuritySchemaWalkthrough.sh script is a worked example that demonstrates how to update the security schema in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the security schema in the Information Store, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Modifying your security schema In the updateSecuritySchemaWalkthrough.sh script, the updateSecuritySchemaFile function copies a modified security-schema.xml from the walkthrouhgs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. For more information about the structure of the security schema, see The i2 Analyze security schema . The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . See the Modifying the security schema section of the walkthrough script. Validating your schema After you modify the security schema, you can use the validateSchemaAndSecuritySchema.sh tool to validate it. If the security schema is invalid, errors are reported. See the Validating the new security section of the walkthrough script. The runi2AnalyzeTool client function is used to run the validateSchemaAndSecuritySchema.sh tool. runi2AnalyzeTool validateSchemaAndSecuritySchema Updating the Information Store See the Updating the Information Store section of the walkthrough script. The runi2AnalyzeTool client function is used to run the updateSecuritySchema.sh tool. runi2AnalyzeTool updateSecuritySchema Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running. Validating database consistency After the system has started, the dbConsistencyCheckScript.sh tool is used to check the state of the database after the Information Store tables are modified. See the Validating database consistency section of the walkthrough script. The runi2AnalyzeTool client function is used to run the dbConsistencyCheckScript.sh tool. runi2AnalyzeTool dbConsistencyCheckScript"
  },
  "versions/2.1.3/content/walkthroughs/walkthroughs.html": {
    "href": "versions/2.1.3/content/walkthroughs/walkthroughs.html",
    "title": "Understanding the walkthroughs",
    "keywords": "Understanding the walkthroughs The walkthroughs are designed to demonstrate how you can configure and administer a deployment of i2 Analyze in a containerized environment. The walkthroughs consist of scripts that you can run and a document to provide further explanation. You can run the scripts against the reference architecture deployment. The walkthrough scripts use some of the server and client functions to reduce repetition. This is a pattern that can be copied and used in your own environment. The list of walkthroughs that are provided is: Adding a connector Adding a Solr node Updating the schema Updating the security schema Updating the i2 Analyze application configuration Updating the system match rules Ingesting data Ingesting correlated data Clearing data from a deployment"
  },
  "versions/2.1.3/index.html": {
    "href": "versions/2.1.3/index.html",
    "title": "i2 Analyze containerised deployment",
    "keywords": "i2 Analyze containerised deployment The analyze-containers repository provides a containerised configuration development environment that you can use to develop i2 Analyze configurations, and a reference architecture to demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. The repository is designed to be used with the i2 Analyze minimal toolkit. The minimal toolkit is similar to the standard i2 Analyze deployment toolkit, except that it only includes the minimum amount of application and configuration files. Configuration development environment The process to deploy i2 Analyze in production is iterative, and includes a number of phases and environments. The containerised configuration development is designed so that it is easy to move between these stages. Because the environment is containerised, the provided scripts manage the creation of any containers that are required by the environment. For example, the database management system container is added to your environment when you specify a deployment pattern that includes it. You do not need to manually create it first. The following features of containerised configuration development environment are designed to enable you to develop i2 Analyze configurations: Simplified configuration directory The configuration directory structure is flattened to make it easier to locate files The directory includes placeholder configuration files so that you can add to existing files The XSD files are included to provide intellisense in VS Code Simplified deployment You can switch between i2 Analyze deployment patterns by changing a single setting. Use a single deploy command to deploy and update a deployment regardless of the deployment pattern used or any changes to the configuration Manage multiple configurations Develop and store multiple configurations Switch between configurations by using the deploy command Backup the database associated with a configuration Add connectors to the environment The environment can build images and run containers for your connectors To start developing configurations, see Creating your configuration development environment . Containerised deployment reference architecture The repository includes Dockerfiles and example scripts that provide a reference architecture for creating a containerised deployment of i2 Analyze. The scripts demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. You can run the scripts to create a local example containerised deployment. For more information about the reference architecture, see Understanding the reference architecture ."
  },
  "versions/2.2.0/content/add_data_config_dev.html": {
    "href": "versions/2.2.0/content/add_data_config_dev.html",
    "title": "Add data to your config development environment",
    "keywords": "Add data to your config development environment Developing the configuration is separated into two parts: enabling i2 Analyze to work with your data, and defining how analysts interact with your data. First, update the DEPLOYMENT_PATTERN variable in your <config_name>/utils/variables.sh file to your intended deployment pattern. For example, to deploy with the i2 Connect gateway and the Information Store set the following value: DEPLOYMENT_PATTERN=\"i2c_istore\" Depending on the composition of your deployment, there are three methods that you can use to get your data into i2 Analyze for analysis. To develop the process for ingesting data into the Information Store, refer to Ingesting data into the Information Store . To develop connections to external data sources, refer to Connecting to external data sources . Ensure that analysts can import and create representative records in Analyst's Notebook Premium. Then, if required, upload records to the Information Store. For more information, see Import data and Create i2 Analyze chart items . When you develop the process to get your data into i2 Analyze, you might realize that your schema or security schema are not correct for your data. You can update the deployed schemas to better represent your data and security model. If you need to make destructive changes, change the DEPLOYMENT_PATTERN variable in your <config_name>/utils/variables.sh file to schema_dev and redeploy. Then complete the process of developing schemas . After you add data to your environment, you can configure the rest of the configuration ."
  },
  "versions/2.2.0/content/backup and restore/backup.html": {
    "href": "versions/2.2.0/content/backup and restore/backup.html",
    "title": "Backup",
    "keywords": "Backup This section describes the process to back up the Solr collections and the Information Store database in SQL Server in an i2 Analyze deployment in a containerized environment. Understanding the back up process In a deployment of i2 Analyze, data is stored in the Information Store database and indexed in Solr collections. You must back up these components as a pair to enable you to restore the data in your system after a failure. When you back up the Solr collections, the configuration in ZooKeeper is also backed up. To ensure that data can be restored correctly, you must back up the components in the following order: Solr collections Information Store database If data is changed in the database after taking the Solr backup, Solr can update the index to reflect these changes when the collections and database are restored. When you create your backups, ensure that you store both backups so that you can identify the pair of backups that you must restore if required. Note: In the walkthrough, both the database and Solr backups are versioned using the variable backup_version . In the walkthrough the backup version is 1 . This is used to create a directory where the all of the backup files are stored for this pair. To create another backup pair, increment the backup_version so that the backup files are stored in a different directory. Backing up the Solr collections To back up Solr collections, you must have a shared filesystem available that is shared and accessible by all Solr nodes. In a containerized environment, a backup volume is shared between all Solr containers. Note: In the example, the backup volume is mounted to /backup in the Solr container. To ensure Solr can write the backup file, you must make sure the Solr process has permission to the backup folder. In order for Solr to write to the folder the user solr and group 0 must have read and write permission to the backup folder. The following chown command is an example that gives the Solr process permission to write to the backup location: chown -R solr:0 /backup The runSolrClientCommand client function is used to run the BACKUP API request. The backup operation must be performed for each non-transient collection. The non-transient collections are the main_index , match_index , and chart_index . The following curl command is an example that creates a backup of the main_index collection: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert /tmp/i2acerts/CA.cer https://solr1:8983//solr/admin/collections?action=BACKUP&async=main_index_backup&name=main_index_backup&collection=main_index&location=/backup/1/ \" To perform a backup operation, use the Solr Collections API. For more information about the backup API command, see BACKUP: Backup Collection Note: The BACKUP API request must be an asynchronous call otherwise the backup procedure will timeout. This is done by adding async flag with a corresponding id to the curl command. In the above example, this is &async=main_index_backup . See the Backing up Solr section of the walkthrough script. Determining status of Solr backup The Monitoring Solr backup process section of the walkthrough script runs a loop around the getAsyncRequestStatus client function that reports the status of the Asynchronous backup request. For more information about the client function, see getAsyncRequestStatus . See the Monitoring Solr backup progress section of the walkthrough script. Backing up the system match rules file The system match rules are stored in the ZooKeeper configuration, and can be backed up by using the Solr ZooKeeper Command Line Interface (zkcli) from the Solr client container. The runSolrClientCommand client function is used to run the zkcli command. The following command is an example of how to use the getfile command to retrieve the system match rules: runSolrClientCommand \"/opt/solr/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd getfile /configs/match_index1/match_index1/app/match-rules.xml /backup/1/system-match-rules.xml Note: The system match rules file is backed up to the /backup/1/ folder where the Solr backup is located. For more information about the ZooKeeper command line utilities, see Using Solrâ€™s ZooKeeper CLI . Backing up the Information Store database In the containerized environment, the mounted backup volume is used to store the backup file. Only the Information Store database is backed up. The SQL Server instance and system databases are not included in the backup. For more information about the backup volume, see Running a SQL Server . The backup is performed by a user that has the built-in db_backupoperator SQL Server role. In this case, that is the dbb user. Use the runSQLServerCommandAsDBB client function to run the following SQL command to create the backup file. For example: runSQLServerCommandAsDBB bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"USE ISTORE; BACKUP DATABASE ISTORE TO DISK = '/backup/1/IStore.bak' WITH FORMAT;\\\"\" For more information about backing up a SQL Server database, see: Backup SQL Documentation SQL Server Backup Permissions For more information about the dbb user and it's role, see: db users See the Backing up SQL Server section of the walkthrough script."
  },
  "versions/2.2.0/content/backup and restore/br.html": {
    "href": "versions/2.2.0/content/backup and restore/br.html",
    "title": "Backup and restore",
    "keywords": "Backup and restore The documentation in this section describes how to back up and restore a deployment of i2 Analyze in a containerized environment."
  },
  "versions/2.2.0/content/backup and restore/restore.html": {
    "href": "versions/2.2.0/content/backup and restore/restore.html",
    "title": "Restore",
    "keywords": "Restore This section describes the process to restore the Solr collections and Information Store database in SQL Server of an i2 Analyze deployment in a containerized environment. Understanding the restore process To restore the data and indexes for a deployment of i2 Analyze, restore the pair of Solr collection and Information Store database backups. To ensure that the data is restored correctly, restore the pair of backups in the following order: Information Store database Solr collections If any data has changed in the Information Store after the Solr backup, the Solr index is updated to reflect the contents of the Information Store database when Liberty is started. Preparing the environment Before you can restore the backups, clean down the Solr, ZooKeeper & SQL Server environment if you are not using a clean environment. See the Simulating Clean down section of the walkthrough script. Stop the liberty servers Before the restore process can begin you must stop the liberty servers. In a this can be done by stopping the containers The following command is an example of how to stop the liberty containers: docker container stop liberty1 liberty2 Restoring the Information Store database The process of restoring the Information Store database contains the following steps: Running a SQL Server container with a new instance of SQL Server Creating the Information Store database from the backup file in the new instance Recreating the required logins in the new SQL Server instance, and the users in the restored database Running a SQL Server container Run a container for the new instance of SQL Server, this will have the backup directory available. For more information about running a SQL Server container, see SQL Server . See the Running the SQL Server section of the walkthrough script. Creating the database from the backup file In a new instance of SQL Server, the only user is the sa user. Use the runSQLServerCommandAsSA client function to run the following SQL command as the sa user to create the ISTORE database from the backup file: runSQLServerCommandAsSA bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"RESTORE DATABASE ISTORE FROM DISK = '/backup/IStore.bak;'\" See the Restoring the ISTORE database section of the walkthrough script. Recreating the logins and users In SQL Server, a login is scoped to the Database Engine. To connect to a specific database, in this case the ISTORE database, a login must be mapped to a database user. Because the backup is completed for the ISTORE database only, the logins from the previous SQL Server instance cannot be restored. Additionally, the database users are restored but there are no logins mapped to them. For more information about SQL Server logins, see SQL Server Login documentation In this environment, create the required logins and users by dropping the users from the database and recreating the logins and the users by using the createDbLoginAndUser client function. The logins and users are created in the same way as when original SQL Server instance was configured. For more information about creating the required logins, users, and permissions, see Configuring SQL Server in the deployment documentation. See the Dropping existing database users and Recreating database logins, users, and permissions sections of the walkthrough script. Restoring the Solr collections Restoring the solr indexes includes the following high-level steps: Deploy a new Solr cluster and ZooKeeper ensemble Restore the non-transient Solr collections Monitor the restore process until completed Recreate transient Solr collections Restore system match rules Deploy a new Solr cluster and ZooKeeper ensemble To restore the Solr indexes, deploy a new Solr cluster & ZooKeeper ensemble. For more information about running a clean ZooKeeper & Solr environment, see Running Solr and ZooKeeper . Create Solr cluster . See the Deploying Clean Solr & Zookeeper section of the walkthrough script. Restoring the non-transient Solr collections The runSolrClientCommand client function is used to run the RESTORE API request. The restore operation must be performed for each non-transient collection that was backed up. The following curl command is an example that restores the main_index collection: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert /tmp/i2acerts/CA.cer https://solr1:8983/solr/admin/collections?action=RESTORE&async=main_index_backup&name=main_index_backup&collection=main_index&location=/backup/1\" \" To perform a restore operation, use the Solr Collections API. For more information about the Restore API command, see RESTORE: Restore Collection . Note: The restore API request must be an asynchronous call otherwise the restore procedure will timeout. This is done by adding async flag with a corresponding id to the curl command. In the above example, this is &async=main_index_backup . See the Restoring non-transient Solr collection section of the walkthrough script. Determining completion of Solr restore procedure The Monitoring Solr restore process section of the walkthrough script runs a loop around the getAsyncRequestStatus client function that reports the status of the Asynchronous request. For more information about the client function, see getAsyncRequestStatus . See the Monitoring Solr restore progress section of the walkthrough script. Recreate transient Solr collections Recreate the transient daod_index , vq_index , and highlightquery_index Solr collections. For more information about the creating Solr collections, see Configuring Solr and ZooKeeper . See the Recreating transient Solr collections section of the walkthrough script. Restore system match rules After the indexes are restored, upload the system match rules file. Use the Solr ZooKeeper Command Line Interface (zkcli) to create the directory in ZooKeeper for the system match rules file and to upload it. The runSolrClientCommand client function is used to run the zkcli request. The following command creates the directory in ZooKeeper where the system match rules file must be stored: runSolrClientCommand \"/opt/solr/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd makepath /configs/match_index1/match_index1/app Note: The path to where the system match rules file must be located is in the following format configs/<index name>/<index name>/app The following command uploads the system match rules file to the directory in ZooKeeper created earlier: runSolrClientCommand \"/opt/solr/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd putfile /configs/match_index1/match_index1/app/match-rules.xml /backup/1/system-match-rules.xml For more information about the ZooKeeper command line utilities, see Using Solrâ€™s ZooKeeper CLI . See the Restoring system match rules section of the walkthrough script. Start the Liberty containers After the Solr collections and Information Store database are restored, start the Liberty containers. The following command is an example of how to start the Liberty containers: docker container start liberty1 liberty2 After the Liberty containers have started, the waitFori2AnalyzeServiceToBeLive common function ensures that the i2 Analyze service is running. See the Restart Liberty containers section of the walkthrough script."
  },
  "versions/2.2.0/content/connector_config_dev.html": {
    "href": "versions/2.2.0/content/connector_config_dev.html",
    "title": "Adding connectors to your development environment",
    "keywords": "Adding connectors to your development environment To add a connector to your deployment of i2 Analyze, you must develop an i2 Connect connector. For more information about developing connectors, see i2 Connect gateway connector development . Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to a pattern that includes the i2 Connect gateway. For example: DEPLOYMENT_PATTERN=\"i2c_istore\" Process overview: Create a custom connector image Update the configuration to reference connectors Build connector images and redeploy Creating a connector image Create a connector image from one of the image templates. You can deploy connectors using the following templates: Spring Boot for Java based connectors i2 Connect server for Node v14 based connectors developed using the i2 Connect server SDK NodeJS for Node v14 based connectors External for connectors hosted outside of the config development environment The connector should be secured and run on port 3443 . For more information about securing your system, see Securing i2 Analyze . Spring Boot based connector image Copy the /templates/springboot-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the springboot-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - target - connector-definition.json - connector-version.json - ... Copy your connector code into the target directory of the connector directory. For example, copy your .jar file into the <connector_name>/target directory. Next, configure your connector . i2 Connect server based connector image Copy the /templates/i2connect-server-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the i2connect-server-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - app - connector-definition.json - connector-version.json - ... Copy the i2 Connect server connector distributable ( the .tar.gz or .tgz file) into the connector directory. Next, configure your connector . NodeJS based connector image Copy the /templates/node-connector-image directory (including the sub-directories) to the /connector-images directory. Rename the node-connector-image folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - app - connector-definition.json - connector-version.json - ... Copy your connector code into the app directory of the connector directory. For example, copy the files into the <connector_name>/app directory. By default, the image will start node with the file app.js . Next, configure your connector . External connector Copy the /templates/external-connector directory to the /connector-images directory. Rename the external-connector folder to a custom connector name. The name of the directory is used to determine which connectors to deploy. Do not use spaces or special characters in the name. The directory structure is: - connector-images - <connector_name> - connector-definition.json Next, configure your connector . Configuring a connector Populate the values in the <connector_name>/connector-definition.json file with information about your connector. Key Description id An identifier that is unique for all connectors that will be deployed. name A name for the connector that is displayed to users in the client. configurationPath The full path to the configuration endpoint of the connector. By default, it is /config . gatewaySchema The short name of an optional gateway schema. When no gateway schema is used, do not provide a value. type Used to identify the type of connector. For i2 Connect server connectors, set to i2connect-server . For external connectors, set to external . For any other connectors, the type key is not required. sendSensitiveHeaders This setting effectively disables connectors that employ user-specific configuration. baseUrl Used only for connectors of type external . The baseUrl value is the URL address of the connector. For any other connectors, the baseUrl key is not required. For example: { \"id\": \"connector1\", \"name\": \"Connector 1\", \"description\": \"First connector\", \"configurationPath\": \"/config\", \"gatewaySchema\": \"template-schema\", \"sendSensitiveHeaders\": \"false\", \"type\": \"i2connect-server\" } or { \"id\": \"connector2\", \"name\": \"Connector 2\", \"description\": \"Second connector\", \"configurationPath\": \"/config\", \"gatewaySchema\": \"template-schema\", \"sendSensitiveHeaders\": \"false\", \"type\": \"external\", \"baseUrl\": \"https://example-connector:3443\" } Note: If your external connector is hosted in a Docker container on a different network to your config development environment, connect the Liberty container to the same network as your external connector container. For example, run docker network connect <connector-network-name> liberty1.<config-name> . If your connector is running on your host machine, use the IP address of your host machine in the baseUrl value. Provide the version of the connector in the <connector_name>/connector-version.json file. For external connectors, this file is not required. The version value specifies the version of the connector. For example: { \"version\": \"0.0.1\", \"tag\": \"0-0-1\" } For more information about configuring connectors, see Managing connectors . Adding connectors to a config In the configuration you want to deploy your connector with, update the /configs/<config_name>/configuration/connector-references.json file and add the directory name for your connector in the connectors array. For example, to add the connector in connector-images\\example-connector the connector-references.json file contains the following connectors array: { \"connectors\": [ { \"name\": \"example-connector\" } ], ... } Defining a gateway schema (Optional) You can develop the gateway schema and associated charting schemes by changing the deployment pattern to schema-dev and following the instructions in Developing schemas in your configuration development environment . After you develop the schemes, complete the following steps to add deploy them with your configuration: Copy the schema and charting scheme files to the /gateway-schemas directory. To define the short name of the gateway schema, the prefix of the file name is used. The convention is defined as: <short>-<name>-schema.xml and <short>-<name>-schema-charting-scheme.xml . For example, files with the names law-enforcement-schema.xml and law-enforcement-charting-schemes.xml have the short name law-enforcement . Add the short name of your schema to the /configs/<config_name>/configuration/connector-references.json file in the gatewaySchemas array. For example, to add a gateway schema with the short name law-enforcement , the file contains the following gatewaySchemas array: { ... ], \"gatewaySchemas\": [ { \"shortName\": \"law-enforcement\" } ] } Note: If you are not using any gateway schemas, do not remove the gatewaySchemas key from the file. Set the short name of your schema in the <connector_name>/connector-definition.json as the value for the gatewaySchema key of the connector that will use the gateway schema. Building the connector images and redeploying Run the ./deploy.sh script with your config name and the connectors task from the /scripts directory to generate the secrets for your connectors, build the connector images, and deploy your environment. ./deploy.sh -c <config_name> -t connectors After you add connectors to your environment, you can configure the rest of the configuration ."
  },
  "versions/2.2.0/content/deploy_config_dev.html": {
    "href": "versions/2.2.0/content/deploy_config_dev.html",
    "title": "Configuration development environment",
    "keywords": "Configuration development environment To create a production deployment of i2 Analyze, you must develop the i2 Analyze configuration. The production deployment process describes a process of using different development environments to produce your i2 Analyze configuration. For more information about the environments, see Deployment phases and environments . The analyze-containers repository enables you to create a development environment that comprises a containerized deployment of i2 Analyze. The containerized deployment enables you to switch between deployment patterns easily and redeploy changes in a consistent manner. Process overview: Install the prerequisites for the analyze-containers repository Create a config template Create a custom config that you can deploy into a development environment Use the environment to develop the i2 Analyze configuration If you have already run the createDevEnvironment script and you want to create a new custom config from the template, go to Creating a development config . Prerequisites Before you create the configuration development environment, you must configure the analyze-containers repository. For more information, see Getting started with the Analyze-Containers repository . Creating the configuration development environment To create the configuration development environment and template config, in the /scripts directory run: ./createDevEnvironment.sh The script performs the following actions: Extracts the required files from the i2 Analyze deployment toolkit Builds the required Docker images for the development environment Generates the secrets that are used in the environment Creates the configuration template The configuration template is located in /templates/config-development . Note: At this release, the config development environment is supported with Microsoft SQL Server only. The createDevEnvironment.sh script creates a number of IBM Db2 artifacts, however they cannot be used with the config development environment. Accepting the licenses Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the utils/simulatedExternalVariables.sh script. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variables are in the utils/simulatedExternalVariables.sh script. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y Modifying the hosts file To enable you to connect to a deployment, the Solr Web UI, and the database, update your hosts file to include entries for the containers: 127.0.0.1 solr1.eia 127.0.0.1 sqlserver.eia 127.0.0.1 i2analyze.eia On Windows, edit the hosts file in C:\\Windows\\System32\\drivers\\etc and the WSL hosts file in /etc/hosts . You can use the integrated terminal in VS Code to edit the WSL hosts file. On MacOS, edit the hosts file in /etc/hosts . You must run Analyst's Notebook Premium on Windows. If you deploy on MacOS, you can use a Windows virtual machine to host Analyst's Notebook Premium. For your virtual machine to connect to i2 Analyze, complete the following: On your MacOS terminal, run ifconfig and identify the IP address for your virtual machine in a section such as vmnet1 . For example, 172.16.100.1 . Then, on your Windows virtual machine add the following line to the C:\\Windows\\System32\\drivers\\etc\\hosts file: 172.16.100.1 i2analyze.eia Installing the certificate To access the system, the server that you are connecting from must trust the certificate that it receives from the deployment. To enable trust, install the /dev-environment-secrets/generated-secrets/certificates/externalCA/CA.cer certificate as a trusted root certificate authority in your browser and operating system's certificate store. For information about installing the certificate, see: Install Certificates with the Microsoft Management Console Add certificates to a keychain using Keychain Access on Mac Creating a config Before you can deploy the environment, create a config from the template: Copy the config-development directory (including the sub-directories) from /templates to the /configs directory. You can rename the config-development folder to a custom config name or use the default value. The name of the directory is used to determine which config to deploy, it is recommended to use a short name with no spaces, special characters, or capital letters. The directory structure is: - configs - <config_name> - configuration - database-scripts - utils The /configs/<config_name>/utils/variables.sh file contains variables that are used when you deploy the environment with this config. You can specify values for the following variables: The DEPLOYMENT_PATTERN variable specifies the deployment pattern for the deployment of i2 Analyze in the environment. You can specify: schema_dev - An i2 Connect gateway only deployment that you can use to develop your data and security models istore - Information Store i2c - i2 Connect gateway only cstore - Chart Store i2c_istore - i2 Connect gateway and Information Store i2c_cstore - i2 Connect gateway and Chart Store For more information about the i2 Analyze deployment patterns, see: Components . The following variables specify the ports that are exposed to the host machine for each component of i2 Analyze. Each variable has a default value: HOST_PORT_SOLR specifies the port that you use to connect to the Solr Web UI from the host machine. By default, 8983 . HOST_PORT_DB specifies the port that you use to connect to the database from the host machine. By default, 1433 . HOST_PORT_I2ANALYZE_SERVICE specifies the port that you use to connect to the deployment from the host machine. By default, 9443 . For example, to deploy with the i2 Connect gateway only for schema development and to use the default ports: DEPLOYMENT_PATTERN=\"schema_dev\" HOST_PORT_SOLR=\"8983\" HOST_PORT_DB=\"1433\" HOST_PORT_I2ANALYZE_SERVICE=\"9443\" Specifying the schema and charting schemes files To deploy i2 Analyze, you must provide a schema and charting scheme. The i2 Analyze toolkit includes example files that you can use as a starting point, or you can use existing files. For more information about the example schemas, see Example schemas . Choose an example schema and associated charting scheme as your starting point from the pre-reqs/i2analyze/toolkit/examples/schemas directory. Copy the chosen schema and charting scheme to your configs/<config_name>/configuration directory. Remove the existing schema.xml and schema-charting-schemes.xml files. Then, rename your chosen schema file to schema.xml and your charting scheme to schema-charting-schemes.xml . Specifying the security schema, user registry, and command access control files To connect to an i2 Analyze deployment, you must provide a security schema and user registry. To control access to features and enable to use of the administrator endpoints, provide a command access control file. The i2 Analyze toolkit includes example files that you can use as a starting point. For more information about the example files, see Example schemas . Copy and overwrite the example security-schema.xml , user-registry.xml , and command-access-control.xml files from the pre-reqs/i2analyze/toolkit/examples/security directory to your configs/<config_name>/configuration directory. Deploying your config in the environment Run the ./deploy.sh script from the /scripts directory to deploy and start your environment. Provide the name of the config to deploy with by using the -c parameter and specify the name of the config directory. For example: ./deploy.sh -c config-development You can run only one environment at a time. When you run the deploy.sh script, it stops any containers on the Docker network. If you are deploying with the same config, it update the images and recreates the containers to update the environment with any configuration changes. For more information, see Managing configs . Accessing the system To connect to the deployment, the URL to use is: https://i2analyze.eia:9443/opal Note: If you are using the schema_dev or i2c deployment patterns, you must connect using Analyst's Notebook Premium. Log in as a user that is specified in the user registry file. If you are using the example user registry, the example user has the following credentials: The user name is Jenny The password is Jenny What to do next? After you deploy your environment, you can start to develop the i2 Analyze configuration. Using your environment for configuration development: Developing schemas in your configuration development environment After you develop your schema files, you can develop the rest of the i2 Analyze configuration. Developing the rest of the configuration"
  },
  "versions/2.2.0/content/develop_config_dev.html": {
    "href": "versions/2.2.0/content/develop_config_dev.html",
    "title": "Developing the rest of the configuration",
    "keywords": "Developing the rest of the configuration After you enable i2 Analyze to work with your data, you can define how analysts interact with your data. To do this, use the following information to modify the i2 Analyze configuration to meet your requirements. Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to your intended deployment pattern. For example, to deploy with the i2 Connect gateway and the Information Store set the following value: DEPLOYMENT_PATTERN=\"i2c_istore\" Note: In the analyze-containers repository, the structure of the configuration directory is modified. The configuration files are in the top level configs/<config_name>/configuration directory. You cannot run any toolkit tasks described in the linked documentation in this environment. Instead, after you modify any configuration files, run the deploy.sh script with your config name as you did when you created your deployment. The list of things that you can configure includes: To configure how analysts search for information, including Quick Search, Visual Query, and Highlight Query, refer to Configuring search To configure how analysts can identify matching records, refer to Configuring matching To configure the geospatial mapping options available to analysts, refer to Configuring geospatial mapping To configure the values that analysts can provide in source references, refer to Configuring source references To configure item type access for analysts, refer to Configuring item type security To update the Solr index configuration, refer to Configuring the Solr index For more information about other configuration changes that you can make, see Configuring i2 Analyze ."
  },
  "versions/2.2.0/content/develop_extensions.html": {
    "href": "versions/2.2.0/content/develop_extensions.html",
    "title": "Developing and deploying extensions for i2 Analyze",
    "keywords": "Developing and deploying extensions for i2 Analyze i2 Analyze Developer Essentials contains tools, libraries, and examples that enable development and deployment of custom extensions to i2 Analyze. The following information describes how you can use the config development environment to quickly deploy an extension for a config to test and develop. For more information about the types of extension that are available, see i2 Analyze Developer Essentials . Prerequisites Install the Java Extension Pack in VS Code. Ensure to have the correct JDK configured: Run the Java Configuration Wizard by pressing F1 and typing Java: Configure Java Runtime . Install temurin 11 JDK if it isn't already installed in your machine. Install Maven. For more information about installing Maven, see Installing Apache Maven . Modify the VS Code settings.json file: Open the settings.json file by pressing F1 and typing Preferences: Open Settings (JSON) . Add the classpath information to deploy extensions. In the settings.json file , add the following object: \"java.project.referencedLibraries\": [ \"pre-reqs/i2analyze/toolkit/application/shared/lib/*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/disco-api-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/solr-core-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/solr-solrj-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/Daod.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/httpclient-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/httpcore-*.jar\", \"pre-reqs/i2analyze/toolkit/application/targets/opal-services/WEB-INF/lib/ApolloLegacy.jar\" ] Creating a project for your extension You must create a Maven artifact to develop your extension in. Copy the template templates/extension-development directory to the i2a-extensions directory and rename it to be the name of your extension. This name is also the artifact id. For example, name the directory opal-default-security-example . The directory structure is: - i2a-extensions - <artifact_id> - src - main - java - test - java - pom.xml Update the pom.xml file inside your new directory with information about your extension. The elements to modify are at the root of the file. Update the values of the following elements: <groupId> is the name of the Java package <artifactId> matches the name of the directory for your extension <version> is the version number for your extension For example: <groupId>com.i2group</groupId> <artifactId>opal-default-security-example</artifactId> <version>1.0.0</version> You can now start developing your extension. For examples on i2 Analyze extensions, check the examples in i2 Analyze Developer Essentials . Copy the contents of src/main folder of the extension into the i2a-extensions/<artifact_id>/src/main directory. Add any settings that are required by the extension to the analyze-settings.properties file. Add any resources that are required by the extension to the /configs/<config_name>/configuration directory. For example, the group-based-default-security-dimension-values.xml file. If your extension depends on another extension. You must add it to your pom.xml file under the comment: <!-- Extension specific dependencies --> . For example, the auditing extensions in Developer Essentials depend on the opal-audit-example-common extension. To add it as a dependency, populate the values of the pom.xml file as follows: <!-- Extension specific dependencies --> <dependency> <groupId>com.i2group</groupId> <artifactId>opal-audit-example-common</artifactId> <version>1.0.0</version> </dependency> Adding extensions to a config In the configuration you want to deploy your extension with, update the /configs/<config_name>/configuration/extension-references.json file and add the directory name for your extension in the extensions array. For example, to add the extension in i2a-extensions/opal-default-security-example the extension-references.json file contains the following extensions array: { \"extensions\": [ { \"name\": \"opal-default-security-example\", \"version\": \"1.0.0\" } ] } Note: The extensions in the extension-references.json file are loaded in order. If an extension depends on another extension, define the dependent extension first. Deploying your extension Use the deploy.sh script with the extensions task to build the extensions for your configuration. For example, to build and deploy the extensions declared in extension-references.json for your configuration: ./deploy.sh -c <config_name> -t extensions If you do not want to build and run all of the declared extensions, you can use the -i and -e options to include or exclude extensions from the process. For example, to build and run only the extensions named opal-default-security-example and opal-default-security-example-2 : ./deploy.sh -c <config_name> -t extensions -i opal-default-security-example -i opal-default-security-example-2 Debugging your extension Deploying in debug mode To deploy with one or more Liberty servers running in debug mode, change the local variable DEBUG_LIBERTY_SERVERS in utils/internalHelperVariables.sh to be a list of the container names of the liberty servers you wish to run in debug mode. For example: DEBUG_LIBERTY_SERVERS=(\"${LIBERTY1_CONTAINER_NAME}\") Then deploy from a clean environment. When the Liberty servers start they all wait for a debugger to be attached before continuing. Note: Each Liberty server will open and expose a unique debug port (starting at port 7777) and wait for a debugger. This behavior is defined by the WLP_DEBUG_SUSPEND environment variable in the liberty container and could be changed if needed. Attaching a debugger The project is shipped with a default launch configuration which can be changed in .vscode/launch.json . Run the Java debugger by pressing F5 and choose Debug Extensions if asked."
  },
  "versions/2.2.0/content/develop_schemas.html": {
    "href": "versions/2.2.0/content/develop_schemas.html",
    "title": "Developing schemas in your configuration development environment",
    "keywords": "Developing schemas in your configuration development environment To reduce the time that it takes to update the environment with your schema changes, use the schema_dev deployment pattern. Note: In the analyze-containers repository, the structure of the configuration directory is modified. The configuration files are in the top level configs/<config_name>/configuration directory. You cannot run any toolkit tasks described in the linked documentation in this environment. Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to schema_dev . For example: DEPLOYMENT_PATTERN=\"schema_dev\" Developing the data model In i2 Analyze Schema Designer, modify the schema.xml and schema-charting-scheme.xml files in the <config_name>\\configuration directory with your changes. For more information about developing your schema, see Updating the schema and the charting scheme . After you make your changes, run the deploy.sh script again with your config name. Test your changes in Analyst's Notebook Premium. Note: You might need to log out of Analyst's Notebook Premium and log back in to see your changes. Developing security model Update the security-schema.xml file in the <config_name>\\configuration directory with your changes. For more information about developing your security schema, see Configuring the security schema . After you make your changes, run the deploy.sh script again with your config name. Update the user-registry.xml file to align to your security schema changes. For more information about updating the user registry, see Configuring the Liberty user registry . Note: In this release, you must ensure that your user registry file contains the user Jenny in the Administrator group. After you make your changes, run the deploy.sh script again with your config name. Update the command-access-control.xml file to align to your users and groups. For more information about controlling access to features, see The command access control file . After you make your changes, run the deploy.sh script again with your config name. Test your changes in Analyst's Notebook Premium. Note: You might need to log out of Analyst's Notebook Premium and log back in to see your changes. After you develop your schemas, Add data to your config development environment ."
  },
  "versions/2.2.0/content/develop_solr_config_dev.html": {
    "href": "versions/2.2.0/content/develop_solr_config_dev.html",
    "title": "Configuring the Solr index",
    "keywords": "Configuring the Solr index The way that Solr indexes data can be configured using the files in the /configs/<config_name>/configuration/solr directory. The deployed Solr configuration is built from the template schema-template-definition.xml file and other files referenced within it. Any reference files must also be in the configuration/solr directory. For more information about the changes you can make to the schema-template-definition.xml file, see Configuring search . Note: The process you complete to update a deployment with the Solr configuration is different when the deployment contains data. If the deployment does not contain data, you clean the deployment and then update the deployment in the usual way If the deployment contains data, the process requires you to back up and restore the database Updating a deployment without data After you change the Solr configuration, clean the deployment by running the deploy.sh script with the clean task. For example: ./deploy.sh -c <config_name> -t clean For more information about the script, see The deploy.sh script . Then, re-deploy in the usual way. For example: ./deploy.sh -c <config_name> Your deployment is running with the updated Solr configuration. Updating a deployment with data When data is present in the system you must use the backup and restore tasks of the deploy script to update the deployment. For more information about these tasks, refer to refer to Back up and restore a development database . After you change the Solr configuration, backup the deployment. For example: ./deploy.sh -c <config_name> -t backup -b <backup_name> Then, restore from the backup that you created. For example: ./deploy.sh -c <config_name> -t restore -b <backup_name> Solr is updated with your configuration, and the data in the database is reindexed when the system starts."
  },
  "versions/2.2.0/content/getting_started.html": {
    "href": "versions/2.2.0/content/getting_started.html",
    "title": "Getting started with the analyze-containers repository",
    "keywords": "Getting started with the analyze-containers repository Prerequisites You can run the code in the analyze-containers repository on Windows with Windows Subsystem for Linux and MacOS. Windows Subsystem for Linux (WSL) If you are on Windows, you must use WSL 2 to run the shell scripts in this repository. For information about installing WSL, see Windows Subsystem for Linux Installation Guide . Code Download the tar.gz or clone the analyze-containers repository from https://github.com/i2group/analyze-containers . If you download the tar.gz file, extract it. On Windows, extract it to a location that is in your WSL file system. For example: \\\\wsl$\\Ubuntu-20.04\\home\\<user-name> . Docker Install Docker CE for your operating system. For more information about installing Docker CE, see https://docs.docker.com/engine/installation/ . Mac OS : Install Docker CE Windows : Install Docker CE Set up Docker on WSL 2 After you install Docker, you must allocate enough memory to Docker to run the containers in the example deployment. For this deployment, allocate at least 5GB of memory. For more information about modifying the resources allocated to Docker, see: Docker Desktop for Windows Docker Desktop for Mac Command line tools If you are using WSL, install and configure the following command line tools: jq XMLStarlet For example: sudo apt-get update sudo apt-get install jq sudo apt-get install xmlstarlet If you are using MacOS, install and configure the following command line tools: GNU sed GNU bash Coreutils - GNU core utilities jq XMLStarlet You can install these command line tools by using Homebrew . Run the following commands to install Homebrew and then install the tools with it: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" brew install gnu-sed bash coreutils jq xmlstarlet After you install the tools, ensure they are added to your shell's path. If you installed the tools by using Homebrew, run the following commands: echo 'export PATH=\"$(brew --prefix)/opt/gnu-sed/libexec/gnubin:$PATH\"' >> ~/.bash_profile echo 'export PATH=\"$(brew --prefix)/opt/bash/bin:$PATH\"' >> ~/.bash_profile echo 'export PATH=\"$(brew --prefix)/opt/coreutils/libexec/gnubin:$PATH\"' >> ~/.bash_profile Visual Studio Code The repository is designed to be used with VS Code to create the development environment. Download and install VS Code To make the environment easier to use, install the following extensions in VS Code: Remote - WSL Docker ShellCheck Bash IDE Red Hat XML Red Hat YAML Modify the VS Code settings.json file: Open the settings.json file by pressing F1 and typing Preferences: Open Settings (JSON) . Add the catalog.xml to enable XSD validation. In the settings.json file , add the following JSON object and reference the catalog.xml file at the root of the analyze-containers directory: \"xml.catalogs\": [ \"catalog.xml\" ] If you are using MacOS, ensure the VS Code uses GNU bash for the integrated terminal. In the settings.json file, add the following code: \"terminal.integrated.profiles.osx\": { \"bash\": { \"path\": \"/usr/local/opt/bash/bin/bash\", \"icon\": \"terminal-bash\" } }, \"terminal.integrated.defaultProfile.osx\": \"bash\", To run the scripts in the analyze-containers repository, use the VS Code integrated terminal. Set the default terminal to WSL. Press F1 and type Terminal: Select Default Profile . Select WSL . To open the integrated terminal, click Terminal > New Terminal . You do not need to run any scripts now, but whenever the documentation instructs you to run a script, do so from the integrated terminal. To set the ANALYZE_CONTAINERS_ROOT_DIR variable, run the following command: . initShell.sh i2 Analyze minimal toolkit Download the i2 Analyze V4.3.5 Minimal for Linux. To download i2 Analyze, follow the procedure described in Where can I download the latest i2 Products? Populate the subject of the form with Request for i2 Analyze 4.3.5 minimal toolkit for Linux . Rename the i2analyzeMinimal_4350.tar.gz file to i2analyzeMinimal.tar.gz , then copy it to the analyze-containers/pre-reqs directory. Note: If you used the analyze-containers repository with a previous version of i2 Analyze, overwrite the existing minimal toolkit. i2 Analyze Fix Packs (optional) If there is a Fix Pack available for the version of i2Analyze, download it and rename the file to i2analyzeFixPack.tar.gz , then copy it to the analyze-containers/pre-reqs directory. Note: If you used the analyze-containers repository with a previous fix pack version of i2 Analyze, overwrite the existing fix pack file. Analyst's Notebook Premium Download i2 Analyst's Notebook Premium version 9.3.2. To download i2 Analyst's Notebook Premium, follow the procedure described in Where can I download the latest i2 Products? Populate the subject of the form with Request for i2 Analyst's Notebook Premium 9.3.2 . Install i2 Analyst's Notebook Premium on a Windows machine. Note: If you are running Docker on MacOS, you can install Analyst's Notebook Premium on a Windows virtual machine. For more information, see Installing i2 Analyst's Notebook Premium . JDBC drivers You must provide the JDBC driver to enable the application to communicate with the database. Download the Microsoft JDBC Driver 9.4.1 for SQL Server from https://github.com/microsoft/mssql-jdbc/releases/tag/v9.4.1 by clicking the mssql-jdbc-9.4.1.jre11.jar asset. Copy the mssql-jdbc-9.4.1.jre11.jar file to the analyze-containers/pre-reqs/jdbc-drivers directory. What to do next Create and use a development environment to develop an i2 Analyze configuration. For more information, see Configuration development environment . Create an example pre-production deployment that is used to demonstrate how i2 Analyze can be deployed in a distributed cloud environment. For more information, see Pre-production example environment . To understand how the containerized environment is created, you can review the documentation that explains the images, containers, tools, and functions: Images and containers Tools and functions"
  },
  "versions/2.2.0/content/HA walkthroughs/ha.html": {
    "href": "versions/2.2.0/content/HA walkthroughs/ha.html",
    "title": "High availability walkthroughs",
    "keywords": "High availability walkthroughs The high availability walkthroughs are designed to demonstrate how a deployment of i2 Analyze responds to container failure in a containerized environment."
  },
  "versions/2.2.0/content/HA walkthroughs/liberty_failure.html": {
    "href": "versions/2.2.0/content/HA walkthroughs/liberty_failure.html",
    "title": "Failure of the leader Liberty container",
    "keywords": "Failure of the leader Liberty container This section demonstrates how a deployment of i2 Analyze responds to the failure and recovery of the Liberty server that hosts the leader Liberty instance. This section also describes the messages that you should monitor to detect the failure and ensure that the recovery was successful. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How Liberty is deployed for high availability and the role of the Liberty leader. For more information about Liberty configuration, see Liberty . How a load balancer is used and configured in a deployment of i2 Analyze. For more information about the load balancer configuration, see Deploying a load balancer . That the load balancer is used to monitor the status of the i2 Analyze service. In a containerized deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The libertyHadrFailureWalkthrough.sh script simulates the Liberty leader failure. Identifying the leader Liberty To simulate a failure of the leader Liberty, first identify which server hosts the leader instance. To identify which container is running the leader Liberty, check the logs of the Liberty servers for the message: We are the Liberty leader . In the walkthrough script, this is done with the following grep command: grep -q \"We are the Liberty leader\" See the Identifying the leader Liberty section of the walkthrough script. Simulating leader Liberty failure To simulate leader Liberty failure, stop the leader Liberty. For example, if liberty1 is the leader, run: docker stop liberty1 See the Simulating leader Liberty failure section of the walkthrough script. Detecting failure The load balancer is used to monitor and determine the status of the i2 Analyze service. The load balancer is configured to report the status of the deployment. The status can be either ACTIVE , DEGRADED , or DOWN . When the leader is taken offline, the other Liberty server must restart to become the new leader. During this time, both servers are down and the i2 Analyze service is DOWN . When the new leader Liberty starts and only 1 of the servers is down, the status of the i2 Analyze services is DEGRADED . In the walkthrough, the waitFori2AnalyzeServiceStatus function is used to run a while loop around the geti2AnalyzeServiceStatus function to wait until the i2 Analyze service is in the DEGRADED state. The geti2AnalyzeServiceStatus function is an example of how to return the i2 Analyze service status from a load balancer. See the Detecting failure section of the walkthrough script. Fail over When the Liberty leader fails, one of the remaining Liberty servers is elected as the leader. To identify the new Liberty leader, check the logs of the remaining Liberty servers for the message: We are the Liberty leader . See the Fail over section of the walkthrough script. Reinstating high availability To reinstate high availability to the deployment, restart the failed Liberty server. In this example, that restart the Liberty container by running the following command: docker start liberty1 When both Liberty servers are up, the status of the i2 Analyze services is ACTIVE . In the walkthrough, the waitFori2AnalyzeServiceStatus function is used to run a while loop around the geti2AnalyzeServiceStatus function to wait until the i2 Analyze service is in the ACTIVE state. The geti2AnalyzeServiceStatus function is an example of how to return the i2 Analyze service status from a load balancer. The recovered Liberty server is in the non-leader mode when it starts because the new leader has already been elected while the server was unavailable. To determine it is in the non-leader mode, the following message is displayed in the logs: We are not the Liberty leader . See the Reinstating high availability section of the walkthrough script."
  },
  "versions/2.2.0/content/HA walkthroughs/solr_cluster_failure.html": {
    "href": "versions/2.2.0/content/HA walkthroughs/solr_cluster_failure.html",
    "title": "Failure of the Solr cluster",
    "keywords": "Failure of the Solr cluster This section demonstrates how a deployment of i2 Analyze responds to the failure of all Solr nodes. This section also describes how to monitor and detect failure and ensure the recovery was successful. Before you begin the walkthrough, there a number of concepts that it is useful to understand: How Solr is deployed for high availability. For more information, see Solr . How the Solr status is reported in the component availability log in Liberty. For more information, see Monitor the system availability . In a containerized deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The solrClusterFailureWalkthrough.sh scripts simulates a Solr cluster failure and recovery. Simulating Solr Cluster failure To simulate the cluster failure, remove all the Solr containers. For example, run: docker stop solr2 solr1 See the Simulating the cluster failure section of the walkthrough script. Detecting failure The component availability log in Liberty is used to monitor and determine the status of the Solr cluster. When the Solr cluster is unavailable, the status is reported as DOWN . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'DOWN'\" Reinstating high availability To reinstate high availability, restart the failed Solr containers. In this example, restart both Solr containers by running the following command: docker start solr2 solr1 After the failed nodes recover, Liberty reports the changes to the cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. The reinstating high availability section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is active: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'ALL_REPLICAS_ACTIVE'\""
  },
  "versions/2.2.0/content/HA walkthroughs/solr_node_failure.html": {
    "href": "versions/2.2.0/content/HA walkthroughs/solr_node_failure.html",
    "title": "Failure of a Solr node",
    "keywords": "Failure of a Solr node This walkthrough demonstrates losing a Solr node from a collection, and describes how to identify failure, continue operations, and reinstate high availability with your Solr nodes. Before you begin the walkthrough, there a number of concepts that it is useful to understand: How Solr is deployed for high availability. For more information, see Solr . How the Solr status is reported in the component availability log in Liberty. For more information, see Monitor the system availability . In a containerized deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. The solrNodeFailureWalkthrough.sh script demonstrates how to monitor the Liberty logs to identify Solr node failure and recovery. In the example, each shard has 2 replicas and 1 replica is located on each Solr node. This means that the Solr cluster can continue to process requests when one Solr node is taken offline. Simulating Solr node failure To simulate a node failure, one of the Solr containers is stopped in the Stop the Solr container section. For example: docker stop solr2 See the Simulating the cluster failure section of the walkthrough script. Detecting failure The component availability log in Liberty is used to monitor and determine the status of the Solr cluster. When the Solr Node is unavailable, the status is reported as DEGRADED . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'DEGRADED'\" Reinstating high availability To reinstate high availability, restart the failed Solr containers. In this example, restart the Solr by running the following command: docker start solr1 After the failed node recovers, Liberty reports the changes to the cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. The reinstating high availability section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is active: grep \"^.*\\[I2AVAILABILITY] .* SolrHealthStatusLogger - '.*', .*'ALL_REPLICAS_ACTIVE'\""
  },
  "versions/2.2.0/content/HA walkthroughs/zookeeper_quorum_failure.html": {
    "href": "versions/2.2.0/content/HA walkthroughs/zookeeper_quorum_failure.html",
    "title": "Loss of the ZooKeeper quorum",
    "keywords": "Loss of the ZooKeeper quorum This walkthrough demonstrates losing more than 50% of the ZooKeeper servers from an ensemble, which causes the loss of the quorum. It also describes how to identify failure, and reinstate high availability with your ZooKeeper ensemble. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How ZooKeeper is deployed for high availability and how the ensemble functions when one or more ZooKeeper servers fail. For more information, see Configuring ZooKeeper for HADR . When the ZooKeeper quorum is lost, the Solr cluster also fails. To monitor that whether the ZooKeeper quorum is met or not the component availability log in Liberty is used. The status of Solr is used to determine the status of ZooKeeper. In a containerized deployment, all log messages are also displayed in the console. You can use the docker logs command to view the console log. Simulating loss of quorum To simulate a loss of quorum, more than 50% of the ZooKeeper servers must be stopped. For example, to stop zk1 and zk2 , run: docker stop zk1 zk2 See the Simulating Zookeeper Quorum failure section of the walkthrough script. Detecting failure When the ZooKeeper quorum is lost, the Solr cluster also fails. The Solr status is reported as DOWN . For more information about losing Solr, see Failure of the Solr cluster . The detecting failure section of the walkthrough script runs a loop around the getSolrStatus client function that reports the status of Solr. For more information about the function, see getSolrStatus . The function uses a grep command for the following message that indicates Solr is down: grep -q \"^.*[com.i2group.apollo.common.toolkit.internal.ConsoleLogger] - (opal-services) - '.*', .*'DOWN'\" For more information see the Detecting failure section of the walkthrough script. Reinstating high availability To reinstate high availability to the deployment, restart the failed ZooKeeper servers. In this example, restart the ZooKeeper containers by running the following command: docker start zk1 zk2 When enough ZooKeeper servers are up to achieve at least a DEGRADED quorum, the status of the i2 Analyze services is ACTIVE and Liberty reports the changes to the Solr cluster status. To ensure the collections recover, monitor the Liberty logs for healthy collections. See the Reinstating high availability section of the walkthrough script."
  },
  "versions/2.2.0/content/HA walkthroughs/zookeeper_server_failure.html": {
    "href": "versions/2.2.0/content/HA walkthroughs/zookeeper_server_failure.html",
    "title": "Failure of a ZooKeeper server",
    "keywords": "Failure of a ZooKeeper server This walkthrough demonstrates losing a single ZooKeeper server from an ensemble, and describes how to identify failure, continue operations, and reinstate high availability with your ZooKeeper ensemble. Before you begin the walkthrough, there are a number of concepts that it is useful to understand: How ZooKeeper is deployed for high availability. For more information, see Multi-server setup . The ZooKeeper AdminServer is used to monitor the status of the ZooKeeper ensemble The AdminServer . In the zookeeperServerFailureWalkthrough.sh script demonstrates stopping one of the ZooKeeper containers, monitoring the status, and reinstating high availability. Simulating a server failure To simulate a server failure in the ensemble, one of the ZooKeeper servers is stopped. For example, to stop zk1 , run: docker stop zk1 See the Simulating ZooKeeper server failure section of the walkthrough script. Detecting failure When one ZooKeeper server goes offline, the other servers can still make a quorum and remain active. Because the ensemble can sustain only one more server failure, the state is defined as DEGRADED . In the walkthrough, the getZkQuorumEnsembleStatus function is used to monitor and determine the ensemble status by calling the commands/srvr resource on each ZooKeeper servers's admin endpoint and reports the status as DEGRADED when one of the servers is unavailable. See the Detecting failure section of the walkthrough script. Restoring high availability To restore high availability to the ensemble, restart the failed ZooKeeper server. In this example, restart the ZooKeeper container by running the following command: docker start zk1 When the ZooKeeper server is up again, the status of the ensemble is ACTIVE . In the walkthrough, the getZkQuorumEnsembleStatus function is used again to determine the ensemble status. See the Reinstating high availability section of the walkthrough script."
  },
  "versions/2.2.0/content/images and containers/etl_client.html": {
    "href": "versions/2.2.0/content/images and containers/etl_client.html",
    "title": "ETL Client",
    "keywords": "ETL Client An ETL Client container is an ephemeral container that is used to run ETL tasks. Building an ETL Client image The ETL Client image is built from the Dockerfile in images/etl_client . It uses the i2 Analyze Tools image as the base image. Docker build command The following docker build command builds the ETL Client image: docker build -t \"etlclient_redhat:4.3.5\" \"/images/etl_client\" \\ --build-arg USER_UID=\"$(id -u \"${USER}\")\" \\ --build-arg BASE_IMAGE=\"i2a_tools_redhat:4.3.5\" The --build-arg flag is used to provide your local user ID to the Docker image when it is built. The value of $USER comes from your shell. For examples of the build commands, see the buildImages.sh script. Running an ETL Client container A ETL Client container uses the ETL Client image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs the ETL Client container: docker run --rm \\ --name \"etl_client\" \\ --network \"eia\" \\ --user \"$(id -u \"${USER}\"):$(id -u \"${USER}\")\" \\ -v \"/examples/pre-prod/configuration/logs:/opt/configuration/logs\" \\ -v \"/prereqs/i2analyze/toolkit/examples/data:/var/i2a-data\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_NAME=\"ISTORE\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_OS_TYPE=\"UNIX\" \\ -e DB_INSTALL_DIR=\"/opt/mssql-tools\" \\ -e DB_LOCATION_DIR=\"/var/opt/mssql/data\" \\ -e ETL_TOOLKIT_JAVA_HOME=\"/opt/java/openjdk\" \\ -e DB_USERNAME=\"i2etl\" \\ -e DB_PASSWORD=\"DB_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"etlclient_image:4.3.5\" \"$@\" For an example of the docker run command, see runEtlToolkitToolAsi2ETL function in the clientFunctions.sh script. For an example of how to use runEtlToolkitToolAsi2ETL function, see runEtlToolkitToolAsi2ETL . Environment variables The ETl Client is built on top of the SQL Client. Any environment variables referenced in the SQL Client can be used in the ETL Client. Additional Environment variables Environment Variable Description DB_DIALECT The database dialect. Currently only sqlserver is supported DB_OS_TYPE The Operating System that the database is on. Can be UNIX , WIN , or AIX . DB_INSTALL_DIR Specifies the database CMD location. DB_LOCATION_DIR Specifies the location of the database. ETL_TOOLKIT_JAVA_HOME Specifies the location on Java. Useful links Defining an ingestion source"
  },
  "versions/2.2.0/content/images and containers/i2analyze_tool.html": {
    "href": "versions/2.2.0/content/images and containers/i2analyze_tool.html",
    "title": "i2 Analyze Tool",
    "keywords": "i2 Analyze Tool An i2 Analyze Tool container is an ephemeral container that is used to run the i2 Analyze tools. For more information about the tools, see i2 Analyze tools . Building the i2 Analyze Tool image The i2 Analyze Tool image is built from the Dockerfile in images/i2a_tools . The image contains the i2-tools & scripts folder from the toolkit. Docker build command The following docker build command builds the i2 Analyze Tool image: docker image build -t i2a_tools_redhat:4.3.5 images/i2a_tools \\ --build-arg USER_UID=\"$(id -u \"${USER}\")\" The --build-arg flag is used to provide your local user ID to the Docker image when it is built. The value of $USER comes from your shell. The local user ID is required so that a user is created in the Docker container with the same user ID as the local user. The user is required to ensure that the local user can access any files that are generated on the container and mounted to the host via a bind mount. For examples of the build commands, see buildImages.sh script. Running an i2 Analyze Tool container An i2 Analyze Tool container uses the i2 Analyze Tool image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs an i2 Analyze Tool container: docker run \\ --rm \\ --name \"i2atool\" \\ --network \"eia\" \\ --user \"$(id -u \"${USER}\"):$(id -u \"${USER}\")\" \\ -v \"/examples/pre-prod/configuration:/opt/configuration\" \\ -v \"/examples/pre-prod/database-scripts/generated:/opt/databaseScripts/generated\" \\ -e LIC_AGREEMENT=\"ACCEPT\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_NAME=\"ISTORE\" \\ -e CONFIG_DIR=\"/opt/configuration\" \\ -e GENERATED_DIR=\"/opt/databaseScripts/generated\" \\ -e DB_USERNAME=\"dba\" \\ -e DB_PASSWORD=\"DBA_PASSWORD\" \\ -e DB_OS_TYPE=\"UNIX\" \\ -e DB_INSTALL_DIR=\"/opt/mssql-tools\" \\ -e DB_LOCATION_DIR=\"/var/opt/mssql/data\" \\ -e SOLR_ADMIN_DIGEST_USERNAME=\"solr\" \\ -e SOLR_ADMIN_DIGEST_PASSWORD=\"SOLR_ADMIN_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD=\"ZOO_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD=\"ZOO_DIGEST_READONLY_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SSL_PRIVATE_KEY=\"SSL_PRIVATE_KEY\" \\ -e SSL_CERTIFICATE=\"SSL_CERTIFICATE\" \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"i2a_tools_redhat:4.3.5\" \"$@\" Bind mounts Configuration : The i2 Analyze Tool container requires the i2 Analyze configuration. To access the configuration, the configuration directory must be mounted into the container. The CONFIG_DIR environment variable must specify the location where the configuration is mounted. Generated scripts directory : Some of the i2 Analyze tools generate scripts to be run against the Information Store database, or run scripts that were generated by other i2 Analyze tools. For the i2 Analyze Tool container to interact with these scripts, the directory where they are generated must be mounted into the container. In the example scripts, this is defaulted to /database-scripts/generated . The GENERATED_DIR environment variable must specify the location where the generated scripts are mounted. This directory must be created with your local user before mounting it to the container, otherwise docker will create this directory as a root. Environment variables To configure the i2 Analyze Tool container, you provide environment variables to the Docker container in the docker run command. The following table describes the supported environment variables that you can use: Environment variable Description LIC_AGREEMENT The license agreement for use the i2Analyze tools. ZK_HOST The connection string the ZooKeeper client to connect to. DB_DIALECT See i2a Tools Environment Variables . DB_SERVER See i2a Tools Environment Variables . DB_PORT See i2a Tools Environment Variables . DB_NAME See i2a Tools Environment Variables . CONFIG_DIR The root location of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. CLASSES_PATH The location to the files required by the i2 Analyze tools. The files are built into the image. DB_USERNAME See i2a Tools Environment Variables . DB_PASSWORD See i2a Tools Environment Variables . SOLR_ADMIN_DIGEST_USERNAME The name of the administrator user for performing administration tasks. SOLR_ADMIN_DIGEST_PASSWORD The password for the administrator user. ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. The following environment variables enable you to use SSL: Environment variable Description SERVER_SSL See Secure Environment variables . DB_SSL_CONNECTION See Secure Environment variables . SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SSL_PRIVATE_KEY See Secure Environment variables . SSL_CERTIFICATE See Secure Environment variables . SSL_CA_CERTIFICATE See Secure Environment variables . GATEWAY_SSL_CONNECTION See Secure Environment variables . SSL_OUTBOUND_PRIVATE_KEY See Secure Environment variables . SSL_OUTBOUND_CERTIFICATE See Secure Environment variables . NOTE: when you set SERVER_SSL and SSL_CA_CERTIFICATE environment variables, the CA.cer certificate will be located at /tmp/i2acerts/CA.cer . when you set GATEWAY_SSL_CONNECTION , SSL_OUTBOUND_PRIVATE_KEY and SSL_OUTBOUND_CERTIFICATE environment variables, the certificated will be located at /tmp/i2acerts/i2Analyze.pem ."
  },
  "versions/2.2.0/content/images and containers/images.html": {
    "href": "versions/2.2.0/content/images and containers/images.html",
    "title": "Images and containers",
    "keywords": "Images and containers The documentation in this section describes the images and containers that are required for a deployment of i2 Analyze in a containerized environment."
  },
  "versions/2.2.0/content/images and containers/liberty.html": {
    "href": "versions/2.2.0/content/images and containers/liberty.html",
    "title": "Liberty",
    "keywords": "Liberty In a containerized deployment, you configure the i2 Analyze application and Liberty in an image that is layered on top of the liberty_ubi_base image. The liberty_ubi_base contains static configuration and application jars that are required by i2 Analyze and should not be changed. Configuring the Liberty server Liberty is configured by exception. The runtime environment operates from a set of built-in configuration default settings, and you only need to specify configuration that overrides those default settings. You do this by editing either the server.xml file or another XML file that is included in server.xml at run time. In a containerized deployment of i2 Analyze, a server.xml file is provided for you. To provide or modify any values, you specify a number of environment variables when you run a Liberty container. Additionally, you can extend the server.xml by using the provided server.extensions.xml in the i2 Analyze configuration. Any elements that you add to the extensions file are included in the server.xml when you run a Liberty container. Configuring the i2 Analyze application The contents of the configuration directory must be copied into the images/liberty_ubi_combined/classes directory. The contents of the classes directory is added to the configured Liberty image when the image is built. If you make changes to the configuration, you must copy the changes to the classes directory and rebuild the configured image. Note: The system match rules are configured differently. The application is updated to use the system-match-rules.xml from the Solr client command line. For more information about updating the system match rules, see Updating the system match rules . Building a configured Liberty image The configured image is built from the base image. The configured image contains the i2 Analyze application and Liberty configuration that is required to start the i2 Analyze application. When you change the configuration, the configured image must be rebuilt to reflect the changes. Docker build command The configured image is built from the Dockerfile in images/liberty_ubi_combined . The following docker build command builds the configured image: docker build -t \"liberty_configured_redhat:4.3.5\" images/liberty_ubi_combined --build-arg BASE_IMAGE=\"liberty_redhat:4.3.5\" An example of providing the configuration to the classes directory and building the image is included in the buildLibertyConfiguredImage function in the serverFunctions.sh script. Running a Liberty container A Liberty container uses the configured image. In the docker run command, you can use -e to pass environment variables to Liberty on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Liberty container: docker run -m 1g -d \\ --name \"liberty1\" \\ --network \"eia\" \\ --net-alias \"liberty1.eia\" \\ -p \"9045:9443\" \\ -v \"liberty1_secrets:/run/secrets\" \\ -v \"liberty1_data:/data\" \\ -e LICENSE=\"accept\" \\ -e FRONT_END_URI=\"https://liberty.eia:9045/opal\" \\ -e DB_DIALECT=\"sqlserver\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_USERNAME=\"i2analyze\" \\ -e DB_PASSWORD_FILE=\"/run/secrets/DB_PASSWORD\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_PASSWORD\" \\ -e SOLR_HTTP_BASIC_AUTH_USER=\"liberty\" \\sp -e SOLR_HTTP_BASIC_AUTH_PASSWORD_FILE=\"/run/secrets/SOLR_APPLICATION_DIGEST_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ -e SSL_CA_CERTIFICATE_FILE=\"/run/secrets/CA.cer\" \\ -e GATEWAY_SSL_CONNECTION=true \\ -e SSL_OUTBOUND_PRIVATE_KEY_FILE=\"/run/secrets/gateway_user.key\" \\ -e SSL_OUTBOUND_CERTIFICATE_FILE=\"/run/secrets/gateway_user.cer\" \\ -e SSL_OUTBOUND_CA_CERTIFICATE_FILE=\"/run/secrets/outbound_CA.cer\" \\ -e LIBERTY_HADR_MODE=1 \\ -e LIBERTY_HADR_POLL_INTERVAL=1 \\ liberty_configured_redhat For an example of the docker run command, see serverFunctions.sh . The runLiberty function takes the following arguments to support running multiple Liberty containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container and the Solr host. VOLUME - the name for the named volume of the Liberty container. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. KEY_FOLDER - The folder with keys and certificates for the container. For more information, see Security . An example of running Liberty container by using runLiberty function: runLiberty liberty1 liberty1.eia liberty1_data 9045 liberty1 Volumes A named volume or a bind mount can be used to persist data, which is generated and used in the Liberty container, outside of the container. To configure the Liberty container to use the volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For Liberty, the directory that is mounted must be /data , this directory folder stores: jobs, record groups and charts. For example: -v liberty_data:/data \\ -v liberty1_secrets:/run/secrets Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access ZooKeeper, the database, and the certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure the Liberty server, you provide environment variables to the Docker container in the docker run command. The following table describes the supported environment variables that you can use: Environment variable Description FRONT_END_URI The URI that clients use to connect to i2 Analyze. For more information, see Specifying the connection URI . DB_DIALECT Specifies which database management system to configure i2 Analyze for. In this release, it can be set to sqlserver . For more information, see properties.microsoft.sqlserver . DB_SERVER Specifies the fully qualified domain name of the database server to connect to. The value populates the serverName attribute in the Liberty server configuration. For more information, see properties.microsoft.sqlserver . DB_PORT Specifies the port number of the SQL Server database to connect to. The value populates the portNumber attribute in the Liberty server configuration. You can specify DB_PORT or DB_INSTANCE . For more information, see properties.microsoft.sqlserver . DB_USERNAME The database user that is used by Liberty to connect to the database. DB_PASSWORD The database user password. ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated. The connection string must be in the following format: <hostname>:<port>,<hostname>:<port> . SOLR_HTTP_BASIC_AUTH_USER The Solr user that Liberty uses to connect to Solr. This is not an administrator user. SOLR_HTTP_BASIC_AUTH_PASSWORD The Solr user password. ZOO_DIGEST_USERNAME The ZooKeeper user that is used by Liberty to connect to ZooKeeper. ZOO_DIGEST_PASSWORD The ZooKeeper user password. The following environment variables enable you to use SSL: Environment variable Description DB_SSL_CONNECTION See Secure Environment variables . SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . GATEWAY_SSL_CONNECTION See Secure Environment variables . SSL_OUTBOUND_PRIVATE_KEY_FILE See Secure Environment variables . SSL_OUTBOUND_CERTIFICATE_FILE See Secure Environment variables . SSL_OUTBOUND_CA_CERTIFICATE_FILE See Secure Environment variables . Liberty HADR You can run Liberty in an active/active configuration with multiple Liberty containers. In an active/active configuration, multiple instance of the i2 Analyze application run concurrently on multiple Liberty containers. One instance of the i2 Analyze application is determined to be the leader at any given time. The following tables describes the environment variables that you can use to configure HADR: Environment variable Description LIBERTY_HADR_MODE Can be set to 1 or 0. If set to 1, Liberty starts in HADR mode. The default is 0. LIBERTY_HADR_POLL_INTERVAL The interval in minutes to poll for liberty leadership status. The default is 5. LIBERTY_HADR_MAX_ERRORS The maximum number of errors allowed before Liberty initiates a leadership poll. The time span in which the errors can occur is determined by LIBERTY_HADR_ERROR_TIME_SPAN . The default is 5. LIBERTY_HADR_ERROR_TIME_SPAN The time span in seconds for the LIBERTY_HADR_MAX_ERRORS to occur within. The default is 30. <!-- markdownlint-configure-file { \"MD013\": false } -->"
  },
  "versions/2.2.0/content/images and containers/solr.html": {
    "href": "versions/2.2.0/content/images and containers/solr.html",
    "title": "Solr",
    "keywords": "Solr In a containerized deployment, Solr is configured and run from a Solr image. Configuring Solr Solr is configured by the solr.xml file. A default solr.xml configuration is generated by the Solr container. To modify the solr.xml , you can modify the Dockerfile to build an image with your solr.xml file. The Dockerfile in images/solr_redhat contains a commented out example COPY command that copies a solr.xml into the image. By copying a solr.xml file into the image, the container does not generate a default solr.xml . For more information about the file, see Format of solr.xml . Building a Solr image The Solr image for i2 Analyze is built from a Dockerfile that is based on the Dockerfile from Apache Solr. The Dockerfile is modified to configure Solr for use with i2 Analyze. For more information about the Dockerfile provided by Apache Solr, see docker-solr . Docker build command The Solr image is built from the Dockerfile in images/solr_redhat . The following docker build command builds the Solr image: docker build -t \"solr_redhat:4.3.5\" images/solr_redhat For examples of the build commands, see buildImages.sh script. Running a Solr container A Solr container uses the Solr image. In the docker run command, you can use -e to pass environment variables to Solr on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Solr server container: docker run -d \\ --name \"sol1\" \\ --net \"eia\" \\ --net-alias \"solr1.eia\" \\ --init \\ -p 8983:8983 \\ -v \"solr1_data:/var/solr\" \\ -v \"solr1_secrets:/run/secrets\" \\ -e ZK_HOST=\"zk1.eia:2281,zk2.eia:2281,zk3.eia:2281\" \\ -e SOLR_HOST=\"solr1.eia\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD_FILE=\"/run/secrets/ZK_DIGEST_READONLY_PASSWORD\" \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ -e SSL_CA_CERTIFICATE_FILE=\"/run/secrets/CA.cer\" \\ \"solr_redhat:4.3.5\" For an example of the docker run command, see serverFunctions.sh . The runSolr function takes the following arguments to support running multiple Solr containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container and the Solr host. VOLUME - The name for the named volume of the Solr container. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. An example of running Solr container by using runSolr function: runSolr solr1 solr1.eia solr1_data 8983 Volumes A named volume or a bind mount can be used to persist data and logs that are generated and used in the Solr container, as well as a separate volume for backups, outside of the container. To configure the Solr container to use the volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For Solr, the directory that must be mounted is /var/solr . For example: -v solr1_data:/var/solr \\ -v solr_backup:/backup \\ -v solr1_secrets:/run/secrets A unique volume name must be used for each Solr container. For more information, see How the image works . Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure Solr, you can provide environment variables to the Docker container in the docker run command. Solr environment variables The following table describes the supported environment variables that you can use for Solr: Environment variable Description SOLR_HOST Specifies the fully qualified domain name of the Solr container. ZooKeeper authentication The following environment variables are used to configure Solr to connect to ZooKeeper as a client: Environment variable Description ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated list. The connection string must be in the following format: <hostname>:<port>,<hostname>:<port> . ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. For more information about configuring Solr to connect to ZooKeeper, see: Client Configuration Parameters . ZooKeeper Access Control Solr SSL The following environment variables enable you use SSl with Solr Environment variable Description SOLR_ZOO_SSL_CONNECTION See below. SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . For more information about the Solr SSL configuration, see Set Common SSL-Related System Properties . For more information about the ZK SSL configuration, see Client Configuration Parameters . Solr Environment Variable Mapping The above environment variables are either passed through to the standard Solr launch script or used to construct the following Solr environment variables. For exact details see the Docker image. SOLR_ZK_CREDS_AND_ACLS SOLR_OPTS"
  },
  "versions/2.2.0/content/images and containers/solr_client.html": {
    "href": "versions/2.2.0/content/images and containers/solr_client.html",
    "title": "Solr Client",
    "keywords": "Solr Client A Solr Client container is an ephemeral container that is used to run Solr commands. Building a Solr Client container The Solr Client uses the same image as the Solr Server container. For more information about building the Solr image, see Solr . Running a Solr Client container A Solr Client container uses the Solr image. In the docker run command, you can use -e to pass environment variables to Solr on the container. The environment variables are described in environment variables For more information about the command, see docker run reference . Docker run command The following docker run command runs a Solr Client container: docker run --rm \\ --net \"eia\" \\ -v \"/examples/pre-prod/configuration:/opt/configuration\" \\ -e SOLR_ADMIN_DIGEST_USERNAME=\"solr\" \\ -e SOLR_ADMIN_DIGEST_PASSWORD=\"SOLR_ADMIN_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_USERNAME=\"solr\" \\ -e ZOO_DIGEST_PASSWORD=\"ZOO_DIGEST_PASSWORD\" \\ -e ZOO_DIGEST_READONLY_USERNAME=\"readonly-user\" \\ -e ZOO_DIGEST_READONLY_PASSWORD=\"ZOO_DIGEST_READONLY_PASSWORD\" \\ -e SECURITY_JSON=\"SECURITY_JSON\" \\ -e SOLR_ZOO_SSL_CONNECTION=true \\ -e SSL_PRIVATE_KEY=\"SSL_PRIVATE_KEY\" \\ -e SSL_CERTIFICATE=\"SSL_CERTIFICATE\" \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"solr_redhat:4.3.5\" \"$@\" For an example of the docker run command, see runSolrClientCommand function in clientFunctions.sh script. For an example of how to use runSolrClientCommand function, see runSolrClientCommand . Bind mounts Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as ZOO_DIGEST_USERNAME_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Configuration : The Solr client requires the i2 Analyze configuration to perform some Solr operations. To access the configuration, the configuration directory must be mounted into the container. Environment variables To configure the Solr client, you can provide environment variables to the Docker container in the docker run command. Environment variable Description SOLR_ADMIN_DIGEST_USERNAME For usage see Command Parsing SOLR_ADMIN_DIGEST_PASSWORD For usage see Command Parsing ZOO_DIGEST_USERNAME The ZooKeeper administrator user name. This environment variable maps to the zkDigestUsername system property. ZOO_DIGEST_PASSWORD The ZooKeeper administrator password. This environment variable maps to the zkDigestPassword system property. ZOO_DIGEST_READONLY_USERNAME The ZooKeeper read-only user name. This environment variable maps to the zkDigestReadonlyUsername system property. ZOO_DIGEST_READONLY_PASSWORD The ZooKeeper read-only password. This environment variable maps to the zkDigestReadonlyPassword system property. SECURITY_JSON The Solr security.json. Solr Basic Authentication SOLR_ZOO_SSL_CONNECTION See Secure Environment Variables . SERVER_SSL See Secure Environment Variables . SSL_PRIVATE_KEY See Secure Environment Variables . SSL_CERTIFICATE See Secure Environment Variables . SSL_CA_CERTIFICATE See Secure Environment Variables . Command parsing When commands are passed to the Solr client by using the \"$@\" notation, the command that is passed to the container must be escaped correctly. On the container, the command is run using docker exec \"$@\" . Because the command is passed to the docker run command using bash -c , the command must be maintained as a double quoted string. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\\\" -H Content-Type:text/xml --data-binary \\\"<delete><query>*:*</query></delete>\\\"\" Different parts of the command must be escaped in different ways: \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" Because the curl command uses the container's local environment variables to obtain the values of SOLR_ADMIN_DIGEST_USERNAME and SOLR_ADMIN_DIGEST_PASSWORD , the $ is escaped by a \\ . The \" around both of the variables are escaped with a \\ to prevent the splitting of the command, which means that the variables are evaluated in the container's environment. \\\"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\\\" The URL is surrounded in \" because the string contains a variable. The \" are escaped with a \\ . Because the SOLR1_FQDN variable is evaluated before it is passed to the container, the $ is not escaped. \\\"<delete><query>*:*</query></delete>\\\" The data portion of the curl command is escaped with \" because it contains special characters. The \" are escaped with a \\ ."
  },
  "versions/2.2.0/content/images and containers/sql_client.html": {
    "href": "versions/2.2.0/content/images and containers/sql_client.html",
    "title": "SQL Server Client",
    "keywords": "SQL Server Client An SQL Server Client container is an ephemeral container that is used to run the sqlcmd commands to create and configure the database. Building an SQL Server Client image The SQL Server Client is built from a Dockerfile that is based on Microsoft SQL Server . The SQL Server Client image is built from the Dockerfile in images/sql_client . Docker build command The following docker build command builds the SQL Server Client image: docker build -t \"sqlserver_client_redhat:4.3.5\" images/sql_client Running a SQL Server Client container An SQL Server Client container uses the SQL Server Client image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command runs a SQL Server Client container: docker run \\ --rm \\ --name \"sqlclient\" \\ --network \"eia\" \\ -v \"pre-reqs/i2analyze/toolkit:/opt/toolkit\" \\ -v \"/examples/pre-prod/database-scripts/generated:/opt/databaseScripts/generated\" \\ -e SQLCMD=\"/opt/mssql-tools/bin/sqlcmd\" \\ -e SQLCMD_FLAGS=\"-N -b\" \\ -e DB_SERVER=\"sqlserver.eia\" \\ -e DB_PORT=1433 \\ -e DB_NAME=\"ISTORE\" \\ -e GENERATED_DIR=\"/opt/databaseScripts/generated\" \\ -e DB_USERNAME=\"dba\" \\ -e DB_PASSWORD=\"DBA_PASSWORD\" \\ -e DB_SSL_CONNECTION=true \\ -e SSL_CA_CERTIFICATE=\"SSL_CA_CERTIFICATE\" \\ \"sqlserver_client_redhat:4.3.5\" \"$@\" For an example of the docker run command, see runSQLServerCommandAsETL function in clientFunctions.sh script. For an example of how to use runSQLServerCommandAsETL function, see runSQLServerCommandAsETL . Note: you can run SQL Server Client container as different users, see runSQLServerCommandAsDBA , runSQLServerCommandAsSA Bind mounts Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access ZooKeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_CA_CERTIFICATE_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Toolkit : For the SQL Server Client to use the tools in /opt/toolkit/i2-tools/scripts , the toolkit must be mounted into the container. In the example scripts, this is defaulted to /opt/toolkit . Generated scripts directory : Some of the i2 Analyze tools generate scripts to be run against the Information Store database. For the SQL Server Client to run these scripts, the directory where they are generated must be mounted into the container. In the example scripts, this is defaulted to /database-scripts/generated . The GENERATED_DIR environment variable must specify the location where the generated scripts are mounted. Environment variables Environment Variable Description DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_USERNAME The user. DB_PASSWORD The password. GENERATED_DIR The root location where any generated scripts are created. The following environment variables enable you use SSL Environment variable Description DB_SSL_CONNECTION See Secure Environment variables . SSL_CA_CERTIFICATE See Secure Environment variables . Command parsing When commands are passed to the Solr client by using the \"$@\" notation, the command that is passed to the container must be escaped correctly. On the container, the command is run using docker exec \"$@\" . Because the command is passed to the docker run command using bash -c , the command must be maintained as a double quoted string. For example: runSQLServerCommandAsETL bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -S \\${DB_SERVER},${DB_PORT} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} -Q \\\"BULK INSERT ${STAGING_SCHEMA}.${table_name} FROM '/var/i2a-data/${BASE_DATA}/${csv_and_format_file_name}.csv' WITH (FORMATFILE = '/var/i2a-data/${BASE_DATA}/sqlserver/format-files/${csv_and_format_file_name}.fmt', FIRSTROW = 2)\\\"\" Different parts of the command must be escaped in different ways: \\${DB_SERVER} , \\${DB_USERNAME} , \\${DB_PASSWORD} , and \\${DB_NAME} Because the command uses the container's local environment variables to obtain the values of these variables, the $ is escaped by a \\ . ${DB_PORT} is not escaped because this is an environment variable available to the script calling the client function. \\\"BULK INSERT ${STAGING_SCHEMA}.${table_name} ... FIRSTROW = 2)\\\" The string value for the -Q argument must be surrounded by \" when it is run on the container. The surrounding \" are escaped with \\ . The variables that are not escaped in the string are evaluated outside of the container when the function is called."
  },
  "versions/2.2.0/content/images and containers/sql_server.html": {
    "href": "versions/2.2.0/content/images and containers/sql_server.html",
    "title": "SQL Server",
    "keywords": "SQL Server In a containerized deployment, the database is located on a SQL Server container. Building a SQL Server image SQL Server is built from a Dockerfile that is based on the Dockerfile from Microsoft SQL Server . The SQL Server image is built from the Dockerfile in images/sql_server . Docker build command The following docker build command builds the SQL Server image: docker build -t \"sqlserver_redhat:4.3.5\" images/sqlserver For examples of the build commands, see buildImages.sh script. Running a SQL Server container A SQL Server container uses the SQL Server image. In the docker run command, you can use -e to pass environment variables to the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command runs a SQL Server container: docker run -d \\ --name \"sqlserver\" \\ --network \"eia\" \\ --net-alias \"sqlserver.eia\" \\ -p \"1433:1433\" \\ -v \"sqlserver_data:/var/opt/mssql\" \\ -v \"sqlserver_sqlbackup:/backup\" \\ -v \"sqlserver_secrets:/run/secrets/\" \\ -v \"i2a_data:/var/i2a-data\" \\ -e ACCEPT_EULA=\"Y\" \\ -e MSSQL_AGENT_ENABLED=true \\ -e MSSQL_PID=\"Developer\" \\ -e SA_PASSWORD_FILE=\"/run/secrets/SA_PASSWORD_FILE\" \\ -e SERVER_SSL=true \\ -e SSL_PRIVATE_KEY_FILE=\"/run/secrets/server.key\" \\ -e SSL_CERTIFICATE_FILE=\"/run/secrets/server.cer\" \\ \"sqlserver_redhat:4.3.5\" For an example of the docker run command, see serverFunctions.sh . The runSQLServer does not take any arguments. Volumes Named volumes are used to persist data and logs that are generated and used in the SQL Server container, as well as a separate volume for backups, outside of the container. Note: It is good practice to have a separate volume for the backup from the database storage. For more information, see SQL Server Backup best practices . To configure the SQL Server container to use these volumes, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For SQL Server, the path to the directory that must be mounted is /var/opt/mssql . For example: -v sqlvolume:/var/opt/mssql -v sqlserver_sqlbackup:/backup For more information, see Use Data Volume Containers . Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is mounted to a location in the container defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_PRIVATE_KEY_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. This is achieved by using a bind mount. In production this would not be required. Example Data : To demonstrate ingesting data into the Information Store, the i2 Analyze toolkit is mounted to /var/i2a-data in the container. Environment variables Environment Variable Description ACCEPT_EULA Set to Y to confirm your acceptance of the End-User Licensing Agreement . MSSQL_AGENT_ENABLED For more information see Configure SQL Server settings with environment variables on Linux MSSQL_PID For more information see Configure SQL Server settings with environment variables on Linux SA_PASSWORD The administrator user's password. The following environment variables enable you to use SSL: Environment variable Description SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . For more information about the SSL in SQLServer, see Specify TLS settings ."
  },
  "versions/2.2.0/content/images and containers/zookeeper.html": {
    "href": "versions/2.2.0/content/images and containers/zookeeper.html",
    "title": "ZooKeeper",
    "keywords": "ZooKeeper The ZooKeeper image for i2 Analyze is built from a Dockerfile that is based on the Dockerfile from Apache ZooKeeper. The Dockerfile is modified to configure ZooKeeper for use with i2 Analyze. Building a ZooKeeper image The ZooKeeper image is built from the Dockerfile located in images/zookeeper_redhat . Docker build command The The following docker build command builds the ZooKeeper image: docker build -t \"zookeeper_redhat:4.3.5\" images/zookeeper_redhat Running a ZooKeeper container A ZooKeeper container uses the ZooKeeper image. In the docker run command, you can use -e to pass environment variables to ZooKeeper on the container. The environment variables are described in environment variables . For more information about the command, see docker run reference . Docker run command The following docker run command starts a ZooKeeper container: docker run --restart always -d \\ --name \"zk1\" \\ --net \"eia\" \\ --net-alias \"zk1.eia\" \\ -p \"8080:8080\" \\ -p \"2181:2181\" \\ -p \"2281:2281\" \\ -p \"3888:3888\" \\ -p \"2888:2888\" \\ -v \"zk1_data:/data\" \\ -v \"zk1_datalog:/datalog\" \\ -v \"zk1_logs:/logs\" \\ -v \"zk1_secrets:/run/secrets\" \\ -e \"ZOO_SERVERS=server.1=zk1.eia:2888:3888 server.2=zk2.eia:2888:3888 server.3=zk3.eia:2888:3888\" \\ -e \"ZOO_MY_ID=1\" \\ -e \"ZOO_SECURE_CLIENT_PORT=2281\" \\ -e \"ZOO_CLIENT_PORT=2181\" \\ -e \"ZOO_4LW_COMMANDS_WHITELIST=ruok, mntr, conf\" \\ -e \"SERVER_SSL=true\" \\ -e \"SSL_PRIVATE_KEY_FILE=/run/secrets/server.key\" \\ -e \"SSL_CERTIFICATE_FILE=/run/secrets/server.cer\" \\ -e \"SSL_CA_CERTIFICATE_FILE=/run/secrets/CA.cer\" \\ \"zookeeper_redhat:4.3.5\" Note: SERVER_SSL variable is set based on the SOLR_ZOO_SSL_CONNECTION switch, see Environment variables . ZooKeeper Service Ports Default ports used by ZooKeeper are: 8080 - By default, the server is started on port 8080, and commands are issued by going to the URL \"/commands/[command name]\", e.g., http://localhost:8080/commands/stat . 2181 - The port at which the clients will connect (non-secure). This is defined by setting ZOO_CLIENT_PORT . 2281 - The port at which the clients will connect (secure). This is defined by setting ZOO_SECURE_CLIENT_PORT . 3888 - Port used by ZooKeeper peers to talk to each other. 2888 - Port used by ZooKeeper peers to talk to each other. For more information, see ZooKeeper Service Ports . For an example of the docker run command, see serverFunctions.sh . The runZK function takes the following arguments to support running multiple ZooKeeper containers: CONTAINER - The name for the container. FQDN - The fully qualified domain name for the container. DATA_VOLUME - The name for the data named volume. For more information, see Volumes . DATALOG_VOLUME - The name for the datalog named volume. For more information, see Volumes . LOG_VOLUME - The name for the log named volume. For more information, see Volumes . HOST_PORT - The port number on the host machine that is mapped to the port on the container. ZOO_ID - An identifier for the ZooKeeper server. For more information, see Environment variables . An example of running Zookeeper container using runZK function: runZK zk1 zk1.eia zk1_data zk1_datalog zk1_logs 8080 1 Volumes A named volume or a bind mount can be used to persist data and logs that are generated and used in the ZooKeeper container, outside of the container. For more information, see Where to store data . Named Volumes To configure the ZooKeeper container to use the named volume, specify the -v option with the name of the volume and the path where the directory is mounted in the container. By setting -v option in the docker run command, a named volume is created. For ZooKeeper, the directories that must be mounted are /data , /datalog , /logs . For example: -v zk1_data:/data \\ -v zk1_datalog:/datalog \\ -v zk1_log:/logs \\ -v zk1_secrets:/run/secrets A unique volume name must be used for each ZooKeeper container. A bind mount can be used instead of the named volume: For example: -v /var/zk/data:/data \\ -v /var/zk/datalog:/datalog \\ -v /var/zk/logs:/logs \\ A unique bind mount must be used for each ZooKeeper container. Secrets : A directory that contains all of the secrets that this tool requires. Specifically this includes credentials to access zookeeper and certificates used in SSL. The directory is used to update the named volume location defined by the CONTAINER_SECRETS_DIR environment variable. This can then be used by other environment variables such as SSL_PRIVATE_KEY_FILE to locate the secrets. In a production environment, the orchestration environment can provide the secrets to the file system or the secrets can be passed in via environment variables. The mechanism that is used here simulates the orchestration system providing the secrets as files. Environment variables To configure ZooKeeper, you can provide environment variables to the Docker container in the docker run command. The zoo.cfg configuration file for ZooKeeper is generated from the environment variables passed to the container. The following table describes the mandatory environment variables for running ZooKeeper in replicated mode: Environment variable Description ZOO_SERVERS Specified the list of ZooKeeper servers in the ZooKeeper ensemble. Servers are specified in the following format: server.id=<address1>:<port1>:<port2>;<client port> . ZOO_MY_ID An identifier for the ZooKeeper server. The identifier must be unique within the ensemble. ZOO_CLIENT_PORT Specifies the port number for client connections. Maps to the clientPort configuration parameter. ZOO_4LW_COMMANDS_WHITELIST A list of comma separated Four Letter Words commands that user wants to use. A valid Four Letter Words command must be put in this list else ZooKeeper server will not enable the command. By default the whitelist only contains \"srvr\" command which zkServer.sh uses. The rest of four letter word commands are disabled by default. For more information, see ZooKeeper Docker hub . The following table described the security environment variables: Environment variable Description ZOO_SECURE_CLIENT_PORT Specifies the port number for client connections that use SSL. Maps to the secureClientPort configuration parameter. SOLR_ZOO_SSL_CONNECTION See Secure Environment variables . SERVER_SSL See Secure Environment variables . SSL_PRIVATE_KEY_FILE See Secure Environment variables . SSL_CERTIFICATE_FILE See Secure Environment variables . SSL_CA_CERTIFICATE_FILE See Secure Environment variables . For more information about securing ZooKeeper, see Encryption, Authentication, Authorization Options . The following table describes the environment variables that are supported: Environment variable Description ZOO_TICK_TIME The length of a single tick, which is the basic time unit used by ZooKeeper, as measured in milliseconds. Maps to the tickTime configuration parameter. The default value is 2000 . ZOO_INIT_LIMIT Amount of time, in ticks, to allow followers to connect and sync to a leader. Increase this value as needed, if the amount of data managed by ZooKeeper is large. Maps to the initLimit configuration parameter. The default value is 10 . ZOO_SYNC_LIMIT Amount of time, in ticks, to allow followers to sync with ZooKeeper. If followers fall too far behind a leader, they will be dropped. Maps to the syncLimit configuration parameter. The default value is 5 . ZOO_AUTOPURGE_PURGEINTERVAL The time interval in hours for which the purge task has to be triggered. Set to a positive integer (1 and above) to enable the auto purging. Maps to the autopurge.purgeInterval configuration parameter. The default value is 24 . ZOO_AUTOPURGE_SNAPRETAINCOUNT When auto purge is enabled, ZooKeeper retains the specified number of most recent snapshots and the corresponding transaction logs in the dataDir and dataLogDir respectively and deletes the rest. Maps to the autopurge.snapRetainCount setting. The default value is 3 . ZOO_MAX_CLIENT_CNXNS Limits the number of concurrent connections (at the socket level) that a single client, identified by IP address, may make to a single member of the ZooKeeper ensemble. Maps to the maxClientCnxns configuration parameter. The default value is 60 . ZOO_STANDALONE_ENABLED When set to true , if ZooKeeper is started with a single server the ensemble will not be allowed to grow, and if started with more than one server it will not be allowed to shrink to contain fewer than two participants. Maps to the standaloneEnabled configuration parameter. The default value is true . ZOO_ADMINSERVER_ENABLED Enables the AdminServer. The AdminServer is an embedded Jetty server that provides an HTTP interface to the four letter word commands. Maps to the admin.enableServer configuration parameter. The default value is true . ZOO_DATA_DIR The location where ZooKeeper stores in-memory database snapshots. Maps to the dataDir configuration parameter. The default value is /data . ZOO_DATA_LOG_DIR The location where ZooKeeper writes the transaction log. Maps to the dataLogDir configuration parameter. The default value is /datalog . ZOO_CFG_EXTRA You can add arbitrary configuration parameters, that are not exposed as environment variables in ZooKeeper, to the Zookeeper configuration file using this variable. ZOO_CONF_DIR Specifies the location for the ZooKeeper configuration directory. The default value is /conf . ZOO_LOG_DIR Specifies the location for the ZooKeeper logs directory. The default value is /logs . For more information about configuring ZooKeeper, see: Configuration Parameters ZooKeeper Docker hub . Note: Values that are specified in the environment variables override any configuration that is included in the ZOO_CFG_EXTRA block."
  },
  "versions/2.2.0/content/ingest_config_dev.html": {
    "href": "versions/2.2.0/content/ingest_config_dev.html",
    "title": "Ingesting development data into the Information Store",
    "keywords": "Ingesting development data into the Information Store When you are developing a configuration, ingest a small amount of representative test data into the system to ensure the schema is suitable for your data and you can configure i2 Analyze to meet your requirements. For more information about ingesting data, see Ingesting data into the Information Store . Ensure that the DEPLOYMENT_PATTERN variable in the <config_name>/utils/variables.sh file is set to a pattern that includes the Information Store. For example: DEPLOYMENT_PATTERN=\"i2c_istore\" Process overview: Provide a data set Create the ingestion sources Provide and run scripts to complete the ingestion process If you have deployed with the law enforcement schema, complete the steps in Example ingestion process to ingest example data into your Information Store. Data sets The /i2a-data directory is used to contain the data sets that you ingest into the Information Store of a deployment. The data that you ingest into the Information Store must conform to the Information Store schema. However, one data set can be ingested with different configs. There is not a 1-to-1 mapping between data sets and configs. Each data set must contain at least one ingestion script. This script contains the functions that populate the staging tables with your data and calls the ETL toolkit tools that ingest the data. The expected directory structure is as follows: - i2a-data - <data_set> - scripts - <script1> - <script2> Ingesting data into the config dev environment The manageData.sh script is used to manage the ingestion process in the dev environment. To ingest data into the Information Store, you must create scripts that call the ETL tools that complete the actions required by i2 Analyze for you ingest data. For more information, see: - ETL tools - Ingesting data into the Information Store You can find example scripts in the examples/ingestion/scripts directory. These scripts demonstrate how to create staging tables, populate them, and ingest the data into the Information Store. To run scripts, the manageData.sh script is called as follows: ./manageData.sh -c <config_name> -t ingest -d <data_set> -s <script_name> Where: <config_name> is the name of the config that is currently deployed and running in the config dev environment <data_set> is the name of a directory in i2a-data <script_name> is the name of a script in the directory specified for <data_set> Creating ingestion sources The ingestion sources for a config are contained in the configuration. Ingestion sources are defined in <config_name>/configuration/ingestion/scripts/createIngestionSources.sh . Copy the examples/ingestion/scripts/createIngestionSources.sh file to the <config_name>/configuration/ingestion/scripts/ directory. In the script, the INGESTION_SOURCES array contains the name and description of 2 example sources. INGESTION_SOURCES=( [Example Ingestion Source 1]=EXAMPLE_1 [Example Ingestion Source 2]=EXAMPLE_2 ) You can modify or add to the array of ingestion sources. To create the ingestion sources in the array, the manageData.sh script is called as follows: ./manageData.sh -c <config_name> -t sources Example ingestion process The i2 Analyze minimal toolkit contains the example law-enforcement-data-set-1 data that can be ingested when the example law enforcement schema ( law-enforcement-schema.xml ) is deployed. This contains a number of CSV files that contain the data, and a mapping.xml file. For more information about the mapping file, see Ingestion mapping files . Before you can ingest the law enforcement example data, complete the following steps to provide the data set and scripts: Copy the pre-reqs/i2analyze/toolkit/examples/data/law-enforcement-data-set-1 directory to the i2a-data directory. Copy the examples/ingestion/scripts directory to the i2a-data/law-enforcement-data-set-1 directory. The directory structure is as follows: - i2a-data - law-enforcement-data-set-1 - scripts - ingestLawEnforcementDataSet1.sh - createStagingTables.sh Copy the examples/ingestion/scripts/createIngestionSources.sh file to the <config_name>/configuration/ingestion/scripts/ directory. Use the manageData.sh to create the ingestion sources defined in the example createIngestionSources.sh . For example: ./manageData.sh -c config-development -t sources The example scripts separate the creation of the staging tables from the ingestion of data. To ingest the example data into the config-development config, run the following commands: ./manageData.sh -c config-development -t ingest -d law-enforcement-data-set-1 -s createStagingTables.sh ./manageData.sh -c config-development -t ingest -d law-enforcement-data-set-1 -s ingestLawEnforcementDataSet1.sh The manageData.sh script The scripts/manageData.sh script is used to manage data in an environment. It can be used to run scripts that use the ETL toolkit tools, or to remove all data from the Information Store. The following usage and help is provided for the manageData.sh script: Usage: manageData.sh -c <config_name> -t {ingest} -d <data_set> -s <script_name> [-v] manageData.sh -c <config_name> -t {sources} [-s <script_name>] [-v] manageData.sh -c <config_name> -t {delete} [-v] manageData.sh -h Options: -c <config_name> Name of the config to use. -t {delete|ingest|sources} The task to run. Either delete or ingest data, or add ingestion sources. Delete permanently removes all data from the database. -d <data_set> Name of the data set to ingest. -s <script_name> Name of the ingestion script file. -v Verbose output. -h Display the help. After you add data to your environment, you can configure the rest of the configuration ."
  },
  "versions/2.2.0/content/manage_backup_restore.html": {
    "href": "versions/2.2.0/content/manage_backup_restore.html",
    "title": "Back up and restore a development database",
    "keywords": "Back up and restore a development database When you are developing a configuration that uses an Information Store, you might populate the Information Store with demonstration or test data. You can use the tooling provided to back up the Information Store that is associated with a config. The Information Store is contained in a Docker volume. Because Docker is not designed for permanent data storage, you can back up an Information Store to your local file system. Backup location The backups are stored in the /backups directory. A sub-directory is created for each config name, with another sub-directory for each backup name. The backup file is called ISTORE.bak . For example: - backups - <config_name> - <backup_name> - ISTORE.bak - <backup_name> - ISTORE.bak Creating a backup Use the deploy script to create your backup. The following usage pattern shows how you create a backup: ./deploy.sh -c <config_name> -t backup -b <backup_name> If you do not provide a backup name, the backup is created in a directory called default . For example, to create a backup called test-1 for the config-development config: ./deploy.sh -c config-development -t backup -b test-1 Restoring from backup Use the deploy script to restore from a backup. The following usage pattern shows how you restore from a backup: ./deploy.sh -c <config_name> -t restore -b <backup_name> For example, to restore a backup called test-data-1 for the config-development config: ./deploy.sh -c config-development -t restore -b test-1"
  },
  "versions/2.2.0/content/managing_config_dev.html": {
    "href": "versions/2.2.0/content/managing_config_dev.html",
    "title": "Managing configurations",
    "keywords": "Managing configurations Configs The /configs directory contains all of your configs. The name of the directory for each config is used to identify it when you run the deploy script. You can have as many different configs in the configs directory, however you can only have one deployed and running in the environment at any time. When you run the deploy.sh script and specify the name of a config that is already deployed, the running deployment is updated with any changes you have made to the configuration. When you run the deploy.sh script and specify the name of a config that is not deployed, the containers in the current environment are stopped and a new environment is deployed with the specified config. The deploy.sh script The scripts/deploy.sh script is used to deploy configs and manage your environment. The following usage and help is provided for the deploy.sh script: Usage: deploy.sh -c <config_name> [-t {clean}] [-v] [-y] deploy.sh -c <config_name> [-t {connectors}] [-v] [-y] deploy.sh -c <config_name> [-t {backup|restore} [-b <backup_name>]] [-v] [-y] deploy.sh -c <config_name> -a [-t {connectors}] -d <deployment_name> -l <dependency_label>] [-v] [-y] deploy.sh -h Options: -c <config_name> Name of the config to use. -t {clean} Clean the deployment. Will permanently remove all containers and data. -t {connectors} Generate the secrets and build the images for connectors. By default, it acts on all connectors in the connector-images directory. -t {backup} Backup the database. -t {restore} Restore the database. -b <backup_name> Name of the backup to create or restore. If not specified, the default backup is used. -a Produce or use artifacts on AWS. -d <deployment_name> Name of deployment to use on AWS. -l <dependency_label> Name of dependency image label to use on AWS. -n <connector_name> Name of a connector. This option is used with the connectors task to specify which connectors to act on. -v Verbose output. -y Answer 'yes' to all prompts. -h Display the help."
  },
  "versions/2.2.0/content/managing_connectors.html": {
    "href": "versions/2.2.0/content/managing_connectors.html",
    "title": "Managing connectors",
    "keywords": "Managing connectors The /connector-images directory contains the files that are required to build each connector image in your config dev environment. The complete contents of each connector directory depends on the template that you used to create it. All the templates include a folder that you use to place the connector source code, the files used to build the Docker images and run the Docker containers, and JSON files that you use to provide information about the connector. For instructions about how to add connectors to your environment, see Adding connectors to your development environment . Connector versioning You specify the version and tag of a connector in the connector-version.json file of each connector. version The version value is used as a version for your connector. If you are deploying an i2 Connect server connector, the version value has more requirements. For more information, see i2 Connect server connector versioning . tag The tag value is used in the image name, host name, and secrets for your connector container. For example: { \"version\": \"0.0.1\", \"tag\": \"1-0-0\" } Building connectors Use the deploy.sh script with the connectors task to generate secrets and build the images for all connectors. You can specify connector names to build a subset of the connectors in your environment. For example, to build and run all of the connectors in the connector-images directory: ./deploy.sh -c <config_name> -t connectors If you do not want to build and run all of the connectors, you can use the -i and -e options to include or exclude connectors from the process. For example, to build and run only the connectors named example-connector and example-connector-2 : ./deploy.sh -c <config_name> -t connectors -i example-connector -i example-connector-2 Connector secrets Before you can use a connector, you must generate secrets for it. For more information about the secrets used in the environment, see Managing container security . The secrets that are generated for a particular connector are tied to the tag, if you change the tag you must to regenerate the secrets for that connector. To regenerate the secrets for a connector after you change the tag, complete the following steps: Remove the dev-environment-secrets/<connector_name> directory for the connector that you changed the tag of. Run the deploy.sh script and specify the connector name to regenerate the secrets, rebuild the image, and deploy the environment. For example: ./deploy.sh -c <config_name> -t connectors -i <connector_name> Note: To secure the connection to an external connector, you must install the following certificates where the connector is running. At this release, your connector must be configured to use the certificates signed by the config development environment CA. dev-environment-secrets/generated-secrets/certificates/<connector-name>/server.key dev-environment-secrets/generated-secrets/certificates/<connector-name>/server.cer dev-environment-secrets/generated-secrets/certificates/CA/CA.cer i2 Connect server connectors If you are deploying one or more connectors that were developed using the i2 Connect SDK. There are two versions that you must consider; the version of a connector and the version of the i2 Connect server that the connectors depends on. Connector version In the connector-version.json file, the version specifies the range of supported versions for the connector. This enables you to have more control of the version of a connector that is used in your deployment. You can specify the range of versions at the major, minor, or patch level. For example, if you specify a major version 1 in the connector-version.json file, any connector that has a major version of 1 is compatible. When the connector is developed, the version of the connector is provided in the package.json file by the developer. If the version of the connector specified in package.json does not match the version, or range of versions, that you specify in connector-version.json a warning is displayed when you deploy your environment. The following table demonstrates how you can specify the version ranges and their compatibility with connector versions: package.json version connector-version.json version Compatible? (Y/N) 1.0.0 1 Y 1.3.0 1 Y 1.4.0 1.3 N 1.4.6 1.4 Y 1.4.8 1.4.7 N 2.0.0 1 N i2 Connect server version i2 Connect server connectors depend on the i2 Connect server. When the connector is developed, the version of the i2 Connect server that the connector depends on is provided in the package.json file by the developer. In the connector-images/i2connect-server-version.json file, you specify the range of versions of the i2 Connect server that you want all i2 Connect server connectors to depend on. The versions in these files are specified in the npm semantic versioning syntax. For more information about the syntax, see semver - the semantic versioning for npm . If the version of the i2 Connect server that a connector depends on specified in package.json does not match the version, or range of versions, that you specify in i2connect-server-version.json a warning is displayed when you deploy your environment. The following table demonstrates how you can specify the version ranges and their compatibility with i2 Connect server versions: package.json version connector-version.json version Compatible? (Y/N) ^1.0.0 ^1.0.0 Y 1.3.2 ^1.0.0 Y 1.4.0 1.3.0 N 2.0.0 ^1.0.0 N i2 Connect server connector configuration (Optional) i2 Connect server connectors can have specific connector configuration that is defined in the connector.conf.json and connector.secrets.json files. When the connector is developed, default files can be provided by the developer. connector.conf.json After the first deployment of the connector, if a connector.conf.json file is provided, the system extracts it to the connector-images/<connector-name>/app directory. The file is in the same directory structure as in the supplied connector. For example, <connector-name>/app/dist/connectors/connector.conf.json . To change the configuration, modify the connector.conf.json and redeploy the environment. connector.secrets.json After the first deployment of the connector, if a connector.secrets.json file is provided, the system extracts it to the connector-images/<connector-name>/app directory. The file is in the same directory structure as in the supplied connector. For example, <connector-name>/app/dist/connectors/connector.secrets.json . The file contains configuration of secrets that are required for a connector. For example an API Key or user name and password that is required to access the data source that the connector queries."
  },
  "versions/2.2.0/content/managing_toolkit_configuration.html": {
    "href": "versions/2.2.0/content/managing_toolkit_configuration.html",
    "title": "Synchronize configurations",
    "keywords": "Synchronize configurations You can synchronize configurations between the i2 Analyze deployment toolkit and the configuration development environment. This enables you to develop the i2 Analyze configuration by using the config development environment, and deploy with it as part of the i2 Analyze deployment toolkit. This can be useful in the following situations: You want to create a new deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to develop the configuration for it. You have an existing deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to further develop the configuration. When you synchronize a configuration, you must do so with a complete i2 Analyze deployment toolkit. You can synchronize from an existing deployment toolkit configuration (import) or from an existing configuration development config (export). The configuration from the i2 Analyze deployment toolkit is the source of truth. When you have an existing deployment toolkit, the instructions direct you to back up the configuration before you synchronize it with the config development environment. You complete the synchronization by using the manageToolkitConfiguration.sh script. Examples The following use cases demonstrate the process of synchronizing configurations between the config development environment and an i2 Analyze deployment toolkit: Use case 1 : You want to create a new deployment that uses the i2 Analyze deployment toolkit and you want to use the config development environment to develop the configuration for it. Use case 2 : You have an existing deployment that uses the i2 Analyze deployment toolkit and you want to convert to use the config development environment to further develop the configuration. After you develop the configuration, you will still deploy i2 Analyze using the deployment toolkit. Use case 1 Use the config development environment to create and develop a config. For more information, see Configuration development environment . When you are happy with the developed configuration, you can complete the process that enables you to use it with the i2 Analyze deployment toolkit. Install the i2 Analyze deployment toolkit. For more information, see Installing i2 Analyze . Export the configuration from the config development environment to the i2 Analyze deployment toolkit. Run the manageToolkitConfiguration.sh script with the create task to create a base configuration in the i2 Analyze deployment toolkit. ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t create Where: -c is the name of the config. -p is the absolute path to the root of the i2 Analyze deployment toolkit that was installed in step 2. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. Note: The base configuration that is created is suitable for the deployment pattern that is specified in the variables.sh file of the specified config. Run the manageToolkitConfiguration.sh script with the export task to export the config dev configuration into the deployment toolkit. ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t export Where: -c is the name of the config to export. -p is the full path to the i2 Analyze deployment toolkit that was installed in step 2. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. You can then deploy your i2 Analyze deployment toolkit configuration. For more information about deploying i2 Analyze, see Deploying i2 Analyze . Start at step 3 from the linked documentation. There are a number of environment specific configuration files that are not populated in the config development environment. Use case 2 This section assumes that you are starting with a configuration from an i2 Analyze deployment toolkit. For more information about deploying i2 Analyze, see Deploying i2 Analyze . The config development environment uses fixed names for schemas and configuration files. Before you can import you deployment toolkit configuration into the config development environment, your configuration must be updated to use the expected file names. The manageToolkitConfiguration.sh script can rename the files for you and update any references to them. You can also decide whether or not to use the config development environment configuration set in your deployment toolkit configuration. If you decide to use the new config set, all settings for the deployment must be specified in the analyze-settings.properties file. If you choose not to use the new configuration set, any additions that you make to the analyze-settings file in the config development environment are not exported. Back up your i2 Analyze deployment toolkit configuration. Fore more information about backing up your configuration, see Back up and restore the configuration . Use the config development environment to create an empty config to import your configuration in to. For more information, see Configuration development environment . You do not need to specify any schema files or deploy the config. Run the manageToolkitConfiguration.sh script with the prepare task to rename the schema and configuration files and update the references: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t prepare Where: -c is the config name you are working with. -p is the absolute path to the root of the i2 Analyze deployment toolkit. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. To use the config development environment configuration set, complete the following 2 steps. Otherwise, move to step 5. Copy the contents of templates/toolkit-config-mod directory to the /toolkit/configuration/fragments/common/WEB-INF/classes directory of the i2 Analyze deployment toolkit. Review the settings that are used in the DiscoClientSettings and DiscoServerSettingsCommon properties files of i2 Analyze deployment toolkit and move any settings that you want to continue using into the /toolkit/configuration/fragments/common/WEB-INF/classes/analyze-settings.properties file. You can remove the DiscoClientSettings and DiscoServerSettingsCommon properties files after you move any settings. Run the manageToolkitConfiguration.sh script with the import task to import the configuration into the config development environment: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t import Where: -c is the config name to import the configuration into. -p is the absolute path to the root of the i2 Analyze deployment toolkit. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. Use the config development environment to develop the configuration. For more information, see Developing the configuration . In the config development environment, a user.registry.xml is required. You can start from the example provided in pre-reqs/i2analyze/toolkit/examples/security and align it to your security schema or copy in an existing one. If you want to use your developed configuration in your i2 Analyze deployment toolkit, you can use the export task to export the configuration into your toolkit. Otherwise, continue to use the config development environment. Run the manageToolkitConfiguration.sh script with the export task. For example: ./manageToolkitConfiguration.sh -c config-development -p /mnt/c/IBM/i2analyze/toolkit -t export Where: -c is the config name you have created to synchronize the configuration to. -p is the absolute path to the root of the i2 Analyze deployment toolkit. On Windows, you must use forward slashes ( / ) and access the Windows filesystem using the /mnt directory in WSL. You must update the i2 Analyze deployment toolkit deployment with the changes to the configuration. Connectors In the config development environment, i2 Connect connectors are run on Docker images and can only be access from within the Docker network. This means that the connectors and URL references to the connectors cannot be synchronized between configurations. The connector IDs are displayed when you run the import and export tasks to enable you to provide the connectors in each environment. For more information about deploying connectors, see: Adding connectors to your config development environment Adding connectors to your i2 Analyze deployment toolkit configuration Extensions In the config development environment, i2Analyze extensions are built and deployed using Maven. To add an extension to your i2 Analyze deployment toolkit, copy the JAR from i2a-extensions/<extension-name>/target/<extension-name>-<version>.jar and follow the DevEssentials deployment steps. For more information, see Configuring the 'group-based default security dimension values' example project for an example of the steps to take. manageToolkitConfiguration.sh script Usage: manageToolkitConfiguration.sh -c <config_name> -p <toolkit_path> -t { create | prepare | import | export } [-v] Options: -c <config_name> Name of the config to use. -p <toolkit_path> The absolute path to the root of an i2 Analyze deployment toolkit. -t {create} Creates a configuration in the i2 Analyze deployment toolkit that can be imported into the config development environment. -t {prepare} Prepares an existing i2 Analyze deployment toolkit configuration to be imported into the config development environment. -t {export} Export a config development environment configuration to an i2 Analyze deployment toolkit configuration. -t {import} Import an i2 Analyze deployment toolkit configuration to a config development environment configuration. -v Verbose output. -h Display the help."
  },
  "versions/2.2.0/content/managing_update_env.html": {
    "href": "versions/2.2.0/content/managing_update_env.html",
    "title": "Updating to the latest version of the analyze-containers repository",
    "keywords": "Updating to the latest version of the analyze-containers repository To update your config development environment to use the latest version of the analyze-containers repository, update the images, secrets, and deploy your configs. Before you update your environment: Back up any databases that are associated with your configs. For more information, see Back up and restore a development database . Back up the following directories from your existing copy of the repository: backups configs connector-images dev-environment-secrets gateway-schemas i2a-data Download the tar.gz or pull the latest version of the analyze-containers repository from https://github.com/i2group/analyze-containers . If you download the tar.gz , extract the contents and overwrite your existing copy of the analyze-containers repository. Then, copy your backed up directories from step 1.1 over the new version of the repository. Update the command line tools in your environment. The latest version of the analyze-containers repository requires XMLStarlet. For more information about installing XMLStarlet, see Command line tools . To update the images and secrets, run the createDevEnvironment.sh script from the scripts directory. ./createDevEnvironment.sh Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the utils/simulatedExternalVariables.sh script. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variables are in the utils/simulatedExternalVariables.sh script. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y To update any deployed configs, run the deploy.sh script from the scripts directory with your config name. You must run the script twice, once with the clean task. For example: ./deploy.sh -c <config_name> -t clean ./deploy.sh -c <config_name>"
  },
  "versions/2.2.0/content/managing_upgrade_env.html": {
    "href": "versions/2.2.0/content/managing_upgrade_env.html",
    "title": "Upgrading",
    "keywords": "Upgrading Upgrade your config development environment to use the latest version of the analyze-containers repository, or i2 Analyze update the images, secrets, and deploy your configs. The process of upgrading the config development environment is completed in a different directory from your current environment. Prerequisites Download the tar.gz for the latest version of the analyze-containers repository from https://github.com/i2group/analyze-containers . Extract the contents into a new directory. For example, analyze-containers-220 . Download the i2 Analyze V4.3.5 Minimal for Linux. To download i2 Analyze, follow the procedure described in Where can I download the latest i2 Products? Populate the subject of the form with Request for i2 Analyze 4.3.5 minimal toolkit for Linux . Rename the downloaded file to i2analyzeMinimal.tar.gz , then copy it to the analyze-containers-220/pre-reqs directory. Update the command line tools in your environment. For more information about installing XMLStarlet, see Command line tools . Download the Microsoft JDBC Driver 9.4.1 for SQL Server from https://github.com/microsoft/mssql-jdbc/releases/tag/v9.4.1 by clicking the mssql-jdbc-9.4.1.jre11.jar asset. Copy the mssql-jdbc-9.4.1.jre11.jar file to the pre-reqs/jdbc-drivers directory. To set the ANALYZE_CONTAINERS_ROOT_DIR variable, run the following command: . initShell.sh To create the configuration development environment and template config, in the analyze-containers-220/scripts directory run: ./createDevEnvironment.sh The script performs the following actions: Extracts the required files from the i2 Analyze deployment toolkit Builds the required Docker images for the development environment Generates the secrets that are used in the environment Creates the configuration template The configuration template is located in /templates/config-development . Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the utils/simulatedExternalVariables.sh script. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variables are in the utils/simulatedExternalVariables.sh script. For example: LIC_AGREEMENT=ACCEPT MSSQL_PID=Developer ACCEPT_EULA=Y There are 2 options that you can choose from when you upgrade your config development environment. Option 1 : Upgrade all of your configs to use the latest version of i2 Analyze. Option 2 : Upgrade a subset of your configs to use the latest version of i2 Analyze, and keep the remaining configs using the previous version. Regardless of the option that you choose, you will have 2 directories for each version of the analyze-containers project. If you choose option 1 and you no longer need to previous version, the manageEnvironment script can delete the analyze-containers directory and rename your analyze-containers-220 directory for you. Option 1 If you want to upgrade all of the configs in your environment to use the latest version, run the following command from the analyze-containers-220/scripts directory: ./manageEnvironment.sh -p <path-to-previous-project> -t upgrade Where <path-to-previous-project> is the absolute path to the root of your previous analyze-containers repository. This script completes the following actions: Creates a backup of the ISTORE database for each config that contains one Copies the following directories from the previous project to the current project: backups configs connector-images dev-environment-secrets gateway-schemas i2a-data i2a-extensions Upgrades the configs to the latest version of i2 Analyze and analyze-containers If you no longer need to previous version, answer yes to the prompt from the manageEnvironment.sh script. Option 2 If you want to upgrade a subset of configs to use the latest version, run the following commands from the analyze-containers-220/scripts directory: Backup the database for the configs you want to upgrade: ./manageEnvironment.sh -p <path-to-previous-project> -i <config-name> -b global-upgrade -t backup Where: <path-to-previous-project> is the absolute path to your previous analyze-containers repository <config-name> is the name of a config to backup. You can provide multiple -i options for each config you want to upgrade. Copy the configs and dependencies from the previous project to the current project: ./manageEnvironment.sh -p <path-to-previous-project> -i <config-name> -t copy Where: <path-to-previous-project> is the absolute path to your previous analyze-containers repository <config-name> is the name of a config to copy. You can provide multiple -i options for each config you want to upgrade. Run the createDevEnvironment.sh script from the analyze-containers-220/scripts directory to create the images for i2 Analyze 4.3.5. ./createDevEnvironment.sh Upgrades your configs to the latest version. For each config that you want to upgrade, run: ./deploy.sh -c <config-name> Where: <config-name> is the name of the config to upgrade."
  },
  "versions/2.2.0/content/reference architecture/deploy_aws.html": {
    "href": "versions/2.2.0/content/reference architecture/deploy_aws.html",
    "title": "Deploying the AWS reference architecture",
    "keywords": "Deploying the AWS reference architecture When you follow the instructions to deploy the AWS reference architecture, the scripts launch, configure, and run the AWS compute, network, storage, and other services for the i2 Analyze on AWS. The reference architecture includes AWS CloudFormation templates that automate the deployment and a deployment guide that describes the architecture. The deployment is running on AWS and you are charged by AWS for the infrastructure that is used. Consult the AWS pricing information before you run the scripts: ... By default, the AWS reference architecture is deployed on the following instances. For information about modifying these values before you deploy the reference architecture, see XXX. (TODO: populate resources and identify steps to modify the reference architecture). Prerequisites Before you can deploy the AWS reference architecture, you need to have installed and configured the following pre-requisites: Have an AWS account Install the AWS CLI v2 For more, see Installing, updating, and uninstalling the AWS CLI version 2 Configure the analyze-containers repository For more information, see Getting started with the analyze-containers repository . In your terminal, navigate to the examples/aws directory. Accepting the licenses Before you can use i2 Analyze and the tools, you must read the license agreement and copyright notices. The license file is in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the examples/aws/utils/simulated-external-variables.sh script. For example: LIC_AGREEMENT=ACCEPT Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variables are in the utils/simulatedExternalVariables.sh script. For example: MSSQL_PID=Developer ACCEPT_EULA=Y Setting up AWS Variables Before you can deploy the AWS reference architecture, you must provide your AWS account information in the examples/aws/utils/simulated-external-variables.sh script. To set up the system to communicate with your AWS account, set the value of the ECR_BASE_NAME environment variable to your default private registry URL. For more information, see Amazon ECR private registries . Set the value of the AWS_REGION environment variable to the region name you want to deploy the architecture in. For example: ECR_BASE_NAME=\"#####.dkr.ecr.eu-west-2.amazonaws.com\" AWS_REGION=\"eu-west-2\" Creating and uploading resources to AWS The create-and-upload-aws-resources.sh script creates the configuration, database scripts, secrets, images, and runbooks then uploads them to AWS. For more information about what is created, see create-and-upload-aws-resources.sh . Run create-and-upload-aws-resources.sh : ./create-and-upload-aws-resources.sh Deploying the i2 Analyze on AWS The deploy-aws.sh script deploys i2 Analyze on AWS. For more information about what is deployed, see deploy-aws.sh . Run deploy-aws.sh : ./deploy-aws.sh"
  },
  "versions/2.2.0/content/reference architecture/deploy_pre_prod.html": {
    "href": "versions/2.2.0/content/reference architecture/deploy_pre_prod.html",
    "title": "Pre-production example environment",
    "keywords": "Pre-production example environment Prerequisites Before you create an example pre-production environment, you must configure the analyze-containers repository. For more information, see Getting started with the analyze-containers repository . Creating a containerized deployment After you have all of the prerequisites in place, use the example scripts and artifacts in the examples/pre-prod directory to create the reference pre-production containerized deployment. Creating the environment and configuration The createEnvironment.sh script performs a number of actions that ensure all of the artifacts for a deployment are created and in the correct locations. These actions include: Extracting the i2 Analyze minimal toolkit to the pre-reqs/i2analyze directory. Creating the i2 Analyze Liberty application Creating and populating the configuration directory structure For more information about the what the script does, see: Create environment To create the environment and configuration, run the following commands: ./createPreProdEnvironment.sh The configuration directory is created in the examples/pre-prod directory. By default, the environment is created for an Information Store and i2 Connect gateway deployment. Accepting the licenses Before you can use i2 Analyze and the tools, you must read and accept the license agreement and copyright notices in the pre-reqs/i2analyze/license directory. To accept the license agreement, change the value of the LIC_AGREEMENT environment variable to ACCEPT . The environment variable is in the examples/pre-prod/utils/simulatedExternalVariables.sh script. Before you can use Microsoft SQL Server, you must accept the license agreement and the EULA. For more information about using the MSSQL_PID and ACCEPT_EULA environment variables, see Configure SQL Server settings with environment variables on Linux To accept the license in this environment, change the value of the MSSQL_PID and ACCEPT_EULA environment variables. The environment variable is in the utils/simulatedExternalVariables.sh script. Running the containers and start i2 Analyze To deploy and start i2 Analyze, run the following command: ./deploy.sh For more information about the actions that are completed, see Deploying i2 Analyze . Modifying the hosts file To enable you to connect to the deployment and the Solr Web UI, update your hosts file to include the following lines: 127.0.0.1 solr1.eia 127.0.0.1 i2analyze.eia NOTE: On Windows, you must edit your hosts file in C:\\Windows\\System32\\drivers\\etc and the hosts file in /etc/hosts for WSL. Installing Certificate To access the system, the server that you are connecting from must trust the certificate that it receives from the deployment. To enable trust, install the /dev-environment-secrets/generated-secrets/certificates/externalCA/CA.cer certificate as a trusted root certificate authority in your browser and operating system's certificate store. For information about installing the certificate, see: Install Certificates with the Microsoft Management Console Setting up certificate authorities in Firefox Set up an HTTPS certificate authority in Chrome Accessing the system To connect to the deployment, the URL to use is: https://i2analyze.eia:9046/opal/ What to do next To understand how the environment is created, you can review the documentation that explains the images, containers, tools, and functions: Images and containers Tools and functions To learn how to configure and administer i2 Analyze in a containerized environment, you can complete the walkthroughs that are included in the repository: Walkthroughs"
  },
  "versions/2.2.0/content/reference architecture/reference_architectures.html": {
    "href": "versions/2.2.0/content/reference architecture/reference_architectures.html",
    "title": "Containerized deployment reference architectures",
    "keywords": "Containerized deployment reference architectures Containerized deployment reference"
  },
  "versions/2.2.0/content/reference architecture/understanding.html": {
    "href": "versions/2.2.0/content/reference architecture/understanding.html",
    "title": "Understanding the reference architecture",
    "keywords": "Understanding the reference architecture The Analyze-Containers repository includes Dockerfiles and example scripts that provide a reference architecture for creating a containerized deployment of i2 Analyze. The scripts demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. How to use the reference architecture? The repository is designed to be used with the i2 Analyze minimal toolkit. The minimal toolkit is similar to the standard i2 Analyze deployment toolkit, except that it only includes the minimum amount of application and configuration files. The i2 Analyze minimal toolkit is used to provide the artifacts that are required to build the images and provide the configuration for the deployment. Bash scripts are then used to build the images and run the containers. To demonstrate creating an example containerized deployment, complete the actions described in Pre-production example environment . The minimal toolkit also contains the tools that are used by the bash scripts to deploy, configure, and administer your deployment of i2 Analyze. The tools are in the form of JAR files that are called from shell scripts. For more information about the tools that are available and their usage, see i2 Analyze tools . Dockerfiles and images A deployment of i2 Analyze consists of the following components: Liberty Solr ZooKeeper Optionally a database management system The Analyze-Containers repository contains the Dockerfiles that are used to build the images for each component. For Liberty and SQL Server, the image provided by Liberty and SQL Server is used. For Solr and ZooKeeper, the repository contains custom Dockerfiles that were created from the ones provided by Solr and ZooKeeper. For more information about the images and containers, see images and containers . Scripts The Analyze-Containers repository provides example scripts that you can use and leverage for your own deployment use cases. The Analyze-Containers repository contains a number of scripts that are designed to be used at various stages when working towards creating a containerized deployment. The repository also includes example artifacts that are used with the scripts. These artifacts include an example certificate authority and certificates, secrets and keys to be used with i2 Analyze, and utilities that are used by the example scripts. Walkthroughs A number of walkthroughs are provided that demonstrate how to complete configuration and administration tasks in a containerized deployment. The walkthroughs consist of a reference script that demonstrates how to complete the action, and a document that explain the process in more detail. What is deployed? When you run the provided scripts to create the example deployment, i2 Analyze is deployed in the following topology: The deployment includes: A load balancer container, using HAProxy Two Liberty containers configured for high availability A Solr cluster with two Solr containers A ZooKeeper ensemble with three ZooKeeper containers A SQL Server container A number of \"client\" ephemeral containers are used to complete a single actions. The following client containers are used: SQL Server client Solr client For more information about the images and containers, see images and containers ."
  },
  "versions/2.2.0/content/reference architecture/understanding_aws.html": {
    "href": "versions/2.2.0/content/reference architecture/understanding_aws.html",
    "title": "Understanding the AWS reference architecture",
    "keywords": "Understanding the AWS reference architecture The analyze-containers repository includes yaml files and example scripts that provide a reference architecture for creating an AWS deployment of i2 Analyze. The scripts demonstrate how to upload resources, create CloudFormation Stacks and enable you to deploy, configure, and run i2 Analyze on AWS. How to use the reference architecture? You should already have an understanding of the Containerized deployment reference before starting with the AWS reference architecture. This architecture uses the same Dockerfiles and images used in the containerized deployment reference which the exception of the database management system (server) and the load balancer. To create the example AWS deployment, complete the actions described in AWS example environment . Scripts The analyze-containers repository provides example scripts that you can use and leverage for your own deployment use cases. The analyze-containers repository contains a number of scripts that are designed to be used at various stages when working towards creating an AWS deployment. The repository also includes example artifacts that are used with the scripts. These artifacts include an example certificate authority and certificates, secrets and keys to be used with i2 Analyze, CloudFormation templates, runbooks, and utilities. Walkthroughs A number of walkthroughs are provided that demonstrate how to complete configuration and administration tasks in an AWS deployment. The walkthroughs consist of a reference script that demonstrates how to complete the action, and a document that explain the process in more detail. What is deployed? When you run the provided scripts to create the example deployment, i2 Analyze is deployed in the following topology: TODO! The deployment includes: A load balancer (AWS ELB) Liberty containers running in ECS as Fargate A Solr cluster with two Solr containers deployed in EC2 instances A ZooKeeper ensemble with three ZooKeeper containers deployed in EC2 instances SQL Server running in EC2 with EBS A number of \"client\" ephemeral containers are used to complete a single actions. The following client containers are used: SQL Server client Solr client For more information about the artifacts that are used, see: Stacks Runbooks AWS Tools"
  },
  "versions/2.2.0/content/runbooks/helper_runbooks.html": {
    "href": "versions/2.2.0/content/runbooks/helper_runbooks.html",
    "title": "Helper runbooks",
    "keywords": "Helper runbooks i2a-UpdateScripts Description This runbook is an automation pulling down i2a-scripts on all required EC2 instances Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group updateScripts Pull down and untar i2a-scripts.tar.gz N/A update-tools/update"
  },
  "versions/2.2.0/content/runbooks/runbooks.html": {
    "href": "versions/2.2.0/content/runbooks/runbooks.html",
    "title": "Runbooks",
    "keywords": "Runbooks The documentation in this section describes the runbooks that are used to deploy i2 Analyze in an AWS environment."
  },
  "versions/2.2.0/content/runbooks/solr_runbooks.html": {
    "href": "versions/2.2.0/content/runbooks/solr_runbooks.html",
    "title": "Solr runbooks",
    "keywords": "Solr runbooks The following runbooks are used to deploy, start, and stop Solr and ZooKeeper. i2a-SolrFirstRun Description Initialize, configure, and start Solr and ZooKeeper Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group runZookeeper1 Start ZooKeeper container on the zk1 instance run-zk.sh solr-first-run/run-zk runZookeeper2 Start ZooKeeper container on the zk2 instance run-zk.sh solr-first-run/run-zk runZookeeper3 Start ZooKeeper container on the zk3 instance run-zk.sh solr-first-run/run-zk configureZkForSolr Create is_cluster and upload solr/security.json configure-zk-for-solr.sh solr-first-run/configure-zk-for-solr runSolr Start Solr containers on all available solr instances run-solr.sh solr-first-run/run-solr configureSolr Create and configure Solr collections configure-solr.sh solr-first-run/configure-solr i2a-SolrStart Description Start previously created ZooKeeper and Solr containers Parameters Name Description DeploymentName The name of the deployment Steps Name Description i2a-script CloudWatch log group runZookeeper1 Start ZooKeeper container on the zk1 instance run-zk.sh solr-start/run-zk runZookeeper2 Start ZooKeeper container on the zk2 instance run-zk.sh solr-start/run-zk runZookeeper3 Start ZooKeeper container on the zk3 instance run-zk.sh solr-start/run-zk RUN_SOLR Start Solr containers on all available solr instances run-solr.sh solr-start/run-solr <!-- markdownlint-configure-file { \"MD024\": false } -->"
  },
  "versions/2.2.0/content/runbooks/sqlserver_runbooks.html": {
    "href": "versions/2.2.0/content/runbooks/sqlserver_runbooks.html",
    "title": "SQL Server runbooks",
    "keywords": "SQL Server runbooks i2a-SqlServerFirstRun Description This runbook is an automation for initializing and configuring the Information Store in SQL Server Parameters Name Description DeploymentName The name of the deployment"
  },
  "versions/2.2.0/content/security and users/aws_security.html": {
    "href": "versions/2.2.0/content/security and users/aws_security.html",
    "title": "",
    "keywords": ""
  },
  "versions/2.2.0/content/security and users/db_users.html": {
    "href": "versions/2.2.0/content/security and users/db_users.html",
    "title": "Database roles, users, and logins",
    "keywords": "Database roles, users, and logins During the deployment, administration, and use of i2 Analyze, a number of different actions are completed against the Information Store database. These actions can be separated into different categories that are usually completed by users with differing permissions. In SQL Server, you can create a number of database roles and assign users to roles. In the example deployment, a number of different roles and users are used to demonstrate the types of roles that might complete each action. Database roles In the example, the following roles are used: Role Description DBA_Role The DBA_Role is used to perform database administrative tasks. External_ETL_Role The External_ETL_Role is used to move data from an external system into the staging tables in the Information Store database. i2_ETL_Role The i2_ETL_Role is used to read data from the staging tables and ingest it into - and delete it from - the Information Store database. i2analyze_Role The i2analyze_Role is used to complete actions required by the Liberty application. For example, returning results for Visual Queries. Database role permissions Each database role requires a specific set of permissions to complete the actions attributed to them. DBA_Role The DBA_Role requires permissions to: Set up and maintain the database management system and Information Store database. Create and modify the database management system objects. For example, bufferpools, tablespoons, and filegroups. Create and modify database objects. For example, tables, views, indexes, sequences. Troubleshoot performance or other issues. For example, has all privileges on all tables. This can be restricted in some environments. Configure high availability. Manage backup and recovery activities. Additionally, the role requires access to two roles in the msdb database: SQLAgentUserRole - for more information, see SQLAgentUserRole Permissions db_datareader - for more information, see Fixed-Database Roles These roles are required for database creation, to initialize the deletion-by-rule objects, and to create the SQL Server Agent jobs for the deletion rules. Note: The configureDbaRolesAndPermissions.sh script is run during deploy to grant the correct permissions. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes All CREATE TABLE, CREATE VIEW, CREATE SYNONYM Required to create the database objects. All ALTER, SELECT, UPDATE, INSERT, DELETE, REFERENCES Required to make changes for maintaining the database. IS_Core EXECUTE Required for deletion-by-rule and database configuration. IS_Public EXECUTE Required to run the stored procedures for deletion-by-rule. The following table provides an overview of the permissions required on the schemas in the msdb database: Schema Permissions Notes dbo SQLAgentUserRole Required to create the deletion jobs during deployment, and to manage the deletion job schedule. dbo db_datareader Required to create the deletion job schedule. The following table provides an overview of the permissions required on the schemas in the master database: Schema Permissions Notes All VIEW SERVER STATE Required for deletion-by-rule automated jobs via the SQL Server Agent. sys EXECUTE ON fn_hadr_is_primary_replica Required for deletion-by-rule automated jobs. The configureDbaRolesAndPermissions.sh script is used to configure the DBA user with all the required role memberships and permissions. External_ETL_Role The External_ETL_Role requires permissions to move data from external systems into the Information Store staging tables. For example, it can be used by an ETL tool - such as DataStage or Informatica - to move and transform data that results in populated staging tables in the Information Store staging schema. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging SELECT, UPDATE, INSERT, DELETE Required to populate the staging tables with date to be ingested or deleted. In addition to these permissions, in an environment running SQL server in a Linux container, users with this role must also be a member of the sysadmin group in order to perform BULK INSERT into the external staging tables. The addEtlUserToSysAdminRole.sh script is used to make the etl user a member of the sysadmin fixed-server role. i2_ETL_Role The i2_ETL_Role requires permissions to use the i2 Analyze ingestion tools to ingest data from the staging tables into the Information Store. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging ALTER, SELECT, UPDATE, INSERT, DELETE Required by the ingestion tools to create and modify objects during the ingestion process. IS_Stg ALTER, SELECT, UPDATE, INSERT, DELETE Required by the ingestion tools to create and modify objects during the ingestion process. IS_Meta SELECT, UPDATE, INSERT UPDATE and INSERT are required to update the ingestion history table. SELECT is required to read the schema meta data. IS_Data ALTER, SELECT, UPDATE, INSERT, DELETE ALTER is required to drop and create indexes and update statistics as part of the ingestion process. IS_Public ALTER, SELECT ALTER is required to delete and create synonyms when enabling merged property views. IS_Core SELECT, EXECUTE Required to check configuration of the database. i2analyze_Role The i2analyze_Role requires permissions to complete actions required by the Liberty application. These actions include: Visual Query, Find Path, Expand, Add to chart, Upload, and Online upgrade. The following table provides an overview of the permissions required on the schemas in the Information Store database: Schema Permissions Notes IS_Staging ALTER, SELECT, UPDATE, INSERT, DELETE Required to run deletion-by-rule jobs. IS_Stg ALTER, SELECT, UPDATE, INSERT, DELETE Required to upload and delete records via Analyst's Notebook Premium and to run deletion-by-rule jobs. IS_Meta SELECT, UPDATE, INSERT, DELETE DELETE is required to process the ingestion history queue. IS_Data SELECT, UPDATE, INSERT, DELETE UPDATE, INSERT, and DELETE are required by deletion-by-rule and to upload and delete records via Analyst's Notebook Premium. IS_Core SELECT, UPDATE, INSERT, DELETE Required for online upgrade. IS_VQ SELECT, UPDATE, INSERT, DELETE Required to complete Visual Queries. IS_FP SELECT, INSERT, EXEC Required to complete Find Path operations. IS_WC SELECT, UPDATE, INSERT, DELETE Required to work with Web Charts. The database backup operator role The example also demonstrates how to perform a database backup. The dbb user will perform this action and is a member of the SQL Server built-in role, db_backupoperator . This gives this user the correct permissions for performing a backup and nothing else. For more information, see Fixed-Database Roles for more details. Database users and logins In the example, a user is created for each role described previously. These users are then used throughout the deployment and administration steps to provide a reference for when each role is required. The following users and logins are used in the example: User and login Description Secrets sa The system administrator user. The sa user has full permissions on the database instance. This user creates the Information Store database, roles, users, and logins. The password is in the SA_PASSWORD file, and the username is in the SA_USERNAME file in the secrets/sqlserver directory. i2analyze The i2analyze user is a member of the i2analyze_Role . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/liberty directory. etl The etl user is a member of External_ETL_Role . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/etl directory. i2etl The i2etl user is a member of i2_ETL_Role . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/i2etl directory. dba The dba user is a member of DBA_Role . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/dba directory. dbb The dbb user is the database backup user, it is a member of the SQL Server built in role: db_backupoperator . The password is in the DB_PASSWORD file, and the username is in the DB_USERNAME file in the secrets/dbb directory. The sa user and login exists on the base SQL Server image. The sa user is used to create the following artifacts: Database: ISTORE Roles: i2analyze_Role , External_ETL_Role, , i2_ETL_Role and DBA_Role Logins: i2analyze , etl , i2etl , dba , and dbb Users: i2analyze , etl , i2etl , dba , and dbb The roles and users must be created after the Information Store database is created. Creating the roles The sa user is used to run the createDbRoles.sh client function that creates the i2Analyze_Role , External_ETL_Role , i2_ETL_Role , and DBA_Role roles. To create the roles, the createDbRoles.sh script is run using the runSQLServerCommandAsSA client function. This function uses an ephemeral SQL Server client container to create the database roles. For more information about the client function, see: runSQLServerCommandAsSA createDbRoles.sh All the secrets required at runtime by the client container are made available by providing a file path to the secret which is converted to an environment variable by the docker container. For example, to provide the SA_USERNAME environment variable to the client container, a file containing the secret is declared in the docker run command: -e \"SA_USERNAME_FILE=${CONTAINER_SECRETS_DIR}/SA_USERNAME_FILE\" The file name can be anything, but the environment variable is fixed. For more information see, managing container security In the example, the createDbRoles.sh script is called in deploy.sh . Create the login and user Use the sa user to create the login and the user on the ISTORE , and make the user a member of the role. You can use an ephemeral SQL Client container to create the login and the user. The createDbLoginAndUser.sh script in /images/sql_client/db-scripts is used to create the login and user. The scripts are called from the deploy.sh scripts. The createDbLoginAndUser function The createDbLoginAndUser function uses an ephemeral SQL Client container to create the database administrator login and user. The login and user are created by the sa user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The createDbLoginAndUser.sh script is used to create the login and user. The function requires the following environment variables to run: Environment variable Description SA_USERNAME The sa username. SA_PASSWORD The sa user password. DB_USERNAME The database user name. DB_PASSWORD The database user password. DB_SSL_CONNECTION Whether to use SSL for connection. SSL_CA_CERTIFICATE The path to the CA certificate. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_ROLE The name of the role that user will be added to. It has to be one of the roles from this list . Changing SA password In a Docker environment, you must start the SQL Server as the existing sa user before you can modify the password. The changeSAPassword function The changeSAPassword function uses an ephemeral SQL Client to change the sa user password. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The changeSAPassword.sh script is used to change the password. The function requires the following environment variables to run: Environment variable Description SA_USERNAME The sa username SA_OLD_PASSWORD The current sa password. SA_NEW_PASSWORD The new sa password. DB_SSL_CONNECTION Whether to use SSL for connection. SSL_CA_CERTIFICATE The path to the CA certificate. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store."
  },
  "versions/2.2.0/content/security and users/security.html": {
    "href": "versions/2.2.0/content/security and users/security.html",
    "title": "Managing container security",
    "keywords": "Managing container security SSL certificates in the deployment The example deployment is configured to use SSL connections for communication between clients and i2 Analyze, and between the components of i2 Analyze. To achieve this, the appropriate certificate authorities and certificates are used. The generateSecrets.sh script is used to simulate the process of creating the required keys and acquiring certificate authority-signed certificates. Note: The keys and certificates used are set to expire after 90 days. To use the certificates for longer than this, you must run the generateSecrets.sh script again. Certificate Authorities (CA) In the example, two CAs are used to provide trust: Internal CA The internal CA is to provide trust for the containers that are used for the components of i2 Analyze. Each container's certificates are signed by the internal CA. External CA The external CA is to provide trust for external requests to the i2 Analyze service via the load balancer. In our example, the external certificate authority is generated for you. However, in production a real certificate should be used. Container certificates To communicate securely using TLS each container requires the following certificates: Private key Certificate key Internal certificate authority The containers will generate truststores and keystores based on the keys provided to the container. For more information about how the keys are passed to the containers securely please see Secure Environment variables . Secure communication between containers When the components communicate, the CA certificate is used to establish trust of the container certificate that is received. Each container has its own private key ZooKeeper requires client authentication to initiate communication. The i2 Analyze, i2 Analyze Tool, and Solr client containers require container certificates to authenticate with ZooKeeper. Creating keys and certificates The following diagram shows a simplified sequence of creating a container certificate from the certificate authority and using it to establish trust: The certificate authority's certificate is distributed to the client. The private key is generated on the server. In the generateSecrets.sh script, the key is created by: openssl genrsa -out server.key 4096 The public part of the private key is used in a Certificate Signing Request (CSR). In the generateSecrets.sh script, this is completed by: openssl req -new -key server.key -subj \"/CN=solr1.eia\" -out key.csr The common name that is used for the certificate is the server's fully qualified domain name. The CSR is sent to the certificate authority (CA). The CA signs and returns a signed certificate for the server. In the generateSecrets.sh script, the CA signing the certificate is completed by: openssl x509 -req -sha256 -CA CA.cer -CAkey CA.key -days 90 -CAcreateserial -CAserial CA.srl -extfile x509.ext -extensions \"solr\" -in key.csr -out server.cer When communication is established, the container certificate is sent to the client. The client uses it's copy of the CA certificate to verify that the container certificate was signed by the same CA. Password generation The example simulates secrets management performed by various secrets managers provided by cloud vendors. The generateSecrets.sh generates these secrets and populates the dev-environment-secrets/simulated-secret-store with secrets required for each container. The docker desktop does not support secrets, but the example environment example simulates this by mounting the secrets folder. For more information see Manage sensitive data with Docker secrets . After these passwords have been generated, they can be uploaded to a secrets manager. Alternatively you can use a secrets manager to generate your passwords. Solr Basic Authentication Solr authorization can be enabled using the BasicAuthPlugin. The basic auth plugin defines users, user roles and passwords for users. For the BasicAuthPlugin to be enabled, solr requires a security.json file to be uploaded. In our example the security.json file is created by the generateSecrets.sh and located in dev-environment-secrets/generated-secrets/secrets/solr/security.json . For more information about solr authentication see Basic Authentication Plugin Secure Environment variables In general secrets used by a particular container can be supplied via an environment variable containing the path to a file containing the secret, or an environment variable specifying the literal secret value, for example: Note: Secrets can be passwords, keys or certificates. docker run --name solr1 -d --net eia --secret source=SOLR_SSL_KEY_STORE_PASSWORD,target=SOLR_SSL_KEY_STORE_PASSWORD \\ -e SOLR_SSL_KEY_STORE_PASSWORD_FILE=\"/run/secrets/SOLR_SSL_KEY_STORE_PASSWORD_FILE\" or docker run --name solr1 -d --net eia -e SOLR_SSL_KEY_STORE_PASSWORD=\"jhga98u43jndfj\" The docker files in the analyze-containers repository have been modified to accept either. The convention is that the environment variable must match the property being set with \"_file\" appended if the secret is in a file, and without if a literal value is being used instead. In the example scripts, each container gets the relevant key stores mounted along with the correct secrets files in a secrets directory. NOTE: By default this is set as CONTAINER_SECRETS_DIR=\"/run/secrets\" in the common variables file. All the containers use the same environment variables to define the location of certificates. These are then used to generate appropriate artifacts for the particular container. There is also a standard way of turning on and off server SSL. Security switching variables Environment variable Description SERVER_SSL Can be set to true or false . If set to true , the container is configured to use encrypted connections. LIBERTY_SSL_CONNECTION Can be set to true or false . If set to true , connections to the Liberty container use an encrypted connection. DB_SSL_CONNECTION Can be set to true or false . If set to true , connections to the database use an encrypted connection SOLR_ZOO_SSL_CONNECTION Can be set to true or false . If set to true , connections to ZooKeeper and Solr use an encrypted connection. GATEWAY_SSL_CONNECTION Can be set to true of false . If set to true , connections to i2 Connect connectors use an encrypted connection. SSL_ENABLED Can be set to true or false . If set to true , the connector container communicates with the i2 Connect gateway using an encrypted connection. Security environment variables Environment variable Description SSL_PRIVATE_KEY The private key for the container certificate. SSL_CERTIFICATE The container certificate. SSL_CA_CERTIFICATE The Certificate Authority certificate. SSL_OUTBOUND_PRIVATE_KEY The private key for the Liberty container, which is used for outbound connections. SSL_OUTBOUND_CERTIFICATE The private certificate for the Liberty container, which is used for outbound connections. SSL_OUTBOUND_CA_CERTIFICATE_FILE The certificate authority used for verifying outbound connections. <!-- cspell:ignore jhga98u43jndfj >"
  },
  "versions/2.2.0/content/security and users/security_users.html": {
    "href": "versions/2.2.0/content/security and users/security_users.html",
    "title": "Security and users",
    "keywords": "Security and users The documentation in this section describes how security and users are configured for a deployment of i2 Analyze in a containerized environment."
  },
  "versions/2.2.0/content/stacks/solr_stack.html": {
    "href": "versions/2.2.0/content/stacks/solr_stack.html",
    "title": "The Solr stack",
    "keywords": "The Solr stack Solr CloudFormation template in the i2a-stack-solr.yaml file creates: 3 ZooKeeper instances Solr Launch template 2 Solr instances SSM parameters for ZooKeeper and Solr ZooKeeper stack The ZooKeeper stack is based on the i2a-stack-docker-ec2.yaml CloudFormation template and uses a launch template to complete the following actions: Downloads i2a-scripts from the S3 resources bucket Installs Docker Downloads the zk image from AWS ECR Sets up CloudWatch ZooKeeper stack tags Tags: - Key: \"instance-name\" Value: \"zk1\" - Key: \"stack-name\" Value: !Ref DeploymentName ZooKeeper stack security groups Each ZooKeeper instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) ZkSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId Solr stack Solr Launch Template This Launch templates specifies the Solr instance configuration information, such as security groups and the commands required to initialize each Solr instance. EBS Volume Each Solr instance has a dedicated EBS Volume. The EBS Volume is in the /dev/sdh directory. (TODO: Add more info what is this for) EFS Volume Each Solr instance has dedicated EFS Storage to store data. EFS Storage is in the /var/solr directory. (TODO: Add more info what is this for) Solr stack security groups Each Solr instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) SolrSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId EFSSecurityGroupId Solr stack tags Tags: - Key: \"Name\" Value: !Sub \"${DeploymentName}-Solr-Instance\" - Key: \"stack-name\" Value: !Ref DeploymentName - Key: \"instance-name\" Value: \"solr\" Parameters DeploymentName: Type: String Description: Deployment Name PrivateSubnetAId: Type: AWS::EC2::Subnet::Id Description: Private subnet id to use for the instance IntraSubnetEncryption: Type: String Description: When set to true, uses an encrypted connection AllowedValues: - \"false\" - \"true\" Default: \"false\" ResourcesS3Bucket: Type: String Description: The AWS S3 bucket for CloudFormation templates ZkInstanceType: Type: String Description: EC2 instance type to use for ZooKeeper AllowedValues: - t3a.medium - t3a.large - t3.medium - t3.large Default: \"t3a.medium\" ConstraintDescription: Must be a valid EC2 instance type SolrInstanceType: Type: String Description: EC2 instance type to use for Solr AllowedValues: - r5.large - r5.xlarge - r5.2xlarge - r5b.large - r5b.xlarge - r5b.2xlarge - r5d.large - r5d.xlarge - r5d.2xlarge Default: \"r5.large\" ConstraintDescription: Must be a valid EC2 instance type SolrEBSVolumeSize: Type: Number Description: Volume size to use for Solr, in GiBs Default: 8 SolrEBSVolumeType: Type: String Description: Volume type to use for Solr AllowedValues: - gp3 - io1 - io2 Default: 'gp3' SolrEBSVolumeIops: Type: Number Description: > The number of I/O operations per second provisioned for the volume to use for Solr The following are the supported values for each volume type: gp3: 3,000-16,000 IOPS io1: 100-64,000 IOPS io2: 100-64,000 IOPS' Default: 3000 LatestAmiId: Type: \"AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>\" Description: This gets the latest AMI image ID. Do NOT change Default: \"/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\" IamInstanceProfile: Type: String Description: The name of the shared instance template stack Default: \"SSMInstanceProfile\" (TODO: do we need to add info about all ssm params being created as a part of this stack?)"
  },
  "versions/2.2.0/content/stacks/sqlserver_stack.html": {
    "href": "versions/2.2.0/content/stacks/sqlserver_stack.html",
    "title": "The SQL Server stack",
    "keywords": "The SQL Server stack SQL Server CloudFormation template in the i2a-sqlserver.yaml file creates: SQL Server instance SSM parameters for SQL Server SQL Server stack The SQL Server stack uses the latest ami image and uses a launch template to complete the following actions: Installs jQuery Installs Amazon EFS client Creates a mount directory Mounts EFS file system Sets up SQL Server SQL Server stack tags Tags: - Key: Name Value: !Join [\"\", [!Ref \"AWS::StackName\", -SQLServer2019-Instance]] - Key: \"instance-name\" Value: \"sqlserver\" - Key: \"stack-name\" Value: !Ref DeploymentName SQL Server stack security groups The SQL Server instance is assigned to the following security groups: (TODO: add docs for the security stack and link from here) SQLServerSecurityGroupId I2AnalyzeSecurityGroupId AwsSecurityGroupId EFSSecurityGroupId EBS Volume The SQL Server instance has a dedicated EBS Volume. The EBS Volume is in the /dev/sdh directory. (TODO: Add more info what is this for) Parameters Parameters: DeploymentName: Description: Deployment Name Type: String PrivateSubnetAId: Description: Private subnet id to use for the instance Type: AWS::EC2::Subnet::Id InstanceType: Description: EC2 instance type to use for SQL Server Type: String AllowedValues: - m5.xlarge - m5.2xlarge - m5.4xlarge - m5.8xlarge - m5.12xlarge - m5.16xlarge - m5.24xlarge - r5.xlarge - r5.2xlarge - r5.4xlarge - r5.8xlarge - r5.12xlarge - r5.16xlarge - r5.24xlarge - i3.xlarge - i3.2xlarge - i3.4xlarge - i3.8xlarge - i3.16xlarge Default: m5.xlarge # Vendor recommended ConstraintDescription: must be a valid EC2 instance type supported by SQL Server. LatestAmiId: Description: This gets the latest SQL Server AMI image ID. Do NOT change Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id> Default: /aws/service/ami-windows-latest/amzn2-x86_64-SQL_2019_Standard DBEBSVolumeSize: Description: Volume size to use for database, in GiBs Type: Number Default: 50 DBEBSVolumeType: Description: Volume type to use for database Type: String AllowedValues: - gp3 - io1 - io2 Default: gp3 DBEBSVolumeIops: Description: 'The number of I/O operations per second provisioned for the volume to use for the database. The following are the supported values for each volume type: gp3: 3,000-16,000 IOPS io1: 100-64,000 IOPS io2: 100-64,000 IOPS' Type: Number Default: 3000 IamInstanceProfile: Description: The name of the shared instance template stack Type: String Default: SSMInstanceProfile (TODO: do we need to add info about all ssm params being created as a part of this stack?)"
  },
  "versions/2.2.0/content/stacks/stacks.html": {
    "href": "versions/2.2.0/content/stacks/stacks.html",
    "title": "Stacks",
    "keywords": "Stacks The documentation in this section describes the stacks that are used to deploy i2 Analyze in an AWS environment."
  },
  "versions/2.2.0/content/tools and functions/aws_tools.html": {
    "href": "versions/2.2.0/content/tools and functions/aws_tools.html",
    "title": "AWS tools",
    "keywords": "AWS tools create-and-upload-aws-resources.sh deploy-aws.sh create-and-upload-aws-resources.sh script The create-and-upload-aws-resources.sh script creates the configuration, secrets, images, and runbooks then uploads them to AWS. The following functions are used when you run the script: Set the dependency tag The setDependenciesTag function sets the value of the dependency tag to the I2ANALYZE_VERSION defined in the version file. This ensures all the core images are labeled in AWS ECR for that version of the product and any other deployment can reuse them. (TODO: populate with more info for CIR-781) Creates the configuration The AWS reference architecture uses an example configuration. The configuration is created by the createBaseConfiguration function. The generateSolrConfiguration function generates the configuration for Solr from the base configuration. Creates the S3 buckets The createS3Bucket function creates two AWS S3 buckets using the AWS CLI: analyze-containers-aws-ref-arch-config stores the configuration. analyze-containers-aws-ref-arch-resources stores other resources. You can use the S3 management console in AWS to view the contents of the buckets. Creates the secrets The createSecrets function generates all the secrets and passwords that are required for the deployment and pushes them to the secret store (TODO - any doc/reference for this) and shreds the local copy of these secrets. Creates the images The createImages function builds the Docker images that are required for the deployment and tags them with the I2ANALYZE_VERSION defined in the version file. The built images are uploaded to your AWS Elastic Container Registry. To see your AWS ECR, go to ECR console The function uses the utils/buildImages.sh script to build the images, and the examples/aws/utils/push-images.sh script to push the images to AWS. Upload artifacts The uploadArtifacts function is used to upload the configuration to the analyze-containers-aws-ref-arch-config S3 bucket. The function also uploads the CloudFormation templates and i2a-scripts to the analyze-containers-aws-ref-arch-resources bucket. Create runbooks The createRunbooks function creates the runbooks that are required to start and stop the system and uploads them to AWS System Manager . For more information about the runbooks, see Runbooks . deploy-aws.sh script The script creates all the stacks that are required for the i2 Analyze deployment. The stacks are created in the following order: vpc security storage launch-templates sqlserver solr admin-client For more information about the stacks, see Stacks . After the stacks are created, the following runbooks are used to start the system: i2a-UpdateScripts i2a-SolrFirstRun i2a-SqlServerFirstRun For more information about the runbooks, see Runbooks ."
  },
  "versions/2.2.0/content/tools and functions/client_functions.html": {
    "href": "versions/2.2.0/content/tools and functions/client_functions.html",
    "title": "Client utilities",
    "keywords": "Client utilities The clientFunctions.sh file contains functions that you can use to perform actions against the server components of i2 Analyze. Secrets utilities The getSecret function gets a secret such as a password for a user. Status utilities The status utilities report whether a component of i2 Analyze is live. waitForIndexesToBeBuilt The waitForIndexesToBeBuilt function sends a request to the admin/indexes/status endpoint. If the response indicates no indexes are still BUILDING the function returns and prints a message to indicate the indexes have been built. If the indexes are not all built after 5 retries the function will print an error and exit. waitForConnectorToBeLive This function takes (1) the fully qualified domain name of a connector and (2) the port of the connector as its arguments. The waitForConnectorToBeLive function sends a request to the connector's /config endpoint. If the response is 200 , the connector is live. If the connector is not live after 50 tires the function will print an error and exit. getAsyncRequestStatus This function takes (1) the request id of the asynchronous request to get the status of. The getAsyncRequestStatus function makes a request to the Asynchronous Collection API and check the state is marked as completed in the JSON response returned. If the state is not marked as completed , the function returns the response message which contains any error messages that are reported with the asynchronous request. For more information about the Asynchronous Collection API, see REQUESTSTATUS: Request Status of an Async Call . getSolrStatus This function takes (1) a timestamp that is used to specify the point in the logs after which the logs are monitored. The getSolrStatus function used the logs from the Liberty container, and uses a grep command to find specific component availability messages. The function returns the entries in the logs that match message in the grep command. If no matching message is found, the function prints the following error and exits: \"No response was found from the component availability log (attempt: ${i}). Waiting...\" Database Security Utilities changeSAPassword The changeSAPassword function uses the generated secrets to call the changeSAPassword.sh with the initial (generated) sa password and the new (generated) password. createDbLoginAndUser The createDbLoginAndUser function takes a user and a role as its arguments. The function creates a database login and user for the provided user , and assigns the user to the provided role . Execution utilities The execution utilities enable you to run commands and tools from client containers against the server components of i2 Analyze. runSolrClientCommand The runSolrClientCommand function uses an ephemeral Solr client container to run commands against Solr. For more information about the environment variables and volume mounts that are required for the Solr client, see Running a Solr client container . The runSolrClientCommand function takes the command you want to run as an argument. For example: runSolrClientCommand \"/opt/solr/server/scripts/cloud-scripts/zkcli.sh\" -zkhost \"${ZK_HOST}\" -cmd clusterprop -name urlScheme -val https For more information about commands you can execute using the Solr zkcli , see Solr ZK Command Line Utilities runi2AnalyzeTool The runi2AnalyzeTool function uses an ephemeral i2 Analyze Tool container to run the i2 Analyze tools. For more information about the environment variables and volume mounts that are requires for the i2Analyze tool, see Running an i2 Analyze Tool container The runi2AnalyzeTool function takes the i2 tool you want to run as an argument. For example: runi2AnalyzeTool \"/opt/i2-tools/scripts/updateSecuritySchema.sh\" runi2AnalyzeToolAsExternalUser The runi2AnalyzeToolAsExternalUser function uses an ephemeral i2 Analyze Tool container to run commands against the i2 Analyze service via the load balancer as an external user. The container contains the required secrets to communicate with the i2 Analyze service from an external container. For example, if you would like to send a Curl request to the load balancer stats endpoint, run: runi2AnalyzeToolAsExternalUser bash -c \"curl \\ --silent \\ --cookie-jar /tmp/cookie.txt \\ --cacert /tmp/i2acerts/CA.cer \\ --request POST \\\"${FRONT_END_URI}/j_security_check\\\" \\ --header 'Origin: ${FRONT_END_URI}' \\ --header 'Content-Type: application/x-www-form-urlencoded' \\ --data-urlencode 'j_username=Jenny' \\ --data-urlencode 'j_password=Jenny' \\ && curl \\ --silent \\ --cookie /tmp/cookie.txt \\ --cacert /tmp/i2acerts/CA.cer\\ \\\"${FRONT_END_URI}/api/v1/admin/indexes/status\\\"\" runSQLServerCommandAsETL The runSQLServerCommandAsETL function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the etl user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsETL function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsETL bash -c \"/opt/mssql-tools/bin/sqlcmd -N -b -S \\${DB_SERVER} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} -Q \\\"BULK INSERT IS_Staging.E_Person FROM '/var/i2a-data/law-enforcement-data-set-2-merge/person.csv' WITH (FORMATFILE = '/var/i2a-data/law-enforcement-data-set-2-merge/sqlserver/format-files/person.fmt', FIRSTROW = 2)\\\"\" runSQLServerCommandAsi2ETL The runSQLServerCommandAsi2ETL function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the i2etl user, such as executing generated drop/create index scripts, created by the ETL toolkit. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsi2ETL function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsi2ETL bash -c \"${SQLCMD} ${SQLCMD_FLAGS} \\ -S \\${DB_SERVER},${DB_PORT} -U \\${DB_USERNAME} -P \\${DB_PASSWORD} -d \\${DB_NAME} \\ -i /opt/database-scripts/ET5-drop-entity-indexes.sql\" runSQLServerCommandAsFirstStartSA The runSQLServerCommandAsFirstStartSA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the sa user with the initial SA password. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . runSQLServerCommandAsSA The runSQLServerCommandAsSA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the sa user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsSA function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsSA \"/opt/i2-tools/scripts/database-creation/runStaticScripts.sh\" runSQLServerCommandAsDBA The runSQLServerCommandAsDBA function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the dba user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsDBA function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsDBA \"/opt/i2-tools/scripts/clearInfoStoreData.sh\" runSQLServerCommandAsDBB The runSQLServerCommandAsDBB function uses an ephemeral SQL Client container to run database scripts or commands against the Information Store database as the dbb (the backup operator) user. For more information about running a SQL Client container and the environment variables required for the container, see SQL Client . The runSQLServerCommandAsDBB function takes the database script or commands that you want to run as an argument. For example: runSQLServerCommandAsDBA \"runSQLServerCommandAsDBB bash -c \" \"/opt/mssql-tools/bin/sqlcmd -N -b -C -S sqlserver.eia,1433 -U \\\"\\${DB_USERNAME}\\\" -P \\\"\\${DB_PASSWORD}\\\" \\ -Q \\\"USE ISTORE; BACKUP DATABASE ISTORE TO DISK = '/backup/istore.bak' WITH FORMAT;\\\"\"\" runEtlToolkitToolAsi2ETL The runEtlToolkitToolAsi2ETL function uses an ephemeral ETL toolkit container to run ETL toolkit tasks against the Information Store using the i2 ETL user credentials. For more information about running the ETL Client container and the environment variables required for the container, see ETL Client . For more information about running the ETL toolkit container and the tasks that you can run, see ETL The runEtlToolkitToolAsi2ETL function takes command that you want to run as an argument. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/addInformationStoreIngestionSource --ingestionSourceName EXAMPLE_1 --ingestionSourceDescription EXAMPLE_1\" runEtlToolkitToolAsDBA Some ETL tasks must be performed by the DBA. The runEtlToolkitToolAsDBA function used the same ephemeral ETL toolkit container but uses the DBA user instead of the ETL user. For more information about running the ETL Client container and the environment variables required for the container, see ETL Client . For more information about running the ETL toolkit container and the tasks that you can run, see ETL The runEtlToolkitToolAsDBA function takes command that you want to run as an argument. For example: runEtlToolkitToolAsDBA bash -c \"/opt/ibm/etltoolkit/enableMergedPropertyValues --schemaTypeId ET5\" Volume utilities getVolume The getVolume function uses an ephemeral Red Hat UBI Docker image to update a local directory with the contents of a specified volume. The getVolume function takes the local directory, volume name, and a directory in that volume as arguments. For example, to get the contents from the run/secrets directory in the liberty1_secrets volume into your local /dev-environment-secrets/simulated-secret-store directory, run: getVolume \"/dev-environment-secrets/simulated-secret-store\" \"liberty1_secrets\" \"/run/secrets\" updateVolume The updateVolume function uses an ephemeral Red Hat UBI Docker image to update a volume with the contents of a local directory. The updateVolume function takes the local directory, volume name, and a directory in that volume as arguments. For example, to update the the run/secrets directory in the liberty1_secrets volume with the content of the local /dev-environment-secrets/simulated-secret-store directory, run: updateVolume \"/dev-environment-secrets/simulated-secret-store\" \"liberty1_secrets\" \"/run/secrets\""
  },
  "versions/2.2.0/content/tools and functions/common_variables.html": {
    "href": "versions/2.2.0/content/tools and functions/common_variables.html",
    "title": "Common Variables",
    "keywords": "Common Variables All scripts in the containerized environment use variables to configure the system such as location of file paths, host names, port etc These variables are all stored in a central script in utils/commonVariables.sh script. The following descriptions explains the variables in the createVariables.sh script in more detail. License Variables This section contains the license variables that are used by the containers. These variables must be accepted before running the system. Network Security Variables This section contains the variables to turn on or off SSL. For more information about switching SSL on or off see the image and containers section or secrets section Ports This section contains the variables for all the ports that are exposed on the containers. Connection Information This section contains connection variables such as the Zookeeper hosts that are used by various containers Localization variables This section contains the localization variables. These variables are used by the createConfiguration.sh script to set up the configuration in the correct locale. Image names This section contains the image names for all the images built by the buildImages.sh script. These variables are also used by the clientFunctions.sh and severFunctions.sh scripts. Container names This section contains the container names for all the containers run in the environment. Volume names This section contains the volume names for all the volumes used by the containers run in the environment. User names This section contains all the usernames used by the servers in the containerized environment. Container secrets paths This section contains variables for the container secrets and certificate directories. For more information about these variables see the secrets section. Security configuration This section contains security variables used by the generateSecrets.sh scripts. For example the duration or the certificate or the certificate key size. Database variables This section contains database connection variables, database paths, OS type and other database related variables used by the scripts. Path variables The section contains the various paths in the environment that are used by the scripts. Front end URI This section contains the variable for the front end URI."
  },
  "versions/2.2.0/content/tools and functions/create_configuration.html": {
    "href": "versions/2.2.0/content/tools and functions/create_configuration.html",
    "title": "Creating the configuration",
    "keywords": "Creating the configuration Before you can deploy i2 Analyze, the configuration for the deployment must exist. You can create the configuration by running the utils/createConfiguration.sh script. The following descriptions explain what the createConfiguration.sh script does in more detail. Creates the base configuration The configuration is created in examples/pre-prod/ . The configuration is created from the all-patterns base configuration in the minimal toolkit. The base configuration can be used for any of the deployment patterns, however some properties are for specific patterns. By default, a schema and security schema are specified. The schemas that are used are based on the law-enforcement-schema.xml . Copies the JDBC drivers The JDBC drivers are copied from the pre-reqs/jdbc-drivers directory to the configuration/environment/common/jdbc-drivers directory in your configuration. Configures Form Based Authentication The createConfiguration.sh scripts configures form based authentication in the examples/pre-prod/configuration/fragments/common/WEB-INF/classes/server.extensions.xml ."
  },
  "versions/2.2.0/content/tools and functions/create_environment.html": {
    "href": "versions/2.2.0/content/tools and functions/create_environment.html",
    "title": "Creating the environment",
    "keywords": "Creating the environment Before you can deploy i2 Analyze in a containerized environment, the local environment must be created. The local environment requires the prerequisites to be in the correct locations, and copies some artifacts and tools from the i2 Analyze toolkit. You can create the local environment by running the utils/createEnvironment.sh script. The following descriptions explain what the createEnvironment.sh script does in more detail. Extracts the i2 Analyze minimal toolkit The script extracts the toolkit tar.gz located in the pre-reqs directory into the the pre-reqs/i2Analyze directory. Populate image clients The createEnvironment.sh script creates populate client images with the contents of the i2-tools & scripts folder of the toolkit. ETL toolkit The createEnvironment.sh script creates the ETL toolkit that is built into the etl toolkit image. The etl toolkit is created in images/etl_client/etltoolkit . Example connector application The createEnvironment.sh script creates the example connector that is built into the connector image. The application consists of the contents of i2analyze/toolkit/examples/connectors/example-connector . The connector application is created in images/example_connector/app . Liberty application The createEnvironment.sh script creates the i2 Analyze liberty application that is built into the Liberty base image. The application consists of files that do not change based on the configuration. After the application is created, you do not need to modify it. The application is created in images/liberty_ubi_base/application ."
  },
  "versions/2.2.0/content/tools and functions/deploy.html": {
    "href": "versions/2.2.0/content/tools and functions/deploy.html",
    "title": "Deploy and start i2 Analyze",
    "keywords": "Deploy and start i2 Analyze This topic describes how to deploy and start i2 Analyze in a containerized environment. For an example of the activities described, see the deploy.sh Running Solr and ZooKeeper The running Solr and ZooKeeper section runs the required containers and creates the Solr cluster and ZooKeeper ensemble. Create Solr cluster The createSecureCluster function creates the secure Solr cluster for the deployment. The function includes a number of calls that complete the following actions: The runZK server function runs the ZooKeeper containers that make up the ZooKeeper ensemble. For more information about running a ZooKeeper container, see ZooKeeper . In deploy.sh , 3 ZooKeeper containers are used. The runSolrClientCommand client function is used a number of times to complete the following actions: Create the znode for the cluster. i2 Analyze uses a ZooKeeper connection string with a chroot. To use a chroot connection string, a znode with that name must exist. For more information, see SolrCloud Mode . Set the urlScheme to be https . Configure the Solr authentication by uploading the security.json file to ZooKeeper. For more information about the function, see runSolrClientCommand . The runSolr server function runs the Solr containers for the Solr cluster. For more information about running a Solr container, see Solr . In deploy.sh , 2 Solr containers are used. At this point, your ZooKeepers are running in an ensemble, and your Solr containers are running in SolrCloud Mode managed by ZooKeeper. Initializing the Information Store database The initializing the Information Store database section creates a persistent database backup volume, runs the database container and configures the database management system. The database backup volume is created first with the Docker command: docker volume create \"${SQL_SERVER_BACKUP_VOLUME_NAME}\" so that the volume will not be automatically deleted when the SQL Server container is remove. This helps maintain any backups created while running a SQL server container. For more information about docker storage, see Docker Storage . Running the database server container The runSQLServer server function creates the secure SQL Server container for the deployment. For more information about building the SQL Server image and running a container, see Microsoft SQL Server . Before continuing, deploy.sh uses the waitForSolrToBeLive client function and waitForSQLServerToBeLive common function to ensure that Solr and SQL Server are running. For more information, see Status utilities Configuring SQL Server The configureSecureSqlServer function uses a number of client and server functions to complete the following actions: The changeSAPassword client function is used to change the sa user's password. For more information, see changeSAPassword Generate the static Information Store database scripts. The runi2AnalyzeTool client function is used to run the generateStaticInfoStoreCreationScripts.sh tool. runi2AnalyzeTool Generate static database scripts tool Create the Information Store database and schemas. The runSQLServerCommandAsSA client function is used to run the runDatabaseCreationScripts.sh tool. runSQLServerCommandAsSA runDatabaseCreationScripts.sh Create the database roles, logins, and users. For more information about the database users and their permissions, see Database users . The runSQLServerCommandAsSA client function runs the createDbRoles.sh script. The createDbLoginAndUser client function creates the logins and users. The runSQLServerCommandAsSA client function runs the applyBuiltInSQLServerRoles.sh script. Grant the dba user the required permissions in the msdb and master databases, this grants the correct permissions for the Deletion by Rule feature of i2Analyze. The runSQLServerCommandAsSA client function runs the configureDbaRolesAndPermissions.sh script. Make the etl user a member of the SQL server sysadmin group to allow this user to perform bulk inserts into the external staging tables. The runSQLServerCommandAsSA client function runs the addEtlUserToSysAdminRole.sh script. Run the static scripts that create the Information Store database objects. The runSQLServerCommandAsSA client function is used to run the runStaticScripts.sh tool. runSQLServerCommandAsSA Run static database scripts tool Configuring Solr and ZooKeeper The configuring Solr and ZooKeeper sections creates Solr configuration and then configures the Solr cluster and creates the Solr collections. The generateSolrSchemas.sh i2-tool creates the solr directory in examples/pre-prod/configuration/solr/generated_config . This directory contains the managed-schema, Solr synonyms file and Solr config files for each index. The configureSecureSolr function uses the runSolrClientCommand client function to upload the managed-schema , solr.xml , and synonyms file for each collection to ZooKeeper. For example: runSolrClientCommand solr zk upconfig -v -z \"${ZK_HOST}\" -n daod_index -d /conf/solr_config/daod_index The setClusterPolicyForSecureSolr function uses the runSolrClientCommand client function to set a cluster policy such that each host has 1 replica of each shard. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer -X POST -H Content-Type:text/xml -d '{ \\\"set-cluster-policy\\\": [ {\\\"replica\\\": \\\"<2\\\", \\\"shard\\\": \\\"#EACH\\\", \\\"host\\\": \\\"#EACH\\\"}]}' \\\"${SOLR1_BASE_URL}/api/cluster/autoscaling\\\"\" For more information about Solr policies, see Autoscaling Policy and Preferences . The createCollectionForSecureSolr function uses the runSolrClientCommand client function to create each Solr collection. For example: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/admin/collections?action=CREATE&name=main_index&collection.configName=main_index&numShards=1&maxShardsPerNode=4&rule=replica:<2,host:*\\\"\" For more information about the Solr collection API call, see CREATE: Create a Collection . Configuring the Information Store database The configuring the Information Store database section creates objects within in the database. Generate the dynamic database scripts that create the schema specific database objects. The runi2AnalyzeTool client function is used to run the generateDynamicInfoStoreCreationScripts.sh tool. runi2AnalyzeTool Generate dynamic Information Store creation scripts tool Run the generated dynamic database scripts. The runSQLServerCommandAsSA client function is used to run the runDynamicScripts.sh tool. runSQLServerCommandAsSA Run dynamic Information Store creation scripts tool Configuring the Example Connector The configuring example connector section runs the example connector used by the i2 Analyze application. The runExampleConnector server function runs the example connector application. The waitForConnectorToBeLive client function checks the connector is live before allowing the script to proceed. Configuring i2 Analyze The configuring i2 Analyze section runs the Liberty containers that run the i2 Analyze application. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . The runLiberty server function runs a Liberty container from the configured image. For more information, see Running a Liberty container In deploy.sh , 2 liberty containers are used. Starting the load balancer. The runLoadBalancer functions in serverFunctions.sh runs HAProxy as a load balancer in a Docker container. The load balancer configuration we use is generated from the haproxy-template.cfg template file. The load balancer routes requests to the application to both Liberty servers that are running. The configuration that used is a simplified configuration for example purposes and is not to be used in production. For more information about configuring a load balancer with i2 Analyze, see Load balancer . Before continuing, deploy.sh uses the waitFori2AnalyzeServiceToBeLive common function to ensure that Liberty is running. Deploy the system match rules. The runSolrClientCommand client function is used to run the runIndexCommand.sh tool. The tool is run twice, once to update the match rules file and once to switch the match indexes. For more information, see Manage Solr indexes tool ."
  },
  "versions/2.2.0/content/tools and functions/etl_tools.html": {
    "href": "versions/2.2.0/content/tools and functions/etl_tools.html",
    "title": "ETL Tools",
    "keywords": "ETL Tools This topic describes how to perform ETL tasks by using the ETL toolkit in a containerized deployment of i2 Analyze. All of the tools described here are located in the images/etl_client directory. The etl_client directory is populated when running createEnvironment.sh . The runEtlToolkitToolAsi2ETL client function is used to run the ETL tools described in this topic as the i2ETL user. For more information about this client function, see runEtlToolkitToolAsi2ETL Building an ETL Client image The ETL client image is built from the Dockerfile in images/etl_client . The following docker run command builds the configured image: docker build -t \"etl_client:4.3.5\" \"images/etl_client\" Add Information Store ingestion source The addInformationStoreIngestionSource tool defines an ingestion source in the Information Store. For more information about ingestion sources in the Information Store, see Defining an ingestion source . You must provide the following arguments to the tool: Argument Description Maximum characters n A unique name for the ingestion source 30 d A description of the ingestion source that might appear in the user interface 100 Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/addInformationStoreIngestionSource -n <> -d <> \" Create Information Store staging table The createInformationStoreStagingTable tool creates the staging tables that you can use to ingest data into the Information Store. For more information about creating the tables, see Creating the staging tables . You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the staging table for sn The name of the database schema to create the staging table in tn The name of the staging table to create Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/createInformationStoreStagingTable -stid <> -sn <> -tn <> \" Ingest Information Store records The ingestInformationStoreRecords is used to ingest data into the Information Store. For more information about ingesting data into the Information Store, see The ingestInformationStoreRecords toolkit task You can use the following arguments with the tool: Argument Description imf The full path to the ingestion mapping file. imid The ingestion mapping identifier in the ingestion mapping file of the mapping to use im Optional: The import mode to use. Possible values are STANDARD, VALIDATE, BULK, DELETE, BULK_DELETE or DELETE_PREVIEW. The default is STANDARD. icf Optional: The full path to an ingestion settings file il Optional: A label for the ingestion that you can use to refer to it later lcl Optional: Whether (true/false) to log the links that were deleted/affected as a result of deleting entities Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/ingestInformationStoreRecords -imf <> -imid <> -im <>\" Sync Information Store records The syncInformationStoreCorrelation tool is used after an error during correlation, to synchronize the data in the Information Store with the data in the Solr index so that the data returns to a usable state Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/syncInformationStoreCorrelation\" Duplicate provenance check The duplicateProvenanceCheck tool can be used for identifying records in the Information Store with duplicate origin identifiers. Any provenance that has a duplicated origin identifier is added to a staging table in the Information Store. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTool bash -c \"/opt/ibm/etltoolkit/syncInformationStoreCorrelation\" Duplicate provenance delete The duplicateProvenanceDelete tool deletes (entity/link) provenance from the Information Store that has duplicated origin identifiers. The provenance to delete is identified in the staging tables created by the duplicateProvenanceCheck tool. You can provide the following argument to the tool: Argument Description stn The name of the staging table that contains the origin identifiers to delete. If no arguments are provided, duplicate origin identifiers are deleted from all staging tables. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/syncInformationStoreCorrelation\" Generate Information Store index creation scripts The generateInformationStoreIndexCreationScript tool generates the scripts that create the indexes for each item type in the Information Store. For more information, see database index management You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the index creation scripts for. op The location to create the scripts. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTask bash -c \"/opt/ibm/etltoolkit/generateInformationStoreIndexCreationScript -op <> -stid <> \" Generate Information Store index drop scripts The generateInformationStoreIndexDropScript tool generates the scripts that drop the indexes for each item type in the Information Store. For more information, see database index management You must provide the following arguments to the tool: Argument Description stid The schema type identifier of the item type to create the index drop scripts for. op The location to create the scripts. Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitTask bash -c \"/opt/ibm/etltoolkit/generateInformationStoreIndexDropScript --op <> -stid <> \" Delete orphaned database objects The deleteOrphanedDatabaseObjects tool deletes (entity/link) database objects that are not associated with an i2 Analyze record from the Information Store. You can provide the following arguments to the tool: Argument Description iti Optional: The schema type identifier of the item type to delete orphaned database objects for. If no item type id is provided, orphaned objects for all item types are removed Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/deleteOrphanedDatabaseObjects -iti <> \" Disable merged property values The disableMergedPropertyValues tool removes the database views used to define the property values of merged i2 Analyze records. You can provide the following arguments to the tool: Argument Description etd The location of the root of the etl toolkit. stid The schema type identifier to disable the views for. If no schema type identifier is provided, the views for all of the item types are be removed Use the runEtlToolkitToolAsi2ETL client function to run the tool. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/disableMergedPropertyValues -etd <> -stid <>\" For more information about correlation, see Information Store data correlation Enable merge property values The enableMergedPropertyValues tool creates the database views used to define the property values of merged i2 Analyze records. You can provide the following arguments to the tool: Argument Description etd The location of the root of the etl toolkit. stid The schema type identifier to create the views for. If no schema type identifier is provided, the views for all of the item types are generated. If the views already exist, they are overwritten. Use the runEtlToolkitToolAsDBA client function to run the tool as the database administrator. For example: runEtlToolkitToolAsi2ETL bash -c \"/opt/ibm/etltoolkit/enableMergedPropertyValues -etd <> -stid <> \" For more information about correlation, see Information Store data correlation"
  },
  "versions/2.2.0/content/tools and functions/i2analyze_tools.html": {
    "href": "versions/2.2.0/content/tools and functions/i2analyze_tools.html",
    "title": "i2 Analyze tools",
    "keywords": "i2 Analyze tools The i2 Analyze deployment toolkit includes a number of tools that enable you to deploy and administer i2 Analyze. Note the TOOLKIT_DIR environment variable is set by the i2Tools image and does not need to be passed to the container at runtime. Generate Information Store scripts The generateInfoStoreToolScripts.sh script is used to generate Information Store scripts from templates in i2-tools/scripts/database-template directory located inside the i2Tools image. The generated scripts end up in in the GENERATED_DIR . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. GENERATED_DIR The root location where any generated scripts are created. DB_TRUSTSTORE_LOCATION The location of the truststore.jks . Schema update tool The generateUpdateSchemaScripts.sh script is used to update the i2 Analyze schema in the Information Store. To update the schema, the tool generates a number of SQL scripts that must be run against the Information Store database. When you run the tool, it compares the schema file in the configuration directory against the schema that is stored in the IS_META.I2_SCHEMAS table in Information Store database. If they are different, the schema in the database is updated with the one from the configuration. Then, the IS_DATA table structure is compared with the schema IS_META.I2_SCHEMAS . If the IS_DATA objects are different, the scripts to update the objects are generated. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. /update is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration ./database/sqlserver/InfoStore/generated/update You can run the generated scripts against the database manually, or you can use the Run database scripts tool . Security schema update tool The updateSecuritySchema.sh script is used to update the security schema in a deployment of i2 Analyze. When you run the script, the security schema file in the configuration directory is compared against the security schema that is stored in the IS_META.I2_SCHEMAS table in Information Store database. If they are different, the security schema in the database is updated with the one from the configuration. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration Run database scripts tool You can run any generated scripts against the database manually, or you can use the runDatabaseScripts.sh script. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires the following environment variables to be set: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. Manage Solr indexes tool By using the indexControls.sh script, you can pause and resume indexing on any index, upload system match rules, and switch a standby index to be the live index. Note: Switching from a standby index to live is possible for the following index types: chart, match, and main. The script requires the following environment variables to be present: Environment Variable Description ZOO_USERNAME The name of the administrator user for performing administration tasks. ZOO_PASSWORD The password for the administrator user. ZK_HOST The connection string for each ZooKeeper server to connect to. ZOO_SSL_TRUST_STORE_LOCATION The location of the truststore.jks file to be used for SSL connections to ZooKeeper. ZOO_SSL_TRUST_STORE_PASSWORD The password for the truststore. ZOO_SSL_KEY_STORE_LOCATION The location of the keystore.jks file to be used for SSL connections to ZooKeeper. ZOO_SSL_KEY_STORE_PASSWORD The password for the keystore. CONFIG_DIR The location of your configuration. This is used to locate the system-match-rules.xml . TASK The index control task to perform. You can specify the following values for TASK : UPDATE_MATCH_RULES SWITCH_STANDBY_MATCH_INDEX_TO_LIVE CLEAR_STANDBY_MATCH_INDEX REBUILD_STANDBY_MAIN_INDEX SWITCH_STANDBY_MAIN_INDEX_TO_LIVE CLEAR_STANDBY_MAIN_INDEX REBUILD_STANDBY_CHART_STORE_INDEX SWITCH_STANDBY_CHART_STORE_INDEX_TO_LIVE CLEAR_STANDBY_CHART_STORE_INDEX PAUSE_INDEX_POPULATION RESUME_INDEX_POPULATION For more information about the index control tasks, see Index control toolkit tasks . Information Store database consistency tool The databaseConsistencyCheck.sh script checks that the database objects in the IS_DATA database schema are consistent with the information in IS_META . The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration If the database is a consistent state with the schema, the following message will be returned in the console The Database is in a consistent state. If the database is an inconsistent state with the schema, errors will be reported in the console where the tool was run. For example: The following changes to the Information Store database tables and/or views have been detected: MODIFIED ITEM TYPES Item type: Arrest ADDED PROPERTY TYPES Property type: Voided Item type: Arrest The following changes to the Information Store database tables and/or views have been detected: ADDED ITEM TYPES Item type: Vehicle > ERROR [DatabaseConsistencyCheckCLI] - Information Store database is inconsistent with the schema stored in the database Schema validation tool The validateSchemaAndSecuritySchema.sh script checks that the database objects in the IS_META database schema are consistent with the i2 Analyze schema and the security schema in the IS_META.I2_SCHEMAS . The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration If the schemas are valid no errors will be reported. However, if the schemas are the same as the version of the database this information will be returned to the console. If the schemas contain validation errors will be reported to the console where the tool was run. For example: VALIDATION ERROR: Schema contains duplicate type EntityType with ID 'ET5'. Generate static database scripts tool The generateStaticInfoStoreCreationScripts.sh script injects values from your configuration into the static database scripts. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The root location where any generated scripts are created. /static is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the toolkit/i2-tools/scripts directory: ../../toolkit ../configuration ../scripts/database/{dialect}/InfoStore/generated/static The script creates the .sql files in the location of the GENERATED_DIR that was supplied to the container. Run static database scripts tool The runStaticScripts.sh script runs the static scripts that create the Information Store database. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the static scripts to be run. /static is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Generate dynamic Information Store creation scripts tool The generateDynamicInfoStoreCreationScripts.sh script creates the .sql scripts that are required to create the database objects that align to your schema in the configuration. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description TOOLKIT_DIR The location of the root of the deployment toolkit. CONFIG_DIR The location of the root of the configuration directory. GENERATED_DIR The location where any generated scripts are created. /dynamic is appended to the specified path. If these environment variables are not present, the script uses the following relative paths from the i2-tools/scripts directory: ../../toolkit ../configuration ../scripts/database/{database dialect}/InfoStore/generated/dynamic The script creates the .sql files in the location of the GENERATED_DIR that was supplied to the container. Run dynamic Information Store creation scripts tool The runDynamicScripts.sh script runs the dynamic scripts that create the Information Store database. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The runSQLServerCommandAsSA client function is used to run the runDynamicScripts.sh script that create the Information store and schemas. runSQLServerCommandAsSA The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the scripts to be run. /dynamic is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Run database creation scripts tool The runDatabaseCreationScripts.sh script runs the create-database-storage.sql & 0001-create-schemas.sql database creation scripts. These scripts create the Information Store database and schemas. This tool can be found in the GENERATED_DIR that was generated by the generateInfoStoreToolScripts.sh script. The script requires connection property values to be loaded from environment variables or from the Connection.properties file in the configuration. For more information about the connection properties, see Loading connection properties . The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The location of the folder that contains the scripts to be run. /dynamic is appended to the specified path. CA_CERT_FILE The location of the certificate file on the docker container. The GENERATED_DIR environment variable must be present and is not defaulted. Remove data from the Information Store tool The runClearInfoStoreData.sh script removes all the data from the Information Store database. The script requires the following environment variables to be present: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user. DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. GENERATED_DIR The path to the root of the directory that contains the generated static scripts. /static is appended to the value of GENERATED_DIR . The GENERATED_DIR environment variable must be present and is not defaulted. For more information about clearing data from the Information Store, see Clearing Data Create static Liberty application tool The createLibertyStaticApp.sh creates the static application to be hosted in Liberty. The tool combines the contents of various folders to create the application, in a similar way to the deployLiberty toolkit task. To run the tool, you must provide a location to create the application files as an argument. Alternatively if you are running the tool from outside of the toolkit/i2-tools/scripts folder the following environment variable must be set: Environment Variable Description TOOLKIT_DIR The location of the toolkit. When you call the createLibertyStaticApp.sh script, you must pass the location where the Liberty application directory is created. Connection properties Some of the tools require database connection properties such as database username, database truststore location & password for example in order to communicate with the database so that the tool can run. These properties can be loaded from a file called Connection.properties that is located in your configuration in the following directory configuration/fragments/common/WEB-INF/classes/ . For example: DBName=ISTORE DBDialect=sqlserver DBServer=sqlserver.eia DBPort=1433 DBOsType=UNIX DBSecureConnection=true DBCreateDatabase=false db.installation.dir=/opt/mssql-tools db.database.location.dir=/opt/mssql In the docker environment, these properties can be alternatively loaded via environment variables passed into the container. The following environment variables are supported for supplying connection properties: Environment Variable Description DB_USERNAME The name of the user that has the write access to the database. DB_PASSWORD The password for the user (Note: you can set DB_PASSWORD_FILE with the location of the file containing the password). DB_DIALECT The dialect for the database. Can be sqlserver or db2 . DB_SERVER The fully qualified domain name of the database server. DB_PORT Specifies the port number to connect to the Information Store. DB_NAME The name of the Information Store database. DB_OS_TYPE The Operating System that the database is on. Can be UNIX , WIN , or AIX . DB_INSTALL_DIR Specifies the database CMD location. DB_LOCATION_DIR Specifies the location of the database. DB_TRUSTSTORE_LOCATION The location of the truststore.jks . * DB_TRUSTSTORE_PASSWORD The password for the truststore. * The security settings are only required if your database is configured to use a secure connection. Generate Solr Config The generateSolrSchemas.sh generates Solr Config by taking the template schema-template-definition.xml file from the configuration/solr and generating solr config per index. For each index, the tool will generate solrconfig.xml , managed_schema and synonyms-<LOCALE>.txt file. NOTE: to change SOLR locale uncomment the relevant section from the schema-template-definition.xml file before deploying. The script requires the following environment variables to be present: Environment Variable Description CONFIG_DIR The location of the root of the configuration directory."
  },
  "versions/2.2.0/content/tools and functions/reset_repository.html": {
    "href": "versions/2.2.0/content/tools and functions/reset_repository.html",
    "title": "Resetting the repository",
    "keywords": "Resetting the repository The local repository can be returned to its initial state by running the resetRepository.sh script. The following artifacts are removed by the resetRepository.sh : All Docker resources such as images, containers, volumes and networks. The simulated-secret-store directory. Resources that are copied to various image directories when the environment is created. Database scripts that are generated as part of the deployment process. The ETL toolkit that is located in images/etl_client/etltoolkit The example connector that is located in images/example_connector/app After you run the resetRepository.sh script, you must follow the getting started process from the Creating a containerized deployment section."
  },
  "versions/2.2.0/content/tools and functions/tools.html": {
    "href": "versions/2.2.0/content/tools and functions/tools.html",
    "title": "Tools and functions",
    "keywords": "Tools and functions The documentation in this section describes the tools and functions that are used to deploy i2 Analyze in a containerized environment."
  },
  "versions/2.2.0/content/upgrade/upgrade.html": {
    "href": "versions/2.2.0/content/upgrade/upgrade.html",
    "title": "Upgrading i2 Analyze Pre-prod Environment",
    "keywords": "Upgrading i2 Analyze Pre-prod Environment This section describes an example of the process to upgrade i2 Analyze in a containerized environment. Updating the configuration includes the following high-level steps: * Backing up Solr and the database * Rebuilding the Docker images for the new version of i2 Analyze * Create the upgrade change set * Remove the previous containers and volumes * Upgrade Solr, the database, the Liberty application, and the example connector Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Before you upgrade, follow the instructions to prepare the prerequisites in Upgrading . The example/pre-prod/walkthroughs/upgrade/upgradeWalkthrough.sh script is a worked example that demonstrates how to upgrade a deployment in a containerized environment. Creating the Solr backup In the upgradeWalkthrough.sh script, the Backup Solr section demonstrates the commands that are used to back up the Solr collections and system match rules. For more information about backing up Solr, see [Backing up Solr](../backup and restore/backup.md#BackingUpTheSolrCollections). Creating SQL Server backups In the upgradeWalkthrough.sh script, the Backup SQL Server section demonstrates the commands that are used to back up the SQL Server Information Store database. For more information about backing up the database, see [Backing up the Information Store database](../backup and restore/backup.md#BackingUpTheInformationStoreDatabase). Creating the change change set In the upgradeWalkthrough.sh script, the Create change set section demonstrates running the createChangeSet.sh script that creates the upgrade change set. The change set is created in XXX and contains the database scripts that are required to upgrade the Information Store database. Remove the previous containers In the upgradeWalkthrough.sh script, the Remove previous containers section demonstrates the commands to delete the previous containers and remove any Docker volumes. Rebuilding the images In the upgradeWalkthrough.sh script, the Rebuilding images section demonstrates running the buildImages.sh script to build images with for the new version of i2 Analyze. Upgrading Solr In the upgradeWalkthrough.sh script, the Upgrading Solr section demonstrates the commands that are used to run the new Solr and ZooKeeper containers. The 3 ZooKeeper containers and 2 Solr containers are started. The Solr collections are configured, including the new vq_index collection. Restoring the Solr collections The non-transient Solr collections that were backed up are restored. For more information about restoring Solr, see [Restoring Solr](../backup and restore/restore.md#RestoreSolr). Restoring the Information Store database In the upgradeWalkthrough.sh script, the Restore Information Store section demonstrates the commands that are used to restore the database to a new SQL Server container. Upgrading the Information Store database In the upgradeWalkthrough.sh script, the Upgrading Information Store section demonstrates the commands that are used to run the generated database scripts that upgrade the Information Store database. Upgrading example connector In the upgradeWalkthrough.sh script, the Upgrade example connector section demonstrates the commands that are used to run the example connector. Upgrading Liberty In the upgradeWalkthrough.sh script, the Upgrading Liberty section demonstrates the commands that are used to build the new Liberty configured image. Two Liberty containers are started using the new configured image. Then, the load balancer container is started."
  },
  "versions/2.2.0/content/walkthroughs/add_connector.html": {
    "href": "versions/2.2.0/content/walkthroughs/add_connector.html",
    "title": "Adding a new Connector",
    "keywords": "Adding a new Connector This section describes an example of the process of adding a new Connector. You can use addExampleConnector.sh script to add a new second example connector to the example deployment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Updating Connectors.json To make liberty aware of the new connector, you need to add a new connector definition to the connectors.json. The addConnectorToConnectorsJson can be used to do this. See Add example Connector to connectors.json section of the walkthrough script. Running a new Connector container To add a connector in the Docker environment, you need to run a new Connector container. See the Add example Connector section of the walkthrough script. The runExampleConnector server function in the serverFunctions.sh is used to run a new Example connector container. Updating your configuration After the connectors.json has been updated and the example connector has been run, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "versions/2.2.0/content/walkthroughs/add_solr_node.html": {
    "href": "versions/2.2.0/content/walkthroughs/add_solr_node.html",
    "title": "Adding a new Solr node",
    "keywords": "Adding a new Solr node This section describes an example of the process of adding a new Solr Node to an existing Solr cluster and using the Solr Collections API to add a replica on the newly created Solr node. You can use addSolrNodeWalkthrough.sh script to add a Solr node to the example deployment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Running a new Solr container To add a Solr node to an existing Solr collection in the Docker environment, you run a new Solr container. See the Running a new Solr container section of the walkthrough script. The runSolr server function in the addSolrNodeWalkthrough.sh is used to run a new Solr container with a node that is added to the existing cluster. For more information about running a Solr container, see Solr . The following environment variables are used to specify the hostname of the container and to add the node to the existing cluster: SOLR_HOST specifies the fully qualified domain name of the Solr container. ZK_HOST Specifies the connection string for each ZooKeeper server to connect to. To connect to more than one ZooKeeper server, the values must be in comma separated list. When the connection string connects to the existing ZooKeeper quorum, the new Solr node is automatically added to the Solr cluster. Adding a Solr replica To use the new Solr node, you must add a replica for a shard and create it on the new Solr node. See the Adding a Solr replica section of the walkthrough script. To add a replica, use the Solr Collections API. For more information about the API command, see ADDREPLICA: Add Replica The following curl command is an example that creates a replica for shard1 in the main_index collection on the new Solr node: curl -u \"${SOLR_ADMIN_DIGEST_USERNAME}\":\"${SOLR_ADMIN_DIGEST_PASSWORD}\" --cacert /CA/CA.cer \"${SOLR1_BASE_URL}/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard1&node=${SOLR3_FQDN}:8983_solr\" In the addSolrNodeWalkthrough.sh script, the runSolrClientCommand function contains an example of how to run the curl command in a containerized environment: runSolrClientCommand bash -c \"curl -u \\\"\\${SOLR_ADMIN_DIGEST_USERNAME}:\\${SOLR_ADMIN_DIGEST_PASSWORD}\\\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \\\"${SOLR1_BASE_URL}/solr/admin/collections?action=ADDREPLICA&collection=main_index&shard=shard1&node=${SOLR3_FQDN}:8983_solr\\\"\" For more information about command parsing, see Command parsing The example above uses ${SOLR3_FQDN} , but you can use the fully qualified domain name of any Solr node in the cluster."
  },
  "versions/2.2.0/content/walkthroughs/clear_data.html": {
    "href": "versions/2.2.0/content/walkthroughs/clear_data.html",
    "title": "Clearing data from a deployment",
    "keywords": "Clearing data from a deployment This section describes an example of the process to clear the data from your deployment in a Docker environment. Clearing the data from a deployment includes the following high-level steps: Removing the Liberty containers Clearing the search index Creating and running a delete query Removing the collection properties Clearing data from the Information Store Remove all of the data in the Information Store Running the Liberty containers The clearDataWalkthrough.sh script is a worked example that demonstrates how to clear the data from the Information Store in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you clear the data from a deployment, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Clearing the search index Creating and running a delete query To clear the search index, run a Solr delete query against your indexes via the Solr API. You can run the delete query by using a curl command. In Solr, data is stored as documents. You must remove every document from each collection in your deployment. The runSolrClientCommand client function is used to run the curl commands that remove the documents from each collection. For more information about the function, see runSolrClientCommand . The following curl command removes every document from a the main_index : curl -u \"${SOLR_ADMIN_DIGEST_USERNAME}:${SOLR_ADMIN_DIGEST_PASSWORD}\" --cacert ${CONTAINER_SECRETS_DIR}/CA.cer \"${SOLR1_BASE_URL}/solr/main_index/update?commit=true\" -H Content-Type:\"text/xml\" --data-binary \"<delete><query>*:*</query></delete>\" See the Clearing the search index section of the walkthrough script. For more information about command parsing, see Command parsing Removing the collection properties The runSolrClientCommand client function is used to remove the file from ZooKeeper. For more information about the function, see runSolrClientCommand . The following zkcli call removes the collection properties for the main_index . zkcli.sh -zkhost \"${ZK_HOST}\" -cmd clear \"/collections/main_index/collectionprops.json\" See the Clearing the search index section of the walkthrough script. The collection properties must be removed for any main, match, or chart collections. The collectionprops.json file is recreated when the i2 Analyze application is started. Clearing data from the Information Store See the Clearing the Information Store database section of the walkthrough script. The runSQLServerCommandAsDBA client function is used to run the clearInfoStoreData.sh tool to remove the data from the Information Store. runSQLServerCommandAsDBA clearInfoStoreData Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "versions/2.2.0/content/walkthroughs/correlated_data.html": {
    "href": "versions/2.2.0/content/walkthroughs/correlated_data.html",
    "title": "Defining the property values of merged records",
    "keywords": "Defining the property values of merged records This section describes an example of the process to define how property values of merged records are calculated in a Docker environment. The process includes the following high-level steps: Enabling merged property values To define the view, enable and then modify it to meet your correlation requirements Updating property value definitions Ingest data that correlates and review its properties The mergedPropertyValuesWalkthrough.sh script is a worked example that demonstrates how to enables and modify the views in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Default merge property behavior To demonstrate the default behavior, use the ingestDataWalkthrough.sh script to ingest some data the correlates. For more information about ingesting the data, see Ingesting data into the Information Store . During the ingestion walk-through, the default behavior is used to determine the property values of the correlated record. In the default behavior, the property values from the merge contributor with the most recent value for source_last_updated are used. For more information about the how the property values for merged records are calculated, see Define how property values of merged records are calculated After you ingest the data, in Analyst's Notebook Premium search for Julia Yochum and add the returned entity to the chart. Keep the chart open for the remainder of the walkthrough script. Enabling merged property values To inform i2 Analyze that you intend to define the property values of merged records, run the enableMergedPropertyValues.sh tool. You can take control of the property values for records of specific item types, or all item types in the i2 Analyze schema. Note that this operation must be performed by the database administrator. See the Enabling merged property values section of the walkthrough script. The runEtlToolkitToolAsDBA client function is used to run the enableMergedPropertyValues.sh tool. runEtlToolkitToolAsDBA enableMergedPropertyValues Defining the property values of merged i2 Analyze records In the mergedPropertyValuesWalkthrough.sh , the views are created for the Person item type. The enableMergedPropertyValues.sh tool is used in the Create the merged property views for the CORRELATED_SCHEMA_TYPE_IDS section. Updating property value definitions The walkthrough provides an example .sql script that drops the existing IS_Public.E_Person_MPVDV view and replaces it with another. The new view prioritizes property values from merge contributors that come from the ingestion source names EXAMPLE_1 over values from EXAMPLE_2 and any other sources. The createAlternativeMergedPropertyValuesView.sql script is in examples/pre-prod/walkthroughs/configurationChanges . After the views are enabled, the merged property values definition view ( Person_MPVDV ) is modified to change how the property values of correlated records are calculated. Note, this step is also performed by the database administrator. See the Updating property value definitions section of the walkthrough script. The runSQLServerCommandAsDBA client functions in used to run the createAlternativeMergedPropertyValuesView.sql script. runSQLServerCommandAsDBA The merged property values definition view Reingesting the data The property values of merged records do not update when the MPVDV views are modified. To update the values of existing records, you must reingest at least one of the merge contributors to the record. To do this, use the ingestDataWalkthrough.sh script to ingest some data the correlates. For more information about ingesting the data, see Ingesting data into the Information Store . See the Reingesting the data section of the walkthrough script. After the data is ingested, in the Analyst's Notebook Premium chart that you have open, select the Julia Yochum item and click Get changes . The name of the item changes to Julie Yocham , because the property values that make up the name are now from the merge contributor where the ingestion source name is EXAMPLE_1 ."
  },
  "versions/2.2.0/content/walkthroughs/ingestion.html": {
    "href": "versions/2.2.0/content/walkthroughs/ingestion.html",
    "title": "Ingesting data into the Information Store",
    "keywords": "Ingesting data into the Information Store The ingestDataWalkthrough.sh script is a worked example that demonstrates how to script the process of ingesting data into the Information Store. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . The example provides a scripted mechanism to ingest data by using ETL toolkit. The script ingests data with and without correlation identifiers using both STANDARD and BULK import modes. For more information about ingestion, see: Information Store data ingestion Information Store data correlation To script the ingestion process, the following information is stored within the script: The mapping of item type identifiers to staging tables The mapping of staging tables to CSV files and format files. A list of import mapping identifiers from the mapping file. The script uses the runSqlServerCommandAsETL client function to import the data into the staging tables, and runEtlToolkitToolAsi2ETL to run the ingestion commands. Note the external ETL user is performing the import into the staging tables, but the i2 Internal ETL user is executing ETL tasks such as creating the ingestion source or performing the import from the staging tables. The data and ingestion artifacts that are used in the example are in the examples/data directory of the minimal toolkit. Creating the ingestion sources The runEtlToolkitToolAsi2ETL client function is used to run addInformationStoreIngestionSource ETL toolkit tool to create the ingestion sources. runEtlToolkitToolAsi2ETL addInformationStoreIngestionSource For more information about ingestion sources in the Information Store, see Defining an ingestion source For an example of how to use the tool, see the Create the ingestion sources section in the ingestDataWalkthrough.sh Creating the staging tables The runEtlToolkitToolAsi2ETL client function is used to run createInformationStoreStagingTable ETL toolkit tool to create the staging tables. runEtlToolkitToolAsi2ETL createInformationStoreStagingTable For more information about creating staging tables in the Information Store, see Creating the staging tables To create all of the staging tables in the example, schema type identifiers are mapped to staging table names. For an example of how the mappings are used, see the Create the staging tables section in the ingestDataWalkthrough.sh Note: Because there are multiple staging tables for the LAC1 link type, a second loop is used to iterate through the staging table names of each schema type. Ingesting data The walkthrough demonstrates how to ingest both non-correlated and correlated data. For each type of data the staging tables and populated, the data ingestion task is executed, then the staging tables are cleaned. Ingesting non correlated data In this case, the SQL Server BULK INSERT command is used to insert the CSV data into the staging tables. Ingesting correlated data See the Ingesting correlated data section of the walkthrough script. The ingestion procedure The runSqlServerCommandAsETL client function is used to run the sql statement that inserts the data into the staging tables. runSqlServerCommandAsETL Populating the staging tables The example uses CSV and format files to insert the data. The files have the same name ( person.csv and person.fmt ). The staging table names are mapped to the CSV and format files. There is one mapping for base data ( BASE_DATA_TABLE_TO_CSV_AND_FORMAT_FILE_NAME ) and one for correlation data ( CORRELATED_DATA_TABLE_AND_FORMAT_FILE_NAME ). For an example of how the mappings are used, see the Insert the base data into the staging tables section in the ingestDataWalkthrough.sh The runEtlToolkitToolAsi2ETL client function is used to run ingestInformationStoreRecords ETL toolkit tool to ingest the data into the Information Store from the staging tables. runEtlToolkitToolAsi2ETL ingestInformationStoreRecords The ingestInformationStoreRecords toolkit task The ingestInformationStoreRecords tool is used with BULK and STANDARD import modes. Standard import mode is used to ingest the correlation data sets. Bulk import mode is used to ingest the base data set. The import mapping identifiers to use with the ingestInformationStoreRecords tool are defined in the IMPORT_MAPPING_IDS and BULK_IMPORT_MAPPING_IDS lists. A loop is used to ingest data for each mapping identifier in the lists."
  },
  "versions/2.2.0/content/walkthroughs/reset_walkthroughs.html": {
    "href": "versions/2.2.0/content/walkthroughs/reset_walkthroughs.html",
    "title": "Resetting your environment",
    "keywords": "Resetting your environment Resetting the deployment after you complete one of the walkthroughs is done in 2 steps: Resets the configuration directory to the base example configuration Runs the deploy script to redeploy the system with the base configuration Resetting the configuration The configuration directory is reset by calling the pre-prod/createPreProdEnvironment.sh script. This creates a clean configuration for your deployment. Redeploying the system The pre-prod/deploy.sh script is run to redeploy the system."
  },
  "versions/2.2.0/content/walkthroughs/update_configuration.html": {
    "href": "versions/2.2.0/content/walkthroughs/update_configuration.html",
    "title": "Updating the i2 Analyze application configuration",
    "keywords": "Updating the i2 Analyze application configuration This section describes an example of the process to update the configuration of a deployment in a Docker environment. After you update the configuration, rebuild the configured Liberty image. You must rebuild the image, because the configuration is part of the image. Updating the configuration includes the following high-level steps: Removing Liberty containers Updating configuration Rebuilding the configured Liberty image with the modified configuration Running the Liberty containers When you start Liberty, the schema that it caches from the database is updated Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the application configuration, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Updating your configuration In the updateConfigurationWalkthrough.sh script, a modified geospatial-configuration.xml is being copied from the /walkthroughs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. For information about modifying the configuration, see Configuring the i2 Analyze application . See the Updating the configuration section of the walkthrough script. After you modify the configuration, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running."
  },
  "versions/2.2.0/content/walkthroughs/update_match_rules.html": {
    "href": "versions/2.2.0/content/walkthroughs/update_match_rules.html",
    "title": "Updating the system match rules",
    "keywords": "Updating the system match rules This section describes an example of the process to update the system match rules of a deployment with the Information Store. Updating your match rules in your deployment includes the following high-level steps after you modify your system match rules file: Update the match rules file Upload the updated system match rules file to ZooKeeper Create a standby match index with the new system match rules, and wait for a response from the server to say that the standby match index is ready Switch your standby match index to live The updateMatchRulesWalkthrough.sh script is a worked example that demonstrates how to update the system match rules in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Updating your system match rules file In the updateMatchRulesWalkthrough.sh script, the modified system-match-rules.xml is copied from the examples/pre-prod/walkthroughs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. See the Updating system-match-rules.xml section of the walkthrough script. For more information about modifying the system match rules file, see Deploying system match rules . Uploading system match rules to ZooKeeper After you modify the system match rules file, use the update_match_rules function of the runIndexCommand.sh script to upload the match rules to ZooKeeper. For more information, see Manage Solr indexes tool . The runIndexCommand function with the update_match_rules argument in the updateMatchRulesWalkthrough.sh script contains the docker run command that uses an ephemeral Solr client container to upload the match rules. To use the tool in a Docker environment, the following prerequisites must be present: The Docker toolkit and your configuration must be available in the container. In the example, the toolkit and configuration are volume mounted in the docker run command. For example: -v \"pre-reqs/i2analyze/toolkit:/opt/toolkit\" \\ -v \"examples/pre-prod/configuration:/opt/\" Environment variables The tool requires a number of environment variables to be set. For the list of environment variables that you can set, see Manage Solr indexes tool . Java The container must be able to run Java executables. In the example, the container uses the adoptopenjdk image from DockerHub. adoptopenjdk/openjdk8:ubi-jre Checking the standby match index status When uploading match rules, the runIndexCommand.sh will wait for 5 minutes for a response from the server. If however, it takes longer to create the new match index a curl command can be run against liberty. The waitForIndexesToBeBuilt client function makes a request to the api/v1/admin/indexes/status endpoint and inspects the JSON response to see if the match index is built. Switching the standby match index to live After the standby index is ready, you can switch the standby index to live for resolving matches. Use the switch_standby_match_index_to_live function of the runIndexCommand.sh script to switch the indexes. The runIndexCommand function with the switch_standby_match_index_to_live argument in the updateMatchRulesWalkthrough.sh script contains the docker run command that uses an ephemeral Solr client container to switch the match indexes. The tool outputs a message when the action is completed successfully and exit code 0 is returned. For example: > INFO [IndexControlHelper] - The server processed the command successfully: > Switched the live Match index from 'match_index1' to 'match_index2'. If there are any errors, the error is displayed in the console."
  },
  "versions/2.2.0/content/walkthroughs/update_schema.html": {
    "href": "versions/2.2.0/content/walkthroughs/update_schema.html",
    "title": "Updating the schema",
    "keywords": "Updating the schema This section describes an example of the process to update the schema of a deployment in a Docker environment. To update the schema in a deployment, you need to update the Information Store database and restart the application server. Updating the schema includes the following high-level steps: Removing Liberty containers Modifying the schema file Generating the database scripts to update the Information Store database Running the generated scripts against your Information Store database Running Liberty containers When you start Liberty, the schema that it caches from the database is updated Validating database consistency The updateSchemaWalkthrough.sh script is a worked example that demonstrates how to update the schema in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the schema in the Information Store, you remove the Liberty containers. To remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Modifying your schema In the updateSchemaWalkthrough.sh script, a modified schema.xml from the walkthroughs/configuration-changes directory is copied to the configuration/fragments/common/WEB-INF/classes/ directory. After you modify the configuration, build a new configured liberty image with the updated configuration. The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . See the Modifying the schema section of the walkthrough script. To modify your schema, use i2 Analyze Schema Designer. For more information on installing Schema Designer please see: i2 Analyze Schema Designer . Validating your schema After you modify the schema, you can use the validateSchemaAndSecuritySchema.sh tool to validate it. If the schema is invalid, errors are reported. See the Validating the schema section of the walkthrough script. The runi2AnalyzeTool client function is used to run the validateSchemaAndSecuritySchema.sh tool. runi2AnalyzeTool validateSchemaAndSecuritySchema Generating the database scripts After you modify and validate the schema, generate the database scripts that are used to update the Information Store database to reflect the change. See the Generating update schema scripts section of the walkthrough script. The runi2AnalyzeTool client function is used to run the generateUpdateSchemaScripts.sh tool. runi2AnalyzeTool generateUpdateSchemaScripts Running the generated scripts After you generate the scripts, run them against the Information Store database to update the database objects to represent the changes to the schema. See the Running the generated scripts section of the walkthrough script. The runSQLServerCommandAsDBA client function is used to run the runDatabaseScripts.sh tool. runSQLServerCommandAsDBA runDatabaseScripts After the database scripts are run, the Information Store database is updated with any changes to the schema. Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running. Validating database consistency After the system has started, the dbConsistencyCheckScript.sh tool is used to check the state of the database after the Information Store tables are modified. See the Validating database consistency section of the walkthrough script. The runi2AnalyzeTool client function is used to run the dbConsistencyCheckScript.sh tool. runi2AnalyzeTool dbConsistencyCheckScript"
  },
  "versions/2.2.0/content/walkthroughs/update_security_schema.html": {
    "href": "versions/2.2.0/content/walkthroughs/update_security_schema.html",
    "title": "Updating the security schema",
    "keywords": "Updating the security schema This section describes an example of the process to update the security schema of a deployment in a Docker environment. To update the security schema in a deployment, you need to update the Information Store database and restart the application server. Updating the security schema includes the following high-level steps: Removing Liberty containers Modifying the security schema file Starting Liberty containers When you start Liberty, the security schema that it caches from the database is updated Updating the Information Store Running the Liberty containers Validating database consistency The updateSecuritySchemaWalkthrough.sh script is a worked example that demonstrates how to update the security schema in a containerized environment. Note: Before you complete this walkthrough, reset your environment to the base configuration. For more information, see Resetting your environment . Removing the Liberty containers Before you update the security schema in the Information Store, you remove the Liberty containers. To do remove the containers, run the following docker commands: docker stop liberty1 liberty2 docker rm liberty1 liberty2 See the Removing the Liberty containers section of the walkthrough script. Modifying your security schema In the updateSecuritySchemaWalkthrough.sh script, the updateSecuritySchemaFile function copies a modified security-schema.xml from the walkthroughs/configuration-changes directory to the configuration/fragments/common/WEB-INF/classes/ directory. For more information about the structure of the security schema, see The i2 Analyze security schema . The buildLibertyConfiguredImage server function builds the configured Liberty image. For more information, see Building a configured Liberty image . See the Modifying the security schema section of the walkthrough script. Validating your schema After you modify the security schema, you can use the validateSchemaAndSecuritySchema.sh tool to validate it. If the security schema is invalid, errors are reported. See the Validating the new security section of the walkthrough script. The runi2AnalyzeTool client function is used to run the validateSchemaAndSecuritySchema.sh tool. runi2AnalyzeTool validateSchemaAndSecuritySchema Updating the Information Store See the Updating the Information Store section of the walkthrough script. The runi2AnalyzeTool client function is used to run the updateSecuritySchema.sh tool. runi2AnalyzeTool updateSecuritySchema Running the Liberty containers The runLiberty server function runs a Liberty container. For more information about running a Liberty container, see Liberty . See the Running the Liberty containers section of the walkthrough script. Note: You must run both Liberty containers. The waitFori2AnalyzeServiceToBeLive common function ensures that Liberty is running. Validating database consistency After the system has started, the dbConsistencyCheckScript.sh tool is used to check the state of the database after the Information Store tables are modified. See the Validating database consistency section of the walkthrough script. The runi2AnalyzeTool client function is used to run the dbConsistencyCheckScript.sh tool. runi2AnalyzeTool dbConsistencyCheckScript"
  },
  "versions/2.2.0/content/walkthroughs/walkthroughs.html": {
    "href": "versions/2.2.0/content/walkthroughs/walkthroughs.html",
    "title": "Understanding the walkthroughs",
    "keywords": "Understanding the walkthroughs The walkthroughs are designed to demonstrate how you can configure and administer a deployment of i2 Analyze in a containerized environment. The walkthroughs consist of scripts that you can run and a document to provide further explanation. You can run the scripts against the reference architecture deployment. The walkthrough scripts use some of the server and client functions to reduce repetition. This is a pattern that can be copied and used in your own environment. The list of walkthroughs that are provided is: Adding a connector Adding a Solr node Updating the schema Updating the security schema Updating the i2 Analyze application configuration Updating the system match rules Ingesting data Ingesting correlated data Clearing data from a deployment"
  },
  "versions/2.2.0/index.html": {
    "href": "versions/2.2.0/index.html",
    "title": "i2 Analyze containerized deployment",
    "keywords": "i2 Analyze containerized deployment The analyze-containers repository provides a containerized configuration development environment that you can use to develop i2 Analyze configurations, and a reference architecture to demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. The repository is designed to be used with the i2 Analyze minimal toolkit. The minimal toolkit is similar to the standard i2 Analyze deployment toolkit, except that it only includes the minimum amount of application and configuration files. If you are already using the analyze-containers environment and want to upgrade to the latest versions, follow the instructions in Upgrading . Configuration development environment The process to deploy i2 Analyze in production is iterative, and includes a number of phases and environments. The containerized configuration development is designed so that it is easy to move between these stages. Because the environment is containerized, the provided scripts manage the creation of any containers that are required by the environment. For example, the database management system container is added to your environment when you specify a deployment pattern that includes it. You do not need to manually create it first. The following features of containerized configuration development environment are designed to enable you to develop i2 Analyze configurations: Simplified configuration directory The configuration directory structure is flattened to make it easier to locate files The directory includes placeholder configuration files so that you can add to existing files The XSD files are included to provide intellisense in VS Code Simplified deployment You can switch between i2 Analyze deployment patterns by changing a single setting. Use a single deploy command to deploy and update a deployment regardless of the deployment pattern used or any changes to the configuration Manage multiple configurations Develop and store multiple configurations Switch between configurations by using the deploy command Backup the database associated with a configuration Add connectors to the environment The environment can build images and run containers for your connectors To start developing configurations, see Creating your configuration development environment . Containerized deployment reference architecture The repository includes Dockerfiles and example scripts that provide a reference architecture for creating a containerized deployment of i2 Analyze. The scripts demonstrate how to build Docker images and enable you to deploy, configure, and run i2 Analyze on Docker containers. You can run the scripts to create a local example containerized deployment. For more information about the reference architecture, see Understanding the reference architecture ."
  }
}